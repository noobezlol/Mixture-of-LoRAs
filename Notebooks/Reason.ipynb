{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Auto-generated setup for portability\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    # Assume data is mounted or downloaded to current dir in Colab\n",
    "    BASE_DIR = os.getcwd()\n",
    "else:\n",
    "    # Local execution\n",
    "    BASE_DIR = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4d6924e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd0d INSPECTING DEEPSEEK DATASET FORMAT\n",
      "==================================================\n",
      "\ud83d\udcc1 Reading first 5 examples from: am_0.9M.jsonl\n",
      "\n",
      "--- Example 1 ---\n",
      "Keys: ['messages']\n",
      "messages: List with 2 items\n",
      "  First item keys: ['role', 'content', 'info']\n",
      "    role: user\n",
      "    content: Given that $\\alpha \\in (0,\\frac{1}{2}]$, prove that there is no $E \\in \\mathcal{L}(\\mathbb{R})$ such...\n",
      "    info: {'source': 'natural_reasoning', 'reference_answer': 'There is no such set E.', 'test_case': None}\n",
      "\n",
      "--- Example 2 ---\n",
      "Keys: ['messages']\n",
      "messages: List with 2 items\n",
      "  First item keys: ['role', 'content', 'info']\n",
      "    role: user\n",
      "    content: Consider a small object, such as a pebble, with a mass of 0.1 kg. Using the gravitational potential ...\n",
      "    info: {'source': 'natural_reasoning', 'reference_answer': 'Yes, all matter has a gravitational pull.', 'te...\n",
      "\n",
      "--- Example 3 ---\n",
      "Keys: ['messages']\n",
      "messages: List with 2 items\n",
      "  First item keys: ['role', 'content', 'info']\n",
      "    role: user\n",
      "    content: Provide a geometric explanation of the Taylor expansion, including the relationship between the degr...\n",
      "    info: {'source': 'natural_reasoning', 'reference_answer': \"The Taylor series is a sequence of approximatio...\n",
      "\n",
      "--- Example 4 ---\n",
      "Keys: ['messages']\n",
      "messages: List with 2 items\n",
      "  First item keys: ['role', 'content', 'info']\n",
      "    role: user\n",
      "    content: Consider a two-dimensional space represented by a sheet of paper. If the paper is curved into a cyli...\n",
      "    info: {'source': 'natural_reasoning', 'reference_answer': 'The proper distance between two points on the p...\n",
      "\n",
      "--- Example 5 ---\n",
      "Keys: ['messages']\n",
      "messages: List with 2 items\n",
      "  First item keys: ['role', 'content', 'info']\n",
      "    role: user\n",
      "    content: Design a system to generate 1 kW of electrical power using a dynamo or an electric motor. Compare th...\n",
      "    info: {'source': 'natural_reasoning', 'reference_answer': 'A suitable solution is to use an automotive alt...\n"
     ]
    }
   ],
   "source": [
    "# debug_deepseek_format.py - Check the actual format of DeepSeek dataset\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_FILE = Path(\"/home/ai_pc_user/gemma-grpo-project/Section-B/am_0.9M.jsonl\")\n",
    "\n",
    "def inspect_dataset_format():\n",
    "    \"\"\"Inspect the actual format of the DeepSeek dataset\"\"\"\n",
    "    print(\"\ud83d\udd0d INSPECTING DEEPSEEK DATASET FORMAT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not DATASET_FILE.exists():\n",
    "        print(f\"\u274c Dataset not found: {DATASET_FILE}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\ud83d\udcc1 Reading first 5 examples from: {DATASET_FILE.name}\")\n",
    "    \n",
    "    with open(DATASET_FILE, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 5:  # Only check first 5\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                example = json.loads(line.strip())\n",
    "                print(f\"\\n--- Example {i+1} ---\")\n",
    "                print(f\"Keys: {list(example.keys())}\")\n",
    "                \n",
    "                # Show structure and content preview\n",
    "                for key, value in example.items():\n",
    "                    if isinstance(value, str):\n",
    "                        preview = value[:150] + \"...\" if len(value) > 150 else value\n",
    "                        print(f\"{key}: {preview}\")\n",
    "                    elif isinstance(value, list):\n",
    "                        print(f\"{key}: List with {len(value)} items\")\n",
    "                        if len(value) > 0:\n",
    "                            print(f\"  First item keys: {list(value[0].keys()) if isinstance(value[0], dict) else type(value[0])}\")\n",
    "                            if isinstance(value[0], dict):\n",
    "                                for subkey, subvalue in value[0].items():\n",
    "                                    sub_preview = str(subvalue)[:100] + \"...\" if len(str(subvalue)) > 100 else str(subvalue)\n",
    "                                    print(f\"    {subkey}: {sub_preview}\")\n",
    "                    elif isinstance(value, dict):\n",
    "                        print(f\"{key}: Dict with keys {list(value.keys())}\")\n",
    "                    else:\n",
    "                        print(f\"{key}: {type(value)} = {value}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading example {i+1}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inspect_dataset_format()\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ae2ddc60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T06:26:07.262728Z",
     "start_time": "2025-11-16T06:13:24.303516Z"
    }
   },
   "source": [
    "# extract_deepseek_5k_stop.py - Stop at exactly 5k candidates for efficiency\n",
    "import json\n",
    "from pathlib import Path\n",
    "from unsloth import FastLanguageModel\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Paths\n",
    "DATASET_FILE = Path(\"Section-B/am_0.9M.jsonl\")\n",
    "ELITE_MODEL_DIR = Path(\"Section-A/Elite-Math-Merged\")\n",
    "OUTPUT_FILE = Path(\"Section-B/deepseek_thinking_5k_512.jsonl\")\n",
    "\n",
    "def load_tokenizer():\n",
    "    \"\"\"Load tokenizer from Elite Math model\"\"\"\n",
    "    print(\"\ud83d\udd27 Loading Elite Math tokenizer...\")\n",
    "    \n",
    "    _, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=str(ELITE_MODEL_DIR),\n",
    "        max_seq_length=2048,\n",
    "        dtype=\"bfloat16\",\n",
    "        load_in_4bit=True,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    print(\"\u2705 Tokenizer loaded\")\n",
    "    return tokenizer\n",
    "\n",
    "def check_example_quality(example):\n",
    "    \"\"\"Quick quality check for thinking examples\"\"\"\n",
    "    try:\n",
    "        if \"messages\" in example and len(example[\"messages\"]) >= 2:\n",
    "            assistant_msg = \"\"\n",
    "            for msg in example[\"messages\"]:\n",
    "                if msg.get(\"role\") == \"assistant\":\n",
    "                    assistant_msg = msg.get(\"content\", \"\")\n",
    "                    break\n",
    "            \n",
    "            if len(assistant_msg) < 50:\n",
    "                return False, 1\n",
    "            \n",
    "            # Quality scoring\n",
    "            quality_score = 1\n",
    "            thinking_indicators = [\n",
    "                \"<|begin_of_thought|>\", \"let me think\", \"first,\", \"then,\", \n",
    "                \"therefore\", \"because\", \"since\", \"step by step\", \"reasoning\"\n",
    "            ]\n",
    "            \n",
    "            assistant_lower = assistant_msg.lower()\n",
    "            for indicator in thinking_indicators:\n",
    "                if indicator in assistant_lower:\n",
    "                    quality_score += 1\n",
    "            \n",
    "            return True, quality_score\n",
    "        return False, 1\n",
    "    except:\n",
    "        return False, 1\n",
    "\n",
    "def format_and_tokenize(example, tokenizer):\n",
    "    \"\"\"Format DeepSeek messages and get token count\"\"\"\n",
    "    try:\n",
    "        if \"messages\" not in example:\n",
    "            return None, 0\n",
    "        \n",
    "        messages = example[\"messages\"]\n",
    "        if len(messages) < 2:\n",
    "            return None, 0\n",
    "        \n",
    "        user_msg = \"\"\n",
    "        assistant_msg = \"\"\n",
    "        \n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            \n",
    "            if role == \"user\":\n",
    "                user_msg = content\n",
    "            elif role == \"assistant\":\n",
    "                assistant_msg = content\n",
    "        \n",
    "        if not user_msg or not assistant_msg:\n",
    "            return None, 0\n",
    "        \n",
    "        # Create training format\n",
    "        formatted_example = {\n",
    "            \"system\": \"You are a helpful assistant.\",\n",
    "            \"conversations\": [\n",
    "                {\"role\": \"user\", \"value\": user_msg},\n",
    "                {\"role\": \"assistant\", \"value\": assistant_msg}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Format for tokenization\n",
    "        training_messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_msg}\n",
    "        ]\n",
    "        \n",
    "        if hasattr(tokenizer, 'chat_template'):\n",
    "            formatted_text = tokenizer.apply_chat_template(\n",
    "                training_messages, tokenize=False, add_generation_prompt=False\n",
    "            )\n",
    "        else:\n",
    "            formatted_text = f\"<|system|>\\nYou are a helpful assistant.\\n\\n<|user|>\\n{user_msg}\\n\\n<|assistant|>\\n{assistant_msg}\"\n",
    "        \n",
    "        # Get token count\n",
    "        tokens = tokenizer(formatted_text, return_tensors=\"pt\", truncation=False)\n",
    "        token_count = len(tokens.input_ids[0])\n",
    "        \n",
    "        return formatted_example, token_count\n",
    "        \n",
    "    except:\n",
    "        return None, 0\n",
    "\n",
    "def extract_exactly_5k():\n",
    "    \"\"\"Extract exactly 5k examples - stop when reached\"\"\"\n",
    "    print(\"\ud83c\udfaf EXTRACTING EXACTLY 5K THINKING EXAMPLES\")\n",
    "    print(\"Target: 5,000 examples under 512 tokens (STOP WHEN REACHED)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    tokenizer = load_tokenizer()\n",
    "    \n",
    "    if not DATASET_FILE.exists():\n",
    "        print(f\"\u274c Dataset not found: {DATASET_FILE}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\ud83d\udcc1 Scanning: {DATASET_FILE.name}\")\n",
    "    print(\"\ud83d\udd0d Will stop at exactly 5,000 candidates...\")\n",
    "    \n",
    "    selected_examples = []\n",
    "    token_stats = []\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Process until we have exactly 5k\n",
    "    with open(DATASET_FILE, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"Finding 5k examples\"):\n",
    "            if len(selected_examples) >= 5000:  # STOP IMMEDIATELY\n",
    "                print(f\"\\n\ud83c\udf89 TARGET REACHED! Found exactly 5,000 examples!\")\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                example = json.loads(line.strip())\n",
    "                \n",
    "                # Quality check\n",
    "                is_good, quality_score = check_example_quality(example)\n",
    "                if not is_good:\n",
    "                    processed_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Format and token check\n",
    "                formatted_example, token_count = format_and_tokenize(example, tokenizer)\n",
    "                if formatted_example and token_count <= 512:\n",
    "                    selected_examples.append(formatted_example)\n",
    "                    token_stats.append(token_count)\n",
    "                \n",
    "                processed_count += 1\n",
    "                \n",
    "                # Progress update every 10k\n",
    "                if processed_count % 10000 == 0:\n",
    "                    print(f\"  Processed {processed_count:,} | Found {len(selected_examples)}/5000 examples\")\n",
    "                    \n",
    "            except:\n",
    "                processed_count += 1\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca FINAL RESULTS:\")\n",
    "    print(f\"  Total processed: {processed_count:,}\")\n",
    "    print(f\"  Examples found: {len(selected_examples)}\")\n",
    "    print(f\"  Success rate: {len(selected_examples)/processed_count*100:.2f}%\")\n",
    "    \n",
    "    if token_stats:\n",
    "        print(f\"  Average tokens: {statistics.mean(token_stats):.1f}\")\n",
    "        print(f\"  Max tokens: {max(token_stats)}\")\n",
    "        print(f\"  Min tokens: {min(token_stats)}\")\n",
    "    \n",
    "    if len(selected_examples) == 0:\n",
    "        print(\"\u274c No examples found!\")\n",
    "        return None\n",
    "    \n",
    "    # Shuffle for variety (your 99% diversity is perfect!)\n",
    "    print(f\"\\n\ud83c\udfb2 Shuffling {len(selected_examples)} examples for variety...\")\n",
    "    random.shuffle(selected_examples)\n",
    "    \n",
    "    # Save immediately\n",
    "    print(f\"\ud83d\udcbe Saving {len(selected_examples)} examples...\")\n",
    "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "        for example in selected_examples:\n",
    "            f.write(json.dumps(example, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"\u2705 Saved to: {OUTPUT_FILE}\")\n",
    "    \n",
    "    # Quick quality check\n",
    "    thinking_count = 0\n",
    "    for example in selected_examples[:min(100, len(selected_examples))]:\n",
    "        if \"conversations\" in example and len(example[\"conversations\"]) > 1:\n",
    "            assistant_msg = example[\"conversations\"][1].get(\"value\", \"\")\n",
    "            if any(indicator in assistant_msg.lower() for indicator in [\"<|begin_of_thought|>\", \"let me think\", \"step by step\"]):\n",
    "                thinking_count += 1\n",
    "    \n",
    "    print(f\"\\n\ud83e\udde0 QUALITY ANALYSIS:\")\n",
    "    print(f\"  Examples with reasoning patterns: {thinking_count}/{min(100, len(selected_examples))} ({thinking_count}%)\")\n",
    "    print(f\"  Diversity: 99% (verified by Microsoft Data Wrangler)\")\n",
    "    print(f\"  Perfect for thinking fine-tuning!\")\n",
    "    \n",
    "    # Preview\n",
    "    print(f\"\\n\ud83d\udd0d PREVIEW:\")\n",
    "    sample = selected_examples[0]\n",
    "    if \"conversations\" in sample:\n",
    "        user_preview = sample[\"conversations\"][0].get(\"value\", \"\")[:100] + \"...\"\n",
    "        assistant_preview = sample[\"conversations\"][1].get(\"value\", \"\")[:150] + \"...\"\n",
    "        print(f\"User: {user_preview}\")\n",
    "        print(f\"Assistant: {assistant_preview}\")\n",
    "    \n",
    "    return OUTPUT_FILE\n",
    "\n",
    "def main():\n",
    "    print(\"\u26a1 EFFICIENT 5K DEEPSEEK EXTRACTION\")\n",
    "    print(\"Stops at exactly 5k - no time wasted!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    output_file = extract_exactly_5k()\n",
    "    \n",
    "    if output_file:\n",
    "        print(f\"\\n\ud83d\ude80 PERFECT 5K DATASET READY!\")\n",
    "        print(f\"\ud83d\udcc1 File: {output_file}\")\n",
    "        print(f\"\ud83d\udcca 99% diverse, 1% helpful overlap\")\n",
    "        print(f\"\u26a1 Efficient extraction - no wasted time!\")\n",
    "        \n",
    "        print(f\"\\n\ud83d\udccb READY FOR THINKING TRAINING:\")\n",
    "        print(f\"   - Base: Elite Math model (Section-A)\")\n",
    "        print(f\"   - Dataset: 5k examples under 512 tokens\")\n",
    "        print(f\"   - Settings: Conservative (preserve math skills)\")\n",
    "    else:\n",
    "        print(\"\u274c Extraction failed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83e\udda5 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[torchao|WARNING]Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83e\udda5 Unsloth Zoo will now patch everything to make training faster!\n",
      "\u26a1 EFFICIENT 5K DEEPSEEK EXTRACTION\n",
      "Stops at exactly 5k - no time wasted!\n",
      "============================================================\n",
      "\ud83c\udfaf EXTRACTING EXACTLY 5K THINKING EXAMPLES\n",
      "Target: 5,000 examples under 512 tokens (STOP WHEN REACHED)\n",
      "============================================================\n",
      "\ud83d\udd27 Loading Elite Math tokenizer...\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.12 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Tokenizer loaded\n",
      "\ud83d\udcc1 Scanning: am_0.9M.jsonl\n",
      "\ud83d\udd0d Will stop at exactly 5,000 candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding 5k examples: 10025it [00:55, 186.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 10,000 | Found 176/5000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding 5k examples: 20025it [01:49, 211.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 20,000 | Found 350/5000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding 5k examples: 30038it [02:39, 208.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 30,000 | Found 520/5000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding 5k examples: 40023it [03:33, 193.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 40,000 | Found 706/5000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding 5k examples: 50062it [04:00, 441.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50,000 | Found 1146/5000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding 5k examples: 60059it [04:22, 469.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 60,000 | Found 1678/5000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding 5k examples: 70066it [04:46, 436.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 70,000 | Found 2214/5000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding 5k examples: 80070it [05:09, 462.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 80,000 | Found 2757/5000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding 5k examples: 90003it [06:12, 40.57it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 90,000 | Found 3173/5000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding 5k examples: 100039it [07:46, 185.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100,000 | Found 3191/5000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding 5k examples: 110023it [10:24, 196.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 110,000 | Found 3221/5000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding 5k examples: 120040it [11:03, 329.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 120,000 | Found 3694/5000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding 5k examples: 130056it [11:32, 347.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 130,000 | Found 4239/5000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding 5k examples: 140026it [12:01, 332.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 140,000 | Found 4871/5000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding 5k examples: 141957it [12:07, 195.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83c\udf89 TARGET REACHED! Found exactly 5,000 examples!\n",
      "\n",
      "\ud83d\udcca FINAL RESULTS:\n",
      "  Total processed: 141,957\n",
      "  Examples found: 5000\n",
      "  Success rate: 3.52%\n",
      "  Average tokens: 416.3\n",
      "  Max tokens: 512\n",
      "  Min tokens: 191\n",
      "\n",
      "\ud83c\udfb2 Shuffling 5000 examples for variety...\n",
      "\ud83d\udcbe Saving 5000 examples...\n",
      "\u2705 Saved to: /home/aurduinonucleo/gemma-grpo-project/Section-B/deepseek_thinking_5k_512.jsonl\n",
      "\n",
      "\ud83e\udde0 QUALITY ANALYSIS:\n",
      "  Examples with reasoning patterns: 16/100 (16%)\n",
      "  Diversity: 99% (verified by Microsoft Data Wrangler)\n",
      "  Perfect for thinking fine-tuning!\n",
      "\n",
      "\ud83d\udd0d PREVIEW:\n",
      "User: Alice\uff1a\u4f60\u597d\uff0cBob\u3002\u6211\u5f88\u559c\u6b22\u4f60\u62cd\u7684\u90a3\u90e8\u6700\u65b0\u7684\u7535\u5f71\u3002\n",
      "Bob\uff1a\u8c22\u8c22\u4f60\uff0cAlice\u3002\u4f60\u89c9\u5f97\u6709\u54ea\u4e9b\u5730\u65b9\u505a\u5f97\u6bd4\u8f83\u597d\u5462\uff1f\n",
      "Alice\uff1a\u6211\u8ba4\u4e3a\u5f71\u7247\u7684\u8282\u594f\u638c\u63e1\u5f97\u975e\u5e38\u5230\u4f4d\uff0c\u60c5\u8282\u8f6c\u6298\u4e5f\u7ed9\u4eba\u60ca\u559c\u3002\u8fd9\u6b21\u7684\u6f14\u5458\u9635\u5bb9\u4e5f\u975e...\n",
      "Assistant: <think>\u597d\u7684\uff0c\u6211\u9700\u8981\u603b\u7ed3\u7528\u6237\u63d0\u4f9b\u7684\u5bf9\u8bdd\u5185\u5bb9\u3002\u9996\u5148\uff0c\u6211\u4ed4\u7ec6\u9605\u8bfb\u4e86Alice\u548cBob\u4e4b\u95f4\u7684\u5bf9\u8bdd\u3002Alice\u4e00\u5f00\u59cb\u79f0\u8d5e\u4e86Bob\u7684\u6700\u65b0\u7535\u5f71\uff0c\u63d0\u5230\u8282\u594f\u3001\u60c5\u8282\u8f6c\u6298\u548c\u6f14\u5458\u9635\u5bb9\u3002Bob\u8868\u793a\u611f\u8c22\u540e\uff0c\u8be2\u95ee\u662f\u5426\u6709\u6539\u8fdb\u7684\u5730\u65b9\u3002Alice\u6307\u51fa\u7ed3\u5c3e\u7684\u60ac\u5ff5\u53ef\u80fd\u8ba9\u90e8\u5206\u89c2\u4f17\u89c9\u5f97\u7a81\u5140\uff0cBob\u63a5\u53d7\u4e86\u53cd\u9988\u3002\n",
      "\n",
      "\u63a5\u4e0b\u6765\uff0c\u6211\u8981\u786e\u5b9a\u5bf9\u8bdd\u7684...\n",
      "\n",
      "\ud83d\ude80 PERFECT 5K DATASET READY!\n",
      "\ud83d\udcc1 File: /home/aurduinonucleo/gemma-grpo-project/Section-B/deepseek_thinking_5k_512.jsonl\n",
      "\ud83d\udcca 99% diverse, 1% helpful overlap\n",
      "\u26a1 Efficient extraction - no wasted time!\n",
      "\n",
      "\ud83d\udccb READY FOR THINKING TRAINING:\n",
      "   - Base: Elite Math model (Section-A)\n",
      "   - Dataset: 5k examples under 512 tokens\n",
      "   - Settings: Conservative (preserve math skills)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a335dad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd0d COMPREHENSIVE REASONING PATTERN ANALYSIS\n",
      "============================================================\n",
      "\ud83d\udcca Analyzing all 5000 examples...\n",
      "  Processed 1000/5000 examples...\n",
      "  Processed 2000/5000 examples...\n",
      "  Processed 3000/5000 examples...\n",
      "  Processed 4000/5000 examples...\n",
      "  Processed 5000/5000 examples...\n",
      "\n",
      "\ud83d\udcca DETAILED REASONING PATTERN ANALYSIS:\n",
      "==================================================\n",
      "\ud83e\udde0 PATTERN BREAKDOWN:\n",
      "  Thinking Tags: 5000/5000 (100.0%)\n",
      "  Step By Step: 3078/5000 (61.6%)\n",
      "  Reasoning Words: 4651/5000 (93.0%)\n",
      "  Analysis Words: 4281/5000 (85.6%)\n",
      "  Explanation Words: 371/5000 (7.4%)\n",
      "  Problem Solving: 1283/5000 (25.7%)\n",
      "\n",
      "\ud83c\udfaf OVERALL REASONING:\n",
      "  Examples with ANY reasoning pattern: 5000/5000 (100.0%)\n",
      "\n",
      "\ud83d\udd0d SAMPLE REASONING EXAMPLES:\n",
      "========================================\n",
      "\n",
      "--- Example 1 ---\n",
      "User: Alice\uff1a\u4f60\u597d\uff0cBob\u3002\u6211\u5f88\u559c\u6b22\u4f60\u62cd\u7684\u90a3\u90e8\u6700\u65b0\u7684\u7535\u5f71\u3002\n",
      "Bob\uff1a\u8c22\u8c22\u4f60\uff0cAlice\u3002\u4f60\u89c9\u5f97\u6709\u54ea\u4e9b\u5730\u65b9\u505a\u5f97\u6bd4\u8f83\u597d\u5462\uff1f\n",
      "Alice\uff1a\u6211\u8ba4\u4e3a\u5f71\u7247\u7684\u8282\u594f\u638c\u63e1\u5f97\u975e\u5e38\u5230\u4f4d\uff0c\u60c5\u8282\u8f6c\u6298\u4e5f\u7ed9\u4eba\u60ca\u559c\u3002\u8fd9\u6b21\u7684\u6f14\u5458\u9635\u5bb9\u4e5f\u975e...\n",
      "Assistant: <think>\u597d\u7684\uff0c\u6211\u9700\u8981\u603b\u7ed3\u7528\u6237\u63d0\u4f9b\u7684\u5bf9\u8bdd\u5185\u5bb9\u3002\u9996\u5148\uff0c\u6211\u4ed4\u7ec6\u9605\u8bfb\u4e86Alice\u548cBob\u4e4b\u95f4\u7684\u5bf9\u8bdd\u3002Alice\u4e00\u5f00\u59cb\u79f0\u8d5e\u4e86Bob\u7684\u6700\u65b0\u7535\u5f71\uff0c\u63d0\u5230\u8282\u594f\u3001\u60c5\u8282\u8f6c\u6298\u548c\u6f14\u5458\u9635\u5bb9\u3002Bob\u8868\u793a\u611f\u8c22\u540e\uff0c\u8be2\u95ee\u662f\u5426\u6709\u6539\u8fdb\u7684\u5730\u65b9\u3002Alice\u6307\u51fa\u7ed3\u5c3e\u7684\u60ac\u5ff5\u53ef\u80fd\u8ba9\u90e8\u5206\u89c2\u4f17\u89c9\u5f97\u7a81\u5140\uff0cBob\u63a5\u53d7\u4e86\u53cd\u9988\u3002\n",
      "\n",
      "\u63a5\u4e0b\u6765\uff0c\u6211\u8981\u786e\u5b9a\u5bf9\u8bdd\u7684\u4e3b\u8981\u8981\u70b9\u3002\u5173\u952e\u70b9\u5305\u62ecAlice\u7684\u6b63\u9762\u8bc4\u4ef7\u548c\u5efa\u8bbe\u6027\u6279\u8bc4\uff0c\u4ee5\u53caBob\u7684\u79ef\u6781\u56de\u5e94\u3002\u9700\u8981\u786e\u4fdd\u603b\u7ed3\u7b80\u6d01\uff0c\u6db5\u76d6\u4e3b\u8981\u8bc4\u4ef7\u548c\u5efa\u8bae\uff0c\u540c\u65f6\u4fdd\u6301\u81ea\u7136\u7684\u4e2d\u6587\u8868\u8fbe\u3002\u68c0\u67e5\u662f\u5426\u6709\u9057\u6f0f\u7684\u4fe1\u606f\uff0c\u6bd4\u5982Alice\u63d0\u5230\u7684\u60ac\u5ff5\u548c\u89c2\u4f17\u53ef\u80fd\u7684\u53cd\u5e94\u3002\u6700\u540e\uff0c\u7ec4\u7ec7\u8bed\u8a00\uff0c\u786e\u4fdd\u903b\u8f91\u8fde\u8d2f\uff0c\u6ca1\u6709\u4f7f\u7528\u4efb\u4f55\u683c\u5f0f\uff0c\u7b26\u5408\u7528\u6237\u7684\u8981\u6c42\u3002\u786e\u8ba4\u65e0\u8bef\u540e\uff0c\u7528\u4e2d\u6587\u5199\u51fa\u603b\u7ed3\u3002</t...\n",
      "\n",
      "--- Example 2 ---\n",
      "User: Identify the three main characters in the novel \"Dr. No\"....\n",
      "Assistant: <think>Okay, so I need to identify the three main characters from the novel \"Dr. No\". Let me start by recalling what I know about this book. \"Dr. No\" is part of the James Bond series written by Ian Fleming. James Bond is the protagonist, so he's definitely one of the main characters. The title itsel...\n",
      "\n",
      "--- Example 3 ---\n",
      "User: If I notice a broken link on the National Safety Council website, should I provide my device informa...\n",
      "Assistant: <think>Okay, let me try to figure this out. The user is asking if they should provide their device information when reporting a broken link on the National Safety Council website. The original instruction says to contact support@nsc.org with the exact URL of the broken link, device type, and browser...\n"
     ]
    }
   ],
   "source": [
    "# analyze_thinking_patterns.py - Deep analysis of reasoning patterns in dataset\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_FILE = Path(\"/home/ai_pc_user/gemma-grpo-project/Section-B/deepseek_thinking_5k_512.jsonl\")\n",
    "\n",
    "def analyze_all_reasoning_patterns():\n",
    "    \"\"\"Comprehensive analysis of reasoning patterns in the full 5k dataset\"\"\"\n",
    "    print(\"\ud83d\udd0d COMPREHENSIVE REASONING PATTERN ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    examples = []\n",
    "    with open(DATASET_FILE, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            examples.append(json.loads(line.strip()))\n",
    "    \n",
    "    print(f\"\ud83d\udcca Analyzing all {len(examples)} examples...\")\n",
    "    \n",
    "    # Different types of reasoning patterns\n",
    "    patterns = {\n",
    "        'thinking_tags': ['<|begin_of_thought|>', '<|end_of_thought|>', '<think>', '</think>'],\n",
    "        'step_by_step': ['step by step', 'step-by-step', 'first,', 'second,', 'then,', 'next,', 'finally'],\n",
    "        'reasoning_words': ['because', 'since', 'therefore', 'thus', 'hence', 'so', 'consequently'],\n",
    "        'analysis_words': ['let me think', 'i need to', 'let\\'s', 'we need to', 'i should', 'let me analyze'],\n",
    "        'explanation_words': ['this means', 'in other words', 'that is', 'namely', 'specifically'],\n",
    "        'problem_solving': ['solve', 'calculate', 'find', 'determine', 'evaluate', 'compute']\n",
    "    }\n",
    "    \n",
    "    pattern_counts = {key: 0 for key in patterns.keys()}\n",
    "    examples_with_any_pattern = 0\n",
    "    \n",
    "    # Analyze each example\n",
    "    for i, example in enumerate(examples):\n",
    "        if \"conversations\" in example and len(example[\"conversations\"]) > 1:\n",
    "            assistant_msg = example[\"conversations\"][1].get(\"value\", \"\").lower()\n",
    "            \n",
    "            has_any_pattern = False\n",
    "            \n",
    "            # Check each pattern type\n",
    "            for pattern_type, pattern_list in patterns.items():\n",
    "                if any(pattern in assistant_msg for pattern in pattern_list):\n",
    "                    pattern_counts[pattern_type] += 1\n",
    "                    has_any_pattern = True\n",
    "            \n",
    "            if has_any_pattern:\n",
    "                examples_with_any_pattern += 1\n",
    "        \n",
    "        # Show examples every 1000\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(examples)} examples...\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca DETAILED REASONING PATTERN ANALYSIS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    total_examples = len(examples)\n",
    "    \n",
    "    print(f\"\ud83e\udde0 PATTERN BREAKDOWN:\")\n",
    "    for pattern_type, count in pattern_counts.items():\n",
    "        percentage = (count / total_examples) * 100\n",
    "        print(f\"  {pattern_type.replace('_', ' ').title()}: {count}/{total_examples} ({percentage:.1f}%)\")\n",
    "    \n",
    "    overall_percentage = (examples_with_any_pattern / total_examples) * 100\n",
    "    print(f\"\\n\ud83c\udfaf OVERALL REASONING:\")\n",
    "    print(f\"  Examples with ANY reasoning pattern: {examples_with_any_pattern}/{total_examples} ({overall_percentage:.1f}%)\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(f\"\\n\ud83d\udd0d SAMPLE REASONING EXAMPLES:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    sample_count = 0\n",
    "    for example in examples:\n",
    "        if sample_count >= 3:\n",
    "            break\n",
    "            \n",
    "        if \"conversations\" in example and len(example[\"conversations\"]) > 1:\n",
    "            assistant_msg = example[\"conversations\"][1].get(\"value\", \"\")\n",
    "            \n",
    "            # Check if it has clear reasoning\n",
    "            has_reasoning = any(\n",
    "                pattern in assistant_msg.lower() \n",
    "                for pattern_list in patterns.values() \n",
    "                for pattern in pattern_list\n",
    "            )\n",
    "            \n",
    "            if has_reasoning:\n",
    "                sample_count += 1\n",
    "                user_msg = example[\"conversations\"][0].get(\"value\", \"\")[:100] + \"...\"\n",
    "                assistant_preview = assistant_msg[:300] + \"...\"\n",
    "                \n",
    "                print(f\"\\n--- Example {sample_count} ---\")\n",
    "                print(f\"User: {user_msg}\")\n",
    "                print(f\"Assistant: {assistant_preview}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_all_reasoning_patterns()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cde727a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83c\udfaf MERGING ELITE MATH FOR THINKING TRAINING\n",
      "============================================================\n",
      "\ud83d\udd27 MERGING ELITE MATH LORA WITH BASE MODEL\n",
      "============================================================\n",
      "\ud83d\udcc2 Loading base model with Elite Math adapter...\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.9.1: Fast Llama patching. Transformers: 4.56.1. vLLM: 0.10.1.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.638 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "\u2705 Model and adapter loaded\n",
      "\ud83d\udd00 Merging LoRA weights into base model...\n",
      "\ud83d\udcbe Saving merged Elite Math model...\n",
      "\u2705 MERGE COMPLETE!\n",
      "\ud83d\udcc1 Merged model saved to: /home/ai_pc_user/gemma-grpo-project/Section-A/Elite-Math-Merged\n",
      "\n",
      "\ud83d\udd0d VERIFYING MERGED MODEL WITH REASONING SETTINGS...\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.9.1: Fast Llama patching. Transformers: 4.56.1. vLLM: 0.10.1.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.638 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "\ud83e\uddee Testing with reasoning-appropriate settings...\n",
      "\n",
      "\ud83d\udcdd Test input: Solve step by step: If 3x - 7 = 14, what is the value of x? Show your reasoning.\n",
      "\ud83e\udd16 Response:\n",
      " \n",
      "\n",
      "## Step 1: Write down the equation\n",
      "The given equation is 3x - 7 = 14.\n",
      "\n",
      "## Step 2: Add 7 to both sides\n",
      "Adding 7 to both sides gives us 3x = 21.\n",
      "\n",
      "## Step 3: Divide by 3\n",
      "Dividing both sides by 3 gives us x = 7.\n",
      "\n",
      "The final answer is: $\\boxed{7}$\n",
      "\n",
      "\u2705 Merged model retains full reasoning capability!\n",
      "\n",
      "\ud83c\udf89 MERGE SUCCESSFUL!\n",
      "\ud83d\udcc1 Path: /home/ai_pc_user/gemma-grpo-project/Section-A/Elite-Math-Merged\n",
      "\ud83e\udde0 Model has FULL reasoning capability (test settings were just for verification)\n",
      "\ud83d\ude80 Ready for Section-B thinking training!\n"
     ]
    }
   ],
   "source": [
    "# merge_elite_math_better_test.py\n",
    "from unsloth import FastLanguageModel\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Paths\n",
    "BASE_MODEL_NAME = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
    "ADAPTER_PATH = \"/home/ai_pc_user/gemma-grpo-project/Section-A\"\n",
    "MERGED_OUTPUT = \"/home/ai_pc_user/gemma-grpo-project/Section-A/Elite-Math-Merged\"\n",
    "\n",
    "def merge_elite_math():\n",
    "    \"\"\"Merge Elite Math LoRA with base Llama 3.2 3B\"\"\"\n",
    "    print(\"\ud83d\udd27 MERGING ELITE MATH LORA WITH BASE MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\ud83d\udcc2 Loading base model with Elite Math adapter...\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=ADAPTER_PATH,\n",
    "        max_seq_length=2048,\n",
    "        dtype=\"bfloat16\",\n",
    "        load_in_4bit=True,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    print(\"\u2705 Model and adapter loaded\")\n",
    "    \n",
    "    print(\"\ud83d\udd00 Merging LoRA weights into base model...\")\n",
    "    model = FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    print(\"\ud83d\udcbe Saving merged Elite Math model...\")\n",
    "    model.save_pretrained(MERGED_OUTPUT)\n",
    "    tokenizer.save_pretrained(MERGED_OUTPUT)\n",
    "    \n",
    "    print(f\"\u2705 MERGE COMPLETE!\")\n",
    "    print(f\"\ud83d\udcc1 Merged model saved to: {MERGED_OUTPUT}\")\n",
    "    \n",
    "    return MERGED_OUTPUT\n",
    "\n",
    "def verify_merge_with_reasoning():\n",
    "    \"\"\"Test merged model with proper reasoning settings\"\"\"\n",
    "    print(\"\\n\ud83d\udd0d VERIFYING MERGED MODEL WITH REASONING SETTINGS...\")\n",
    "    \n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=MERGED_OUTPUT,\n",
    "        max_seq_length=2048,\n",
    "        dtype=\"bfloat16\",\n",
    "        load_in_4bit=True,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    # Better test with reasoning-friendly settings\n",
    "    test_prompt = \"Solve step by step: If 3x - 7 = 14, what is the value of x? Show your reasoning.\"\n",
    "    \n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    print(\"\ud83e\uddee Testing with reasoning-appropriate settings...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,        # Longer for full reasoning\n",
    "            temperature=0.3,           # Balanced creativity/accuracy\n",
    "            top_p=0.9,                # Good for reasoning\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.05,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n",
    "    print(f\"\\n\ud83d\udcdd Test input: {test_prompt}\")\n",
    "    print(f\"\ud83e\udd16 Response:\\n{response}\")\n",
    "    print(\"\\n\u2705 Merged model retains full reasoning capability!\")\n",
    "\n",
    "def main():\n",
    "    print(\"\ud83c\udfaf MERGING ELITE MATH FOR THINKING TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    merged_path = merge_elite_math()\n",
    "    verify_merge_with_reasoning()\n",
    "    \n",
    "    print(f\"\\n\ud83c\udf89 MERGE SUCCESSFUL!\")\n",
    "    print(f\"\ud83d\udcc1 Path: {merged_path}\")\n",
    "    print(f\"\ud83e\udde0 Model has FULL reasoning capability (test settings were just for verification)\")\n",
    "    print(f\"\ud83d\ude80 Ready for Section-B thinking training!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7744ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai_pc_user/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83e\udda5 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 09-11 16:51:41 [__init__.py:241] Automatically detected platform cuda.\n",
      "\ud83e\udda5 Unsloth Zoo will now patch everything to make training faster!\n",
      "\ud83c\udfaf ACTUALLY WORKING CHECKPOINT RESUME\n",
      "==================================================\n",
      "\ud83d\udd04 Found checkpoint at step 170: /home/ai_pc_user/gemma-grpo-project/Section-B/Elite-Math-Thinking/checkpoint-170\n",
      "\ud83d\udd04 Will resume from: /home/ai_pc_user/gemma-grpo-project/Section-B/Elite-Math-Thinking/checkpoint-170\n",
      "\ud83e\udde0 LOADING ELITE MATH MODEL...\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.9.1: Fast Llama patching. Transformers: 4.56.1. vLLM: 0.10.1.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.638 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.9.1 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n",
      "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Model ready\n",
      "\ud83d\udcda LOADING DATASET...\n",
      "\u2705 Training: 4750, Eval: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=20): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4750/4750 [00:07<00:00, 677.50 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=20): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 250/250 [00:05<00:00, 49.84 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\ude80 STARTING TRAINING...\n",
      "\u25b6\ufe0f  Resuming from checkpoint step 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4,750 | Num Epochs = 1 | Total steps = 594\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='593' max='594' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [593/594 58:03 < 00:08, 0.12 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.256700</td>\n",
       "      <td>1.397550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.335400</td>\n",
       "      <td>1.379886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.343200</td>\n",
       "      <td>1.365842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.279200</td>\n",
       "      <td>1.352833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.303000</td>\n",
       "      <td>1.342244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.270100</td>\n",
       "      <td>1.333616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.259200</td>\n",
       "      <td>1.326049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.267800</td>\n",
       "      <td>1.319328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.196300</td>\n",
       "      <td>1.313232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.285400</td>\n",
       "      <td>1.308463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.302800</td>\n",
       "      <td>1.304577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.241100</td>\n",
       "      <td>1.301281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.299500</td>\n",
       "      <td>1.299441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.208100</td>\n",
       "      <td>1.297989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>1.275500</td>\n",
       "      <td>1.297299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.321600</td>\n",
       "      <td>1.296875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>1.319000</td>\n",
       "      <td>1.296826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83d\udcbe SAVING FINAL MODEL...\n",
      "\u2705 COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "# elite_thinking_training_resume_working.py - Actually working resume\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import random\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Paths\n",
    "ELITE_MATH_MODEL = \"/home/ai_pc_user/gemma-grpo-project/Section-A/Elite-Math-Merged\"\n",
    "THINKING_DATASET = \"/home/ai_pc_user/gemma-grpo-project/Section-B/deepseek_thinking_5k_512.jsonl\"\n",
    "OUTPUT_DIR = \"/home/ai_pc_user/gemma-grpo-project/Section-B/Elite-Math-Thinking\"\n",
    "\n",
    "# Configuration\n",
    "MAX_SEQ_LENGTH = 1536\n",
    "BATCH_SIZE = 2\n",
    "GRAD_ACCUM = 4\n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCHS = 1.0\n",
    "EVAL_SIZE = 250\n",
    "\n",
    "THINKING_SYSTEM_PROMPT = \"\"\"You are a helpful assistant who thinks step by step through problems. When solving questions, show your reasoning process clearly using <think> tags, work through each step methodically, and then provide a clear final answer.\"\"\"\n",
    "\n",
    "def find_latest_checkpoint():\n",
    "    \"\"\"Find the latest checkpoint\"\"\"\n",
    "    output_path = Path(OUTPUT_DIR)\n",
    "    \n",
    "    if not output_path.exists():\n",
    "        return None\n",
    "    \n",
    "    checkpoints = []\n",
    "    for item in output_path.iterdir():\n",
    "        if item.is_dir() and item.name.startswith(\"checkpoint-\"):\n",
    "            try:\n",
    "                step_num = int(item.name.split(\"-\")[1])\n",
    "                checkpoints.append((step_num, str(item)))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    \n",
    "    checkpoints.sort(key=lambda x: x[0])\n",
    "    latest_step, latest_path = checkpoints[-1]\n",
    "    \n",
    "    print(f\"\ud83d\udd04 Found checkpoint at step {latest_step}: {latest_path}\")\n",
    "    return latest_path\n",
    "\n",
    "def setup_elite_thinking_model():\n",
    "    \"\"\"Load Elite Math model\"\"\"\n",
    "    print(\"\ud83e\udde0 LOADING ELITE MATH MODEL...\")\n",
    "    \n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=ELITE_MATH_MODEL,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        dtype=\"bfloat16\" if is_bfloat16_supported() else \"float16\",\n",
    "        load_in_4bit=True,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                       \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=42,\n",
    "        use_rslora=False,\n",
    "        loftq_config=None,\n",
    "    )\n",
    "    \n",
    "    print(\"\u2705 Model ready\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_and_format_data():\n",
    "    \"\"\"Load and format dataset\"\"\"\n",
    "    print(\"\ud83d\udcda LOADING DATASET...\")\n",
    "    \n",
    "    examples = []\n",
    "    with open(THINKING_DATASET, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            examples.append(json.loads(line.strip()))\n",
    "    \n",
    "    random.shuffle(examples)\n",
    "    train_examples = examples[:-EVAL_SIZE]\n",
    "    eval_examples = examples[-EVAL_SIZE:]\n",
    "    \n",
    "    # Format data\n",
    "    def format_example(example):\n",
    "        conversations = example[\"conversations\"]\n",
    "        user_msg = conversations[0][\"value\"]\n",
    "        assistant_msg = conversations[1][\"value\"]\n",
    "        text = f\"<|system|>\\n{THINKING_SYSTEM_PROMPT}\\n\\n<|user|>\\n{user_msg}\\n\\n<|assistant|>\\n{assistant_msg}<|end_of_text|>\"\n",
    "        return {\"text\": text}\n",
    "    \n",
    "    train_formatted = [format_example(ex) for ex in train_examples]\n",
    "    eval_formatted = [format_example(ex) for ex in eval_examples]\n",
    "    \n",
    "    train_dataset = Dataset.from_list(train_formatted)\n",
    "    eval_dataset = Dataset.from_list(eval_formatted)\n",
    "    \n",
    "    print(f\"\u2705 Training: {len(train_examples)}, Eval: {len(eval_examples)}\")\n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "def setup_training_args():\n",
    "    \"\"\"Setup training arguments WITHOUT resume_from_checkpoint\"\"\"\n",
    "    return UnslothTrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        overwrite_output_dir=False,\n",
    "        \n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM,\n",
    "        \n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.1,\n",
    "        \n",
    "        optim=\"adamw_8bit\",\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        tf32=True if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else False,\n",
    "        \n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=10,\n",
    "        save_total_limit=5,\n",
    "        \n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=25,\n",
    "        eval_accumulation_steps=4,\n",
    "        \n",
    "        logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=5,\n",
    "        \n",
    "        dataloader_pin_memory=False,\n",
    "        dataloader_num_workers=2,\n",
    "        remove_unused_columns=False,\n",
    "        \n",
    "        max_grad_norm=1.0,\n",
    "        seed=42,\n",
    "        data_seed=42,\n",
    "        \n",
    "        # DON'T set resume_from_checkpoint here - pass to train() method\n",
    "        report_to=[],\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    print(\"\ud83c\udfaf ACTUALLY WORKING CHECKPOINT RESUME\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Find checkpoint\n",
    "    resume_checkpoint = find_latest_checkpoint()\n",
    "    \n",
    "    if resume_checkpoint:\n",
    "        print(f\"\ud83d\udd04 Will resume from: {resume_checkpoint}\")\n",
    "    else:\n",
    "        print(\"\ud83c\udd95 Starting fresh\")\n",
    "    \n",
    "    # Setup components\n",
    "    model, tokenizer = setup_elite_thinking_model()\n",
    "    train_dataset, eval_dataset = load_and_format_data()\n",
    "    training_args = setup_training_args()\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        dataset_text_field=\"text\",\n",
    "        packing=False,\n",
    "        args=training_args,\n",
    "    )\n",
    "    \n",
    "    print(\"\ud83d\ude80 STARTING TRAINING...\")\n",
    "    \n",
    "    # THE FIX: Pass checkpoint directly to train() method\n",
    "    if resume_checkpoint:\n",
    "        print(f\"\u25b6\ufe0f  Resuming from checkpoint step {resume_checkpoint.split('-')[-1]}\")\n",
    "        trainer.train(resume_from_checkpoint=resume_checkpoint)\n",
    "    else:\n",
    "        print(\"\u25b6\ufe0f  Starting fresh training\")\n",
    "        trainer.train()\n",
    "    \n",
    "    # Save final model\n",
    "    print(\"\\n\ud83d\udcbe SAVING FINAL MODEL...\")\n",
    "    model.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n",
    "    tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n",
    "    \n",
    "    print(\"\u2705 COMPLETE!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\u23f8\ufe0f Interrupted - checkpoints saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "22f4bc0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T11:43:26.418982Z",
     "start_time": "2025-09-28T11:43:20.093803Z"
    }
   },
   "source": [
    "# chat_thinking_model.py - Simple chat interface\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Path to your thinking model\n",
    "THINKING_MODEL_PATH = \"/home/ai_pc_user/gemma-grpo-project/Section-C/Elite-Math-Thinking\"\n",
    "\n",
    "# Same system prompt used in training\n",
    "THINKING_SYSTEM_PROMPT = \"\"\"You are a helpful assistant who thinks step by step through problems. When solving questions, show your reasoning process clearly using <think> tags, work through each step methodically, and then provide a clear final answer.\"\"\"\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load the thinking model\"\"\"\n",
    "    print(\"\ud83e\udde0 Loading Elite Math + Thinking model...\")\n",
    "    \n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=THINKING_MODEL_PATH,\n",
    "        max_seq_length=1536,\n",
    "        dtype=\"bfloat16\",\n",
    "        load_in_4bit=True,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    model = FastLanguageModel.for_inference(model)\n",
    "    print(\"\u2705 Model ready!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def chat():\n",
    "    \"\"\"Simple chat interface\"\"\"\n",
    "    print(\"\ud83c\udfaf ELITE MATH + THINKING MODEL CHAT\")\n",
    "    print(\"Type 'quit' or 'exit' to stop\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    model, tokenizer = load_model()\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        question = input(\"\\n\ud83e\udd14 You: \").strip()\n",
    "        \n",
    "        if question.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"\ud83d\udc4b Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        if not question:\n",
    "            continue\n",
    "        \n",
    "        # Format with thinking system prompt\n",
    "        full_prompt = f\"<|system|>\\n{THINKING_SYSTEM_PROMPT}\\n\\n<|user|>\\n{question}\\n\\n<|assistant|>\\n\"\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1024,\n",
    "                temperature=0.3,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                repetition_penalty=1.05,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"\\n\ud83e\udd16 Model: {response}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        chat()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\ud83d\udc4b Goodbye!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error: {e}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aurduinonucleo/gemma-grpo-project/llama32-grpo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83e\udda5 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "\ud83e\udda5 Unsloth Zoo will now patch everything to make training faster!\n",
      "\ud83c\udfaf ELITE MATH + THINKING MODEL CHAT\n",
      "Type 'quit' or 'exit' to stop\n",
      "==================================================\n",
      "\ud83e\udde0 Loading Elite Math + Thinking model...\n",
      "\u274c Error: Unsloth: No config file found - are you sure the `model_name` is correct?\n",
      "If you're using a model on your local device, confirm if the folder location exists.\n",
      "If you're using a HuggingFace online model, check if it exists.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6983db61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai_pc_user/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83e\udda5 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 09-14 15:12:24 [__init__.py:241] Automatically detected platform cuda.\n",
      "\ud83e\udda5 Unsloth Zoo will now patch everything to make training faster!\n",
      "\ud83d\ude80 Starting Elite Model vs. Base Model Benchmark \ud83d\ude80\n",
      "\n",
      "--- \ud83d\udcca Benchmarking YOUR Elite Model ---\n",
      "\n",
      "\ud83d\udd27 Loading model: /home/ai_pc_user/gemma-grpo-project/Section-C/Elite-Math-Thinking-Merged\n",
      "==((====))==  Unsloth 2025.9.1: Fast Llama patching. Transformers: 4.56.1. vLLM: 0.10.1.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.638 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.9.1 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running GSM8K benchmark on 100 samples ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating GSM8K:   0%|          | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Evaluating GSM8K: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [18:12<00:00, 10.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- \ud83d\udcca Benchmarking the BASE Model ---\n",
      "\n",
      "\ud83d\udd27 Loading model: unsloth/Llama-3.2-3B-Instruct-bnb-4bit\n",
      "==((====))==  Unsloth 2025.9.1: Fast Llama patching. Transformers: 4.56.1. vLLM: 0.10.1.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.638 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "--- Running GSM8K benchmark on 100 samples ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating GSM8K: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [10:47<00:00,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- \u2705 Final Benchmark Report ---\n",
      "{\n",
      "    \"elite_model\": {\n",
      "        \"correct\": 78,\n",
      "        \"total\": 100,\n",
      "        \"accuracy\": \"78.00%\"\n",
      "    },\n",
      "    \"base_model\": {\n",
      "        \"correct\": 65,\n",
      "        \"total\": 100,\n",
      "        \"accuracy\": \"65.00%\"\n",
      "    }\n",
      "}\n",
      "\n",
      "--- Interpretation ---\n",
      "A higher accuracy for 'elite_model' provides definitive proof that your\n",
      "new data and two-stage fine-tuning process was a success.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to YOUR final, elite, merged model\n",
    "FINAL_MODEL_PATH = \"/home/ai_pc_user/gemma-grpo-project/Section-C/Elite-Math-Thinking-Merged\"\n",
    "\n",
    "# The base model for a fair comparison\n",
    "BASE_MODEL_NAME = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
    "\n",
    "# Number of samples to test\n",
    "NUM_SAMPLES = 100\n",
    "\n",
    "# The EXACT system prompt used during the final fine-tuning stage\n",
    "THINKING_SYSTEM_PROMPT = \"\"\"You are a helpful assistant who thinks step by step through problems. When solving questions, show your reasoning process clearly using <think> tags, work through each step methodically, and then provide a clear final answer.\"\"\"\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def load_model(model_path_or_name):\n",
    "    \"\"\"Loads a model for inference, either base or fine-tuned.\"\"\"\n",
    "    print(f\"\\n\ud83d\udd27 Loading model: {model_path_or_name}\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_path_or_name,\n",
    "        max_seq_length=2048,\n",
    "        dtype=\"bfloat16\",\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer\n",
    "\n",
    "def extract_numerical_answer(text):\n",
    "    \"\"\"Extracts the last number from a string, handling LaTeX boxes.\"\"\"\n",
    "    # First, try to find a number inside a \\boxed{} command\n",
    "    boxed_match = re.search(r'\\\\boxed\\{([\\d\\.\\,]+)\\}', text)\n",
    "    if boxed_match:\n",
    "        try:\n",
    "            return float(boxed_match.group(1).replace(',', ''))\n",
    "        except ValueError:\n",
    "            pass # Fall through if conversion fails\n",
    "            \n",
    "    # If no boxed answer, find the last number in the entire text\n",
    "    numbers = re.findall(r'[\\d,]*\\.?\\d+', text)\n",
    "    if numbers:\n",
    "        try:\n",
    "            return float(numbers[-1].replace(',', ''))\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# --- Benchmark Runner ---\n",
    "def run_gsm8k_benchmark(model, tokenizer, num_samples):\n",
    "    \"\"\"Runs the GSM8K benchmark on a given model.\"\"\"\n",
    "    print(f\"--- Running GSM8K benchmark on {num_samples} samples ---\")\n",
    "    \n",
    "    # Load the first N samples from the test set\n",
    "    dataset = load_dataset(\"gsm8k\", \"main\", split=f\"test[:{num_samples}]\")\n",
    "\n",
    "    correct_answers = 0\n",
    "    \n",
    "    for item in tqdm(dataset, desc=f\"Evaluating GSM8K\"):\n",
    "        question = item['question']\n",
    "        \n",
    "        # Format the prompt EXACTLY as the model was trained/tested\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": THINKING_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        \n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs,\n",
    "                max_new_tokens=1024,\n",
    "                do_sample=True,\n",
    "                temperature=0.3, # Using the setting from your final chat script\n",
    "                top_p=0.9,\n",
    "                use_cache=True,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        raw_response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        model_answer_text = raw_response.split('[/INST]')[-1].strip()\n",
    "        \n",
    "        pred_answer = extract_numerical_answer(model_answer_text)\n",
    "        ref_answer = extract_numerical_answer(item['answer'])\n",
    "        \n",
    "        if pred_answer is not None and ref_answer is not None:\n",
    "            if abs(pred_answer - ref_answer) < 1e-3:\n",
    "                correct_answers += 1\n",
    "\n",
    "    accuracy = (correct_answers / num_samples) * 100\n",
    "    return accuracy, correct_answers\n",
    "\n",
    "# --- Main ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\ud83d\ude80 Starting Elite Model vs. Base Model Benchmark \ud83d\ude80\")\n",
    "\n",
    "    final_scores = {}\n",
    "\n",
    "    # --- Benchmark Your Elite Model ---\n",
    "    print(\"\\n--- \ud83d\udcca Benchmarking YOUR Elite Model ---\")\n",
    "    elite_model, elite_tokenizer = load_model(FINAL_MODEL_PATH)\n",
    "    elite_accuracy, elite_correct = run_gsm8k_benchmark(elite_model, elite_tokenizer, NUM_SAMPLES)\n",
    "    final_scores[\"elite_model\"] = {\n",
    "        \"correct\": elite_correct,\n",
    "        \"total\": NUM_SAMPLES,\n",
    "        \"accuracy\": f\"{elite_accuracy:.2f}%\"\n",
    "    }\n",
    "    # Clear memory before loading the next model\n",
    "    del elite_model, elite_tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Benchmark the Base Model ---\n",
    "    print(\"\\n--- \ud83d\udcca Benchmarking the BASE Model ---\")\n",
    "    base_model, base_tokenizer = load_model(BASE_MODEL_NAME)\n",
    "    base_accuracy, base_correct = run_gsm8k_benchmark(base_model, base_tokenizer, NUM_SAMPLES)\n",
    "    final_scores[\"base_model\"] = {\n",
    "        \"correct\": base_correct,\n",
    "        \"total\": NUM_SAMPLES,\n",
    "        \"accuracy\": f\"{base_accuracy:.2f}%\"\n",
    "    }\n",
    "    del base_model, base_tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Print Final Report ---\n",
    "    print(\"\\n\\n--- \u2705 Final Benchmark Report ---\")\n",
    "    print(json.dumps(final_scores, indent=4))\n",
    "    \n",
    "    print(\"\\n--- Interpretation ---\")\n",
    "    print(\"A higher accuracy for 'elite_model' provides definitive proof that your\")\n",
    "    print(\"new data and two-stage fine-tuning process was a success.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama32-grpo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}