{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Auto-generated setup for portability\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    # Assume data is mounted or downloaded to current dir in Colab\n",
    "    BASE_DIR = os.getcwd()\n",
    "else:\n",
    "    # Local execution\n",
    "    BASE_DIR = os.getcwd()\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T09:46:07.808461Z",
     "start_time": "2025-10-02T09:45:32.940880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "chat_thinking_model_FIXED.py - Working chat interface with patch\n",
    "\"\"\"\n",
    "import unsloth\n",
    "# PATCH FIRST - This fixes the num_logits_to_keep error\n",
    "import transformers.generation.utils as gen_utils\n",
    "\n",
    "original_prepare = gen_utils.GenerationMixin._prepare_generation_config\n",
    "\n",
    "def patched_prepare_generation_config(self, generation_config, use_model_defaults, **kwargs):\n",
    "    config, model_kwargs = original_prepare(self, generation_config, use_model_defaults, **kwargs)\n",
    "    model_kwargs.pop('num_logits_to_keep', None)  # Remove the problematic parameter\n",
    "    return config, model_kwargs\n",
    "\n",
    "gen_utils.GenerationMixin._prepare_generation_config = patched_prepare_generation_config\n",
    "print(\"\u2705 Applied num_logits_to_keep fix\")\n",
    "\n",
    "# NOW your original imports and code\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Path to your thinking model\n",
    "THINKING_MODEL_PATH = \"Section-C/Elite-Math-Thinking-Merged\"\n",
    "\n",
    "# Same system prompt used in training\n",
    "THINKING_SYSTEM_PROMPT = \"\"\"You are a helpful assistant who thinks step by step through problems. When solving questions, show your reasoning process clearly using <think> tags, work through each step methodically, and then provide a clear final answer.\"\"\"\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load the thinking model\"\"\"\n",
    "    print(\"\ud83e\udde0 Loading Elite Math + Thinking model...\")\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=THINKING_MODEL_PATH,\n",
    "        max_seq_length=1536,\n",
    "        dtype=\"bfloat16\",\n",
    "        load_in_4bit=True,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    model = FastLanguageModel.for_inference(model)\n",
    "    print(\"\u2705 Model ready!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def chat():\n",
    "    \"\"\"Simple chat interface\"\"\"\n",
    "    print(\"\ud83c\udfaf ELITE MATH + THINKING MODEL CHAT\")\n",
    "    print(\"Type 'quit' or 'exit' to stop\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    model, tokenizer = load_model()\n",
    "\n",
    "    while True:\n",
    "        # Get user input\n",
    "        question = input(\"\\n\ud83e\udd14 You: \").strip()\n",
    "\n",
    "        if question.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"\ud83d\udc4b Goodbye!\")\n",
    "            break\n",
    "\n",
    "        if not question:\n",
    "            continue\n",
    "\n",
    "        # Format with thinking system prompt\n",
    "        full_prompt = f\"<|system|>\\n{THINKING_SYSTEM_PROMPT}\\n\\n<|user|>\\n{question}\\n\\n<|assistant|>\\n\"\n",
    "\n",
    "        # Generate response\n",
    "        inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1024,\n",
    "                temperature=0.3,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                repetition_penalty=1.05,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "        print(f\"\\n\ud83e\udd16 Model: {response}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        chat()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\ud83d\udc4b Goodbye!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error: {e}\")\n"
   ],
   "id": "1dafd1fec06db664",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aurduinonucleo/miniconda3/envs/ai_py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83e\udda5 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "\ud83e\udda5 Unsloth Zoo will now patch everything to make training faster!\n",
      "\u2705 Applied num_logits_to_keep fix\n",
      "\ud83c\udfaf ELITE MATH + THINKING MODEL CHAT\n",
      "Type 'quit' or 'exit' to stop\n",
      "==================================================\n",
      "\ud83e\udde0 Loading Elite Math + Thinking model...\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.9.10: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.9.10 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Model ready!\n",
      "\n",
      "\ud83e\udd16 Model: <think>Okay, the user just sent a greeting message. Let me check if I need to respond. Since the greeting is a common way to start a conversation, I should probably reply with something friendly but not too long. Maybe a simple \"Hello\" or a more personalized greeting if possible. Wait, the user didn't specify any context or topic. So I should keep it general. Hmm, maybe a question to keep the conversation going? Like \"How's your day?\" or \"What's new?\" That way, I'm engaging without being too pushy. Yeah, that's a good approach. Also, I don't want to overdo it with too many words. Keep it short and sweet. Let me check the tone. The user started with \"Hi,\" so maybe a casual tone. Okay, let me draft a response. \"Hi! How's your day going?\" That's friendly and open-ended. No need to add anything else. Yep, that's perfect.</think>\n",
      "<answer>Hello! How's your day going?</answer>\n",
      "--------------------------------------------------\n",
      "\n",
      "\ud83d\udc4b Goodbye!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T12:53:52.754060Z",
     "start_time": "2025-10-03T12:53:06.989967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "\n",
    "class CodeKnowledgeRAG:\n",
    "    \"\"\"Retrieval system over your knowledge base.\"\"\"\n",
    "    def __init__(self, knowledge_base_path=\"./knowledge_base\"):\n",
    "        self.knowledge_base_path = Path(knowledge_base_path)\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.documents = []\n",
    "        self.document_embeddings = None\n",
    "        self.index = None\n",
    "        self._load_knowledge_base()\n",
    "        self._build_index()\n",
    "\n",
    "    def _load_knowledge_base(self):\n",
    "        kb_files = sorted(self.knowledge_base_path.glob(\"*.txt\"))\n",
    "        for file_path in kb_files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            sections = content.split('\\n\\n')\n",
    "            for section in sections:\n",
    "                if section.strip():\n",
    "                    self.documents.append({'source': file_path.name, 'content': section.strip()})\n",
    "\n",
    "    def _build_index(self):\n",
    "        doc_texts = [doc['content'] for doc in self.documents]\n",
    "        self.document_embeddings = self.embedding_model.encode(doc_texts, show_progress_bar=False, convert_to_numpy=True)\n",
    "        dim = self.document_embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "        self.index.add(self.document_embeddings.astype('float32'))\n",
    "\n",
    "    def retrieve_relevant_context(self, query, top_k=3):\n",
    "        query_embedding = self.embedding_model.encode([query], convert_to_numpy=True).astype('float32')\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        relevant_docs = []\n",
    "        for idx in indices[0]:\n",
    "            if idx < len(self.documents):\n",
    "                relevant_docs.append(self.documents[idx]['content'])\n",
    "        return relevant_docs\n",
    "\n",
    "\n",
    "class RAGChatAssistant:\n",
    "    \"\"\"Chat assistant with RAG and enforced stepwise reasoning.\"\"\"\n",
    "    def __init__(self, model_path, knowledge_base_path=\"./knowledge_base\"):\n",
    "        print(\"Initializing RAG Chat Assistant...\")\n",
    "        self.rag = CodeKnowledgeRAG(knowledge_base_path)\n",
    "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_path,\n",
    "            max_seq_length=2048,\n",
    "            dtype=None,\n",
    "            load_in_4bit=True,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        FastLanguageModel.for_inference(self.model)\n",
    "\n",
    "        # Strong system prompt encouraging stepwise thinking with <think> tags\n",
    "        self.system_prompt = (\n",
    "            \"You are a helpful assistant who thinks step by step through problems. \"\n",
    "            \"Always show your reasoning clearly using <think> tags, work through each step \"\n",
    "            \"methodically, and then provide a clear final answer. When you incorporate any reference information, \"\n",
    "            \"explain how it relates inside <think> tags.\"\n",
    "        )\n",
    "\n",
    "    def chat(self):\n",
    "        print(\"Chat Started. Type 'exit' to quit.\")\n",
    "        chat_history = []\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"User: \").strip()\n",
    "            if user_input.lower() in {\"exit\", \"quit\"}:\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "\n",
    "            # Detect if query is code-related to activate RAG retrieval\n",
    "            code_keywords = [\"code\", \"python\", \"list\", \"dict\", \"algorithm\", \"function\", \"set\", \"file\", \"json\", \"search\"]\n",
    "            use_rag = any(kw in user_input.lower() for kw in code_keywords)\n",
    "\n",
    "            context = \"\"\n",
    "            if use_rag:\n",
    "                relevant_docs = self.rag.retrieve_relevant_context(user_input, top_k=2)\n",
    "                if relevant_docs:\n",
    "                    context = \"\\n\\n\".join(f\"Reference:\\n{doc}\" for doc in relevant_docs)\n",
    "\n",
    "            # Build chat messages as list of dicts preserving history\n",
    "            messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "            for (u, a) in chat_history:\n",
    "                messages.append({\"role\": \"user\", \"content\": u})\n",
    "                messages.append({\"role\": \"assistant\", \"content\": a})\n",
    "            prompt_text = context + \"\\n\\n\" + user_input if context else user_input\n",
    "            messages.append({\"role\": \"user\", \"content\": prompt_text})\n",
    "\n",
    "            # Tokenize prompt for model\n",
    "            prompt = self.tokenizer.apply_chat_template(\n",
    "                messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "            ).to(self.model.device)\n",
    "\n",
    "            # Generate response\n",
    "            with torch.inference_mode():\n",
    "                output_ids = self.model.generate(\n",
    "                    input_ids=prompt,\n",
    "                    max_new_tokens=1536,\n",
    "                    temperature=0.2,\n",
    "                    top_p=0.95,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                )\n",
    "            response_text = self.tokenizer.decode(output_ids[0][prompt.shape[1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "            # Enforce <think> tags if missing for reasoning queries\n",
    "            reasoning_keywords = [\"how\", \"why\", \"explain\", \"code\", \"python\", \"algorithm\", \"calculate\"]\n",
    "            if \"<think>\" not in response_text and any(kw in user_input.lower() for kw in reasoning_keywords):\n",
    "                response_text = f\"<think>{response_text}</think>\"\n",
    "\n",
    "            print(f\"Assistant: {response_text}\")\n",
    "\n",
    "            chat_history.append((user_input, response_text))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MODEL_PATH = \"Final-Dynamic-Model/final_model(Math)\"\n",
    "    KNOWLEDGE_BASE_PATH = \"./knowledge_base\"\n",
    "\n",
    "    assistant = RAGChatAssistant(MODEL_PATH, KNOWLEDGE_BASE_PATH)\n",
    "    assistant.chat()\n"
   ],
   "id": "dd35fce690990505",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RAG Chat Assistant...\n",
      "==((====))==  Unsloth 2025.9.10: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Chat Started. Type 'exit' to quit.\n",
      "Assistant: <think>Okay, let's tackle this problem. The task is to find the shortest transformation sequence from a start word to an end word, where only one letter can be changed at a time, and each transformed word must exist in a given word list. All words should have the same length.\n",
      "\n",
      "First, I need to understand the approach. The problem mentions using a stack to track matching pairs and nested structures. But here, we're dealing with a single-letter change, so that stack approach might be overkill. Instead, I should use a breadth-first search (BFS) algorithm. BFS is efficient for finding the shortest path in an unweighted graph, which is what this problem is.\n",
      "\n",
      "The key insight is to use a queue to keep track of words to visit next. Start with the start word, and then for each word, check all possible transformations (by changing one letter) and add them to the queue if they're in the word list. Then, mark the current word as visited to avoid revisiting it. The first word in the queue that matches the end word is the shortest path.\n",
      "\n",
      "Another important point is to handle edge cases. If the end word is not in the word list, return 0. Also, if the start and end words are the same, return 0. The algorithm should be efficient because it only visits each word once.\n",
      "\n",
      "Let me write the Python function. I'll use a queue to keep track of words to visit, and a set to keep track of visited words. The time complexity should be O(n*m), where n is the number of words in the list and m is the maximum length of a word. That's because each word is visited once, and each transformation takes O(m) time.</think>\n",
      "Answer:\n",
      "```python\n",
      "from collections import deque\n",
      "from typing import List\n",
      "\n",
      "def word_ladder(start_word: str, end_word: str, word_list: List[str]) -> int:\n",
      "    \"\"\"\n",
      "    Find the shortest transformation sequence from start_word to end_word, \n",
      "    where only one letter can be changed at a time, and each transformed word \n",
      "    must exist in the given word list. All words have the same length.\n",
      "\n",
      "    Args:\n",
      "    start_word (str): The starting word.\n",
      "    end_word (str): The target word.\n",
      "    word_list (List[str]): The list of valid words.\n",
      "\n",
      "    Returns:\n",
      "    int: The length of the shortest transformation sequence, or 0 if no sequence exists.\n",
      "    \"\"\"\n",
      "    # Check if start and end words are the same\n",
      "    if start_word == end_word:\n",
      "        return 0\n",
      "\n",
      "    # Check if end word is in the word list\n",
      "    if end_word not in word_list:\n",
      "        return 0\n",
      "\n",
      "    # Create a set of valid words for efficient lookups\n",
      "    valid_words = set(word_list)\n",
      "\n",
      "    # Create a queue for BFS, with the start word as the first element\n",
      "    queue = deque([(start_word, 0)])\n",
      "\n",
      "    # Create a set to keep track of visited words\n",
      "    visited = set(start_word)\n",
      "\n",
      "    while queue:\n",
      "        # Dequeue the next word and its distance\n",
      "        word, distance = queue.popleft()\n",
      "\n",
      "        # If the word is the end word, return the distance\n",
      "        if word == end_word:\n",
      "            return distance\n",
      "\n",
      "        # Generate all possible transformations of the current word\n",
      "        for i in range(len(word)):\n",
      "            for char in 'abcdefghijklmnopqrstuvwxyz':\n",
      "                # Skip if the character is the same as the current character\n",
      "                if char == word[i]:\n",
      "                    continue\n",
      "\n",
      "                # Create the new word by replacing the character\n",
      "                new_word = word[:i] + char + word[i+1:]\n",
      "\n",
      "                # Check if the new word is valid and not visited\n",
      "                if new_word in valid_words and new_word not in visited:\n",
      "                    # Mark the new word as visited and enqueue it\n",
      "                    visited.add(new_word)\n",
      "                    queue.append((new_word, distance + 1))\n",
      "\n",
      "    # If no sequence is found, return 0\n",
      "    return 0\n",
      "```\n",
      "Testing:\n",
      "```python\n",
      "print(word_ladder(\"hit\", \"cog\", [\"hot\",\"dot\",\"dog\",\"lot\",\"log\",\"cog\"]))  # Output: 5\n",
      "print(word_ladder(\"hit\", \"cog\", [\"hot\",\"dot\",\"dog\",\"lot\",\"log\"]))  # Output: 0\n",
      "print(word_ladder(\"a\", \"c\", [\"a\",\"b\",\"c\"]))  # Output: 2\n",
      "```\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 127\u001b[0m\n\u001b[1;32m    124\u001b[0m KNOWLEDGE_BASE_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./knowledge_base\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m assistant \u001b[38;5;241m=\u001b[39m RAGChatAssistant(MODEL_PATH, KNOWLEDGE_BASE_PATH)\n\u001b[0;32m--> 127\u001b[0m \u001b[43massistant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 72\u001b[0m, in \u001b[0;36mRAGChatAssistant.chat\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m chat_history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUser: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoodbye!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1275\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1320\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1319\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T11:55:48.539064Z",
     "start_time": "2025-10-02T11:55:27.849779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(model_name, load_in_4bit=True)\n",
    "model = model.for_inference()\n",
    "\n",
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "def chat_step(user_input):\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    prompt_text = tokenizer.apply_chat_template(chat_history, tokenize=False)\n",
    "    tokens_str = tokenizer.tokenize(prompt_text)  # returns tokens as strings like ['\u2581Hello', ...]\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens_str)  # convert to list of int token ids\n",
    "\n",
    "    input_tensor = torch.tensor([input_ids], device=model.device)  # batch dim added\n",
    "\n",
    "    output_tokens = model.generate(\n",
    "        input_tensor,\n",
    "        max_new_tokens=1536,\n",
    "        temperature=0.2,\n",
    "        min_p=0.1,\n",
    "    )\n",
    "\n",
    "    output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": output_text.strip()})\n",
    "    return output_text.strip()\n",
    "\n",
    "user_message = \"\"\"Write a Python function called 'longest_valid_parentheses' that finds the length of the longest valid (well-formed) parentheses substring in a given string containing only '(' and ')' characters.\n",
    "\n",
    "A valid parentheses string is one where:\n",
    "- Every opening parenthesis '(' has a matching closing parenthesis ')'\n",
    "- The parentheses are properly nested\n",
    "\n",
    "Example inputs and expected outputs:\n",
    "- longest_valid_parentheses(\"(()\") should return 2\n",
    "- longest_valid_parentheses(\")()())\") should return 4\n",
    "- longest_valid_parentheses(\"\") should return 0\n",
    "- longest_valid_parentheses(\"()(()\") should return 2\n",
    "- longest_valid_parentheses(\"(()())\") should return 6\n",
    "\n",
    "Provide a complete, working implementation with clear logic. The solution should be efficient (O(n) time complexity preferred). Test your function with all the examples above and explain your approach.\n",
    "\n",
    "\"\"\"\n",
    "response = chat_step(user_message)\n",
    "print(\"Assistant:\", response)\n"
   ],
   "id": "574c69648118d30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.10: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Assistant: system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 02 Oct 2025\n",
      "\n",
      "You are a helpful assistant.user\n",
      "\n",
      "Write a Python function called 'longest_valid_parentheses' that finds the length of the longest valid (well-formed) parentheses substring in a given string containing only '(' and ')' characters.\n",
      "\n",
      "A valid parentheses string is one where:\n",
      "- Every opening parenthesis '(' has a matching closing parenthesis ')'\n",
      "- The parentheses are properly nested\n",
      "\n",
      "Example inputs and expected outputs:\n",
      "- longest_valid_parentheses(\"(()\") should return 2\n",
      "- longest_valid_parentheses(\")()())\") should return 4\n",
      "- longest_valid_parentheses(\"\") should return 0\n",
      "- longest_valid_parentheses(\"()(()\") should return 2\n",
      "- longest_valid_parentheses(\"(()())\") should return 6\n",
      "\n",
      "Provide a complete, working implementation with clear logic. The solution should be efficient (O(n) time complexity preferred). Test your function with all the examples above and explain your approach.assistant\n",
      "\n",
      "```python\n",
      "def longest_valid_parentheses(s: str) -> int:\n",
      "    \"\"\"\n",
      "    Finds the length of the longest valid (well-formed) parentheses substring in a given string.\n",
      "\n",
      "    Args:\n",
      "    s (str): A string containing only '(' and ')' characters.\n",
      "\n",
      "    Returns:\n",
      "    int: The length of the longest valid parentheses substring.\n",
      "    \"\"\"\n",
      "    # Initialize a stack to keep track of the indices of the opening parentheses\n",
      "    stack = [-1]\n",
      "    # Initialize the maximum length of valid parentheses\n",
      "    max_len = 0\n",
      "    # Iterate over the string\n",
      "    for i in range(len(s)):\n",
      "        # If the current character is an opening parenthesis, push its index onto the stack\n",
      "        if s[i] == '(':\n",
      "            stack.append(i)\n",
      "        # If the current character is a closing parenthesis\n",
      "        else:\n",
      "            # Pop the index of the matching opening parenthesis from the stack\n",
      "            stack.pop()\n",
      "            # If the stack is not empty, update the maximum length\n",
      "            if stack:\n",
      "                max_len = max(max_len, i - stack[-1])\n",
      "    # Return the maximum length of valid parentheses\n",
      "    return max_len\n",
      "\n",
      "\n",
      "# Test the function with the provided examples\n",
      "print(longest_valid_parentheses(\"(()\"))  # Output: 2\n",
      "print(longest_valid_parentheses(\")()())\"))  # Output: 4\n",
      "print(longest_valid_parentheses(\"\"))  # Output: 0\n",
      "print(longest_valid_parentheses(\"()(()\"))  # Output: 2\n",
      "print(longest_valid_parentheses(\"(()())\"))  # Output: 6\n",
      "```\n",
      "\n",
      "This function uses a stack data structure to keep track of the indices of the opening parentheses. It iterates over the string, pushing the indices of the opening parentheses onto the stack and popping the indices of the matching opening parentheses when it encounters a closing parenthesis. The maximum length of valid parentheses is updated whenever a longer valid parentheses substring is found. The function returns the maximum length of valid parentheses.\n",
      "\n",
      "The time complexity of this function is O(n), where n is the length of the input string, because it makes a single pass over the string. The space complexity is also O(n), because in the worst case, the stack can grow up to the length of the string.\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}