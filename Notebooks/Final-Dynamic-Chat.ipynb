{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Auto-generated setup for portability\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    # Assume data is mounted or downloaded to current dir in Colab\n",
    "    BASE_DIR = os.getcwd()\n",
    "else:\n",
    "    # Local execution\n",
    "    BASE_DIR = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-24T13:29:10.308157Z",
     "start_time": "2025-10-24T13:24:29.860059Z"
    }
   },
   "source": [
    "# Complete MoE System with System Prompts for Each Expert\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "class MoERouterWithPrompts:\n",
    "    def __init__(\n",
    "        self,\n",
    "        router_path=\"Classifier\",\n",
    "        base_model_name=\"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "        math_adapter_path=\"Final-Dynamic-Model/final_model(Math)\",\n",
    "        code_adapter_path=\"Final-Dynamic-Model/final_model(Code)\",\n",
    "        device=\"cuda\"\n",
    "    ):\n",
    "        # System prompts for each expert\n",
    "        self.MATH_PROMPT = (\n",
    "            \"You are a helpful assistant who thinks step by step through problems. \"\n",
    "            \"When solving questions, show your reasoning process clearly using <think> tags, \"\n",
    "            \"work through each step methodically, and then provide a clear final answer.\"\n",
    "        )\n",
    "\n",
    "        self.CODE_PROMPT = (\n",
    "            \"You are a Python code generator. Output ONLY the function body (not the def line). \"\n",
    "            \"Start the first line with 4 spaces. Do NOT include explanations, comments, or markdown fences. \"\n",
    "            \"If you include <think>, place it before the code, but the final output must remain only the function body.\"\n",
    "        )\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "        print(\"INITIALIZING MoE SYSTEM WITH EXPERT PROMPTS\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # 1. Load Router\n",
    "        print(\"\\n\ud83d\udcca Loading router...\")\n",
    "        self.router = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=router_path,\n",
    "            device=0 if device == \"cuda\" else -1,\n",
    "            max_length=128,\n",
    "            truncation=True\n",
    "        )\n",
    "        print(\"\u2713 Router loaded\")\n",
    "\n",
    "        # 2. Load Base Model\n",
    "        print(\"\\n\ud83e\udde0 Loading base model...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        print(f\"\u2713 Base model loaded: {base_model_name}\")\n",
    "\n",
    "        # 3. Load Math Adapter\n",
    "        print(\"\\n\ud83d\udd22 Loading Math adapter...\")\n",
    "        self.math_model = PeftModel.from_pretrained(\n",
    "            self.base_model,\n",
    "            math_adapter_path\n",
    "        )\n",
    "        print(f\"\u2713 Math adapter loaded\")\n",
    "\n",
    "        # 4. Load Code Adapter\n",
    "        print(\"\\n\ud83d\udcbb Loading Code adapter...\")\n",
    "        self.code_model = PeftModel.from_pretrained(\n",
    "            self.base_model,\n",
    "            code_adapter_path\n",
    "        )\n",
    "        print(f\"\u2713 Code adapter loaded\")\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"\u2705 MoE SYSTEM READY WITH EXPERT PROMPTS!\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    def format_prompt(self, query, category):\n",
    "        \"\"\"Format query with appropriate system prompt based on category\"\"\"\n",
    "        if category == \"math\":\n",
    "            # Math format: System prompt + user query\n",
    "            formatted = f\"{self.MATH_PROMPT}\\n\\nUser: {query}\\n\\nAssistant:\"\n",
    "        elif category == \"code\":\n",
    "            # Code format: System prompt + user query\n",
    "            formatted = f\"{self.CODE_PROMPT}\\n\\nUser: {query}\\n\\nAssistant:\"\n",
    "        else:\n",
    "            # General format: just the query\n",
    "            formatted = f\"User: {query}\\n\\nAssistant:\"\n",
    "\n",
    "        return formatted\n",
    "\n",
    "    def route(self, query):\n",
    "        \"\"\"Route query to appropriate expert\"\"\"\n",
    "        result = self.router(query)[0]\n",
    "        category = result['label'].lower()\n",
    "        confidence = result['score']\n",
    "\n",
    "        print(f\"\\n\ud83d\udd00 Router Decision: {category.upper()} (confidence: {confidence:.0%})\")\n",
    "        return category, confidence\n",
    "\n",
    "    def generate(self, query, max_new_tokens=512, temperature=0.7, do_sample=True):\n",
    "        \"\"\"Generate response using routed expert with proper prompting\"\"\"\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"USER QUERY: {query}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # Step 1: Route the query\n",
    "        category, confidence = self.route(query)\n",
    "\n",
    "        # Step 2: Format prompt with system prompt\n",
    "        formatted_prompt = self.format_prompt(query, category)\n",
    "\n",
    "        # Step 3: Select expert model\n",
    "        if category == \"math\":\n",
    "            print(\"\ud83d\udd22 Using Math expert with step-by-step reasoning prompt...\")\n",
    "            model = self.math_model\n",
    "        elif category == \"code\":\n",
    "            print(\"\ud83d\udcbb Using Code expert with function body generation prompt...\")\n",
    "            model = self.code_model\n",
    "        else:\n",
    "            print(\"\ud83c\udf10 Using base model for general queries...\")\n",
    "            model = self.base_model\n",
    "\n",
    "        # Step 4: Generate response\n",
    "        print(f\"\u2699\ufe0f  Generating response...\\n\")\n",
    "\n",
    "        inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=do_sample,\n",
    "                top_p=0.95,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        # Decode and clean response\n",
    "        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Extract only the assistant's response (remove prompt)\n",
    "        if \"Assistant:\" in full_response:\n",
    "            response = full_response.split(\"Assistant:\")[-1].strip()\n",
    "        else:\n",
    "            response = full_response\n",
    "\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"category\": category,\n",
    "            \"confidence\": confidence,\n",
    "            \"formatted_prompt\": formatted_prompt,\n",
    "            \"response\": response\n",
    "        }\n",
    "\n",
    "# Initialize MoE system\n",
    "print(\"Initializing MoE System...\")\n",
    "moe = MoERouterWithPrompts()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MoE System...\n",
      "================================================================================\n",
      "INITIALIZING MoE SYSTEM WITH EXPERT PROMPTS\n",
      "================================================================================\n",
      "\n",
      "\ud83d\udcca Loading router...\n",
      "\u2713 Router loaded\n",
      "\n",
      "\ud83e\udde0 Loading base model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a1b3ce70e22428cbdb27dbbc9a0404e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f61010f10894af2975c855cd7324326"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Base model loaded: unsloth/Llama-3.2-3B-Instruct-bnb-4bit\n",
      "\n",
      "\ud83d\udd22 Loading Math adapter...\n",
      "\u2713 Math adapter loaded\n",
      "\n",
      "\ud83d\udcbb Loading Code adapter...\n",
      "\u2713 Code adapter loaded\n",
      "\n",
      "================================================================================\n",
      "\u2705 MoE SYSTEM READY WITH EXPERT PROMPTS!\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:29:56.460796Z",
     "start_time": "2025-10-24T13:29:24.401321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test with Math Query\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 1: MATH QUERY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "math_result = moe.generate(\n",
    "    \"Solve the equation 2x + 5 = 15 step by step\",\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.3  # Lower for math (more deterministic)\n",
    ")\n",
    "\n",
    "print(\"\\n\ud83d\udcca RESULT:\")\n",
    "print(math_result['response'])\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Test with Code Query\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 2: CODE QUERY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "code_result = moe.generate(\n",
    "    \"Write a function to reverse a string\",\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.2  # Lower for code (more precise)\n",
    ")\n",
    "\n",
    "print(\"\\n\ud83d\udcbb RESULT:\")\n",
    "print(code_result['response'])\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Test with General Query\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 3: GENERAL QUERY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "general_result = moe.generate(\n",
    "    \"Who invented the telephone?\",\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7  # Normal for general\n",
    ")\n",
    "\n",
    "print(\"\\n\ud83c\udf10 RESULT:\")\n",
    "print(general_result['response'])\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ],
   "id": "987fbf9d5ca7f864",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST 1: MATH QUERY\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "USER QUERY: Solve the equation 2x + 5 = 15 step by step\n",
      "================================================================================\n",
      "\n",
      "\ud83d\udd00 Router Decision: MATH (confidence: 100%)\n",
      "\ud83d\udd22 Using Math expert with step-by-step reasoning prompt...\n",
      "\u2699\ufe0f  Generating response...\n",
      "\n",
      "\n",
      "\ud83d\udcca RESULT:\n",
      "<think>Okay, let's solve the equation 2x + 5 = 15 step by step.\n",
      "\n",
      "First, I need to isolate x. The equation is 2x + 5 = 15. To start, I'll subtract 5 from both sides to get rid of the +5 on the left side.\n",
      "\n",
      "2x + 5 - 5 = 15 - 5\n",
      "2x = 10\n",
      "\n",
      "Next, I'll divide both sides by 2 to solve for x.\n",
      "\n",
      "2x / 2 = 10 / 2\n",
      "x = 5\n",
      "\n",
      "So, the value of x is 5.\n",
      "\n",
      "<answer>Answer: The solution to the equation 2x + 5 = 15 is x = 5.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TEST 2: CODE QUERY\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "USER QUERY: Write a function to reverse a string\n",
      "================================================================================\n",
      "\n",
      "\ud83d\udd00 Router Decision: CODE (confidence: 100%)\n",
      "\ud83d\udcbb Using Code expert with function body generation prompt...\n",
      "\u2699\ufe0f  Generating response...\n",
      "\n",
      "\n",
      "\ud83d\udcbb RESULT:\n",
      "<think>Okay, I need to write a Python function that takes a string as input and returns the reversed string. Let me recall the steps. The user wants a function that reverses a string. The approach is to use slicing. The slicing syntax in Python is [start:stop:step]. Here, start and stop are the same, and the step is -1. So, the reversed string is obtained by slicing from the end to the beginning. Let me test with some examples. For example, \"hello\" reversed is \"olleh\". Another example is \"abc\" reversed is \"cba\". So the function should return the reversed string. The function name is reverse_string. Let me write the code. \n",
      "\n",
      "def reverse_string(s):\n",
      "    return s[::-1]\n",
      "    print(reverse_string(\"hello\"))  # Output: olleh\n",
      "    print(reverse_string(\"abc\"))    # Output: cba\n",
      "    print(reverse_string(\"123\"))    # Output: 321\n",
      "    print(reverse_string(\"\"))        # Output: \n",
      "    print(reverse_string(None))     # Output: None\n",
      "    print(reverse_string(123))      # Output: 321\n",
      "    print(reverse_string(\"a\"))      # Output: a\n",
      "    print(reverse_string(\"\"))       # Output\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TEST 3: GENERAL QUERY\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "USER QUERY: Who invented the telephone?\n",
      "================================================================================\n",
      "\n",
      "\ud83d\udd00 Router Decision: OTHER (confidence: 100%)\n",
      "\ud83c\udf10 Using base model for general queries...\n",
      "\u2699\ufe0f  Generating response...\n",
      "\n",
      "\n",
      "\ud83c\udf10 RESULT:\n",
      "Alexander Graham Bell is credited with inventing the first practical telephone. However, there is also a controversy surrounding the invention, as Elisha Gray and Antonio Meucci also developed devices that could transmit voice messages. The US House of Representatives later officially recognized Bell as the inventor of the telephone.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T08:24:31.149987Z",
     "start_time": "2025-11-14T08:22:07.945742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# mixture_of_loras_single_file.py\n",
    "import unsloth\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from transformers import pipeline, StoppingCriteria, StoppingCriteriaList, TextStreamer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 1: MASTER CONFIGURATION\n",
    "# ==============================================================================\n",
    "# --- Router Path ---\n",
    "ROUTER_PATH = \"Classifier\"\n",
    "\n",
    "# --- Expert Model Paths (Update these) ---\n",
    "BASE_MODEL_PATH = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
    "CODE_LORA_PATH = \"Section-D/Universal-Code-Master/final_model\"\n",
    "MATH_LORA_PATH = \"Final-Dynamic-Model/final_model(Math)\"\n",
    "\n",
    "# --- Expert-Specific System Prompts (CRITICAL to match training) ---\n",
    "CODE_SYSTEM_PROMPT = \"\"\"You are an elite software engineer who writes syntactically perfect, logically sound code across all programming languages.\n",
    "\n",
    "MANDATORY THINKING PROCESS - You MUST use <thinking> tags before <answer>:\n",
    "\n",
    "Inside <thinking>:\n",
    "1. RESTATE THE PROBLEM: Paraphrase the task in your own words to confirm understanding\n",
    "2. IDENTIFY CONSTRAINTS: List all input/output specs, data types, time/space complexity requirements\n",
    "3. ENUMERATE EDGE CASES: Empty inputs, null values, negative numbers, zero, boundary conditions, duplicates, special characters\n",
    "4. COMPARE APPROACHES: Analyze 2-3 different algorithms with their time/space complexity\n",
    "5. CHOOSE OPTIMAL APPROACH: Select the best algorithm and justify why (correctness, efficiency, readability)\n",
    "6. PLAN IMPLEMENTATION: Write pseudocode or step-by-step logic flow\n",
    "7. ANTICIPATE BUGS: Think through off-by-one errors, integer overflow, null pointer issues, index out of bounds\n",
    "\n",
    "Inside <answer>:\n",
    "- Write ONLY the complete, runnable code\n",
    "- Use proper syntax (correct indentation, matching braces, semicolons where needed)\n",
    "- Handle ALL edge cases explicitly in code\n",
    "- Use meaningful variable names\n",
    "- Add minimal inline comments only for complex logic\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- ALWAYS use <thinking> tags for your reasoning process\n",
    "- ALWAYS use <answer> tags for the final code\n",
    "- Code must be syntactically correct (no errors, proper formatting)\n",
    "- Code must be logically sound (handles edge cases, correct algorithm)\n",
    "- Code must be production-ready (no TODOs, no placeholder logic)\n",
    "\"\"\"\n",
    "MATH_SYSTEM_PROMPT = \"\"\"You are a helpful assistant who thinks step by step through problems. When solving questions, show your reasoning process clearly using <think> tags, work through each step methodically, and then provide a clear final answer.\"\"\"\n",
    "OTHER_SYSTEM_PROMPT = \"\"\"You are a helpful and friendly AI assistant. Answer the user's question clearly and concisely.\"\"\"\n",
    "\n",
    "# --- Shared Configuration ---\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "CODE_STOP_TOKEN = \"[END]\"\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 2: UTILITIES AND MODEL CACHING\n",
    "# ==============================================================================\n",
    "# Global cache to hold loaded models, so we don't reload them\n",
    "model_cache = {\n",
    "    \"router\": None,\n",
    "    \"code\": None,\n",
    "    \"math\": None,\n",
    "    \"other\": None, # Will hold the base model\n",
    "}\n",
    "\n",
    "class StopOnToken(StoppingCriteria):\n",
    "    def __init__(self, stop_token_id):\n",
    "        self.stop_token_id = stop_token_id\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        return input_ids[0, -1] == self.stop_token_id\n",
    "\n",
    "def load_expert_model(expert_name):\n",
    "    \"\"\"Lazy-loads an expert model into the cache the first time it's needed.\"\"\"\n",
    "    # Check if the model is already in the cache\n",
    "    if model_cache.get(expert_name):\n",
    "        return model_cache[expert_name]\n",
    "\n",
    "    print(f\"\\n[System] Loading '{expert_name}' expert for the first time... (This may take a moment)\")\n",
    "\n",
    "    if expert_name == \"other\": # Load base model only\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=BASE_MODEL_PATH, max_seq_length=MAX_SEQ_LENGTH, dtype=None, load_in_4bit=True\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model)\n",
    "    else: # Load base model + a specific LoRA adapter\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=BASE_MODEL_PATH, max_seq_length=MAX_SEQ_LENGTH, dtype=None, load_in_4bit=True\n",
    "        )\n",
    "\n",
    "        lora_path = CODE_LORA_PATH if expert_name == \"code\" else MATH_LORA_PATH\n",
    "\n",
    "        if expert_name == \"code\":\n",
    "            tokenizer.add_special_tokens({\"additional_special_tokens\": [CODE_STOP_TOKEN]})\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        model.load_adapter(lora_path)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "\n",
    "    model_cache[expert_name] = (model, tokenizer)\n",
    "    print(f\"[System] \u2705 '{expert_name}' expert is now loaded and ready.\")\n",
    "    return model_cache[expert_name]\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 3: INFERENCE FUNCTIONS FOR EACH EXPERT\n",
    "# ==============================================================================\n",
    "\n",
    "### --- Code Expert Function --- ###\n",
    "def handle_code_query(user_prompt):\n",
    "    model, tokenizer = load_expert_model(\"code\")\n",
    "\n",
    "    stop_token_id = tokenizer.convert_tokens_to_ids(CODE_STOP_TOKEN)\n",
    "    stopping_criteria = StoppingCriteriaList([StopOnToken(stop_token_id)])\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": CODE_SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs, streamer=streamer, max_new_tokens=2048, temperature=0.2,\n",
    "        do_sample=True, stopping_criteria=stopping_criteria, pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # Self-healing logic for code\n",
    "    full_response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    heal_and_reprint_if_needed(full_response.replace(CODE_STOP_TOKEN, \"\").strip())\n",
    "\n",
    "def heal_and_reprint_if_needed(raw_response):\n",
    "    was_healed = False\n",
    "    healed_response = raw_response\n",
    "    if \"<thinking>\" in healed_response and \"</thinking>\" not in healed_response:\n",
    "        answer_pos = healed_response.find(\"<answer>\")\n",
    "        if answer_pos != -1:\n",
    "            healed_response = healed_response[:answer_pos] + \"</thinking>\\n\\n\" + healed_response[answer_pos:]\n",
    "            was_healed = True\n",
    "    if \"<answer>\" in healed_response and \"</answer>\" not in healed_response:\n",
    "        healed_response += \"\\n</answer>\"\n",
    "        was_healed = True\n",
    "    if was_healed:\n",
    "        print(\"\\n\" + \"=\"*40 + \" HEALED RESPONSE \" + \"=\"*40)\n",
    "        print(\"[SYSTEM] Original output was malformed. Displaying corrected version:\")\n",
    "        print(healed_response)\n",
    "        print(\"=\"*96)\n",
    "\n",
    "### --- Math Expert Function --- ###\n",
    "def handle_math_query(user_prompt):\n",
    "    model, tokenizer = load_expert_model(\"math\")\n",
    "\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    full_prompt = f\"<|system|>\\n{MATH_SYSTEM_PROMPT}\\n\\n<|user|>\\n{user_prompt}\\n\\n<|assistant|>\\n\"\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.generate(\n",
    "        **inputs, streamer=streamer, max_new_tokens=1024, temperature=0.3,\n",
    "        top_p=0.9, do_sample=True, repetition_penalty=1.05, pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "### --- Other (Base Model) Function --- ###\n",
    "def handle_other_query(user_prompt):\n",
    "    model, tokenizer = load_expert_model(\"other\")\n",
    "\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": OTHER_SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.generate(\n",
    "        **inputs, streamer=streamer, max_new_tokens=1024, temperature=0.7,\n",
    "        do_sample=True, pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 4: MAIN EXECUTION LOOP\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading the expert router...\")\n",
    "    try:\n",
    "        router = pipeline(\"text-classification\", model=ROUTER_PATH, device=0)\n",
    "        model_cache[\"router\"] = router\n",
    "        print(\"\u2705 Router loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c CRITICAL ERROR: Could not load the router model from '{ROUTER_PATH}'. Error: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"\ud83d\ude80 Mixture-of-LoRAs Chatbot is now running! \ud83d\ude80\")\n",
    "    print(\"   Experts available: Code, Math, Other (Base Model)\")\n",
    "    print(\"   Type 'exit' or 'quit' to close.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            user_input = input(\"\\n>> You: \")\n",
    "            if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "                break\n",
    "            if not user_input:\n",
    "                continue\n",
    "\n",
    "            # 1. Route the query\n",
    "            start_time = time.time()\n",
    "            route_result = model_cache[\"router\"](user_input)[0]\n",
    "            expert_choice = route_result['label'].lower()\n",
    "            confidence = route_result['score']\n",
    "            route_time = time.time() - start_time\n",
    "\n",
    "            print(f\"\\n[Router] -> Decided: **{expert_choice.upper()}** (Confidence: {confidence:.1%}, Time: {route_time:.2f}s)\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            # 2. Dispatch to the appropriate expert function\n",
    "            start_time = time.time()\n",
    "            if expert_choice == \"code\":\n",
    "                print(\"\ud83e\udd16 Code Expert:\")\n",
    "                handle_code_query(user_input)\n",
    "            elif expert_choice == \"math\":\n",
    "                print(\"\ud83e\udd16 Math Expert:\")\n",
    "                handle_math_query(user_input)\n",
    "            else: # 'other'\n",
    "                print(\"\ud83e\udd16 General Assistant:\")\n",
    "                handle_other_query(user_input)\n",
    "            end_time = time.time()\n",
    "\n",
    "            print(f\"\\n(Expert generation time: {end_time - start_time:.2f} seconds)\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExiting...\")\n",
    "\n",
    "    print(\"\\nChat session finished.\")"
   ],
   "id": "704429f8b3c153d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83e\udda5 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[torchao|WARNING]Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83e\udda5 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the expert router...\n",
      "\u2705 Router loaded successfully.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\ud83d\ude80 Mixture-of-LoRAs Chatbot is now running! \ud83d\ude80\n",
      "   Experts available: Code, Math, Other (Base Model)\n",
      "   Type 'exit' or 'quit' to close.\n",
      "================================================================================\n",
      "\n",
      "[Router] -> Decided: **CODE** (Confidence: 99.6%, Time: 0.18s)\n",
      "--------------------------------------------------\n",
      "\ud83e\udd16 Code Expert:\n",
      "\n",
      "[System] Loading 'code' expert for the first time... (This may take a moment)\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[System] \u2705 'code' expert is now loaded and ready.\n",
      "<thinking>\n",
      "- Restate the problem clearly: The task is to write a Python function that prints the string \"Hello\" to the console.\n",
      "- Compare at least two concrete implementation approaches:\n",
      "  1. Using a built-in function: Python has a built-in `print()` function that can be used to print strings.\n",
      "  2. Using a custom function: A custom function can be created to print the string \"Hello\".\n",
      "- Analyze time and space complexity (Big O) for each:\n",
      "  1. Using `print()`: The time complexity is O(1) because it involves a constant number of operations. The space complexity is also O(1) as it doesn't use any additional space that scales with input size.\n",
      "  2. Using a custom function: The time complexity is also O(1) because it involves a constant number of operations. The space complexity is O(1) for the same reason.\n",
      "- Choose the best approach and justify why: Using the built-in `print()` function is the best approach because it is more Pythonic and efficient. It directly achieves the task without the need for additional function calls.\n",
      "- List 2\u20133 key edge cases:\n",
      "  1. Empty input: The function should still print \"Hello\" even if no input is provided.\n",
      "  2. None input: The function should handle None input and still print \"Hello\".\n",
      "  3. Non-string input: The function should handle non-string inputs and print \"Hello\" or raise an appropriate exception.\n",
      "</thinking>\n",
      "\n",
      "<answer>\n",
      "```python\n",
      "def print_hello() -> None:\n",
      "    \"\"\"\n",
      "    Prints \"Hello\" to the console.\n",
      "\n",
      "    This function is a simple example of how to print a string in Python.\n",
      "    \"\"\"\n",
      "    # Using the built-in print function\n",
      "    print(\"Hello\")\n",
      "\n",
      "# Example usage:\n",
      "print_hello()\n",
      "```\n",
      "</answer>\n",
      "[END]\n",
      "\n",
      "(Expert generation time: 33.87 seconds)\n",
      "\n",
      "[Router] -> Decided: **MATH** (Confidence: 99.6%, Time: 0.02s)\n",
      "--------------------------------------------------\n",
      "\ud83e\udd16 Math Expert:\n",
      "\n",
      "[System] Loading 'math' expert for the first time... (This may take a moment)\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[System] \u2705 'math' expert is now loaded and ready.\n",
      "<think>Okay, so the user wants to know what 5 plus 5 is. Let me start by recalling basic arithmetic operations. Addition is the process of combining two or more numbers to get a total. So when you add 5 and 5, you're counting how many units you have in total.\n",
      "\n",
      "First, I need to remember that in addition, you start with the first number and then add the second number to it. So starting with 5: 5 + 5. Let me think about this. If I have 5 apples and someone gives me 5 more apples, how many apples do I have now?\n",
      "\n",
      "Wait, let me break it down. If I have 5 apples and I give away 5, then I'm left with... hmm. Let me count again. 5, 6, 7, 8, 9. So if I start with 5 and add 5, that's 10. Yeah, that makes sense. So the answer should be 10.</think>\n",
      "<answer>\n",
      "The answer is \\boxed{10}.\n",
      "</answer>\n",
      "\n",
      "(Expert generation time: 24.75 seconds)\n",
      "\n",
      "[Router] -> Decided: **OTHER** (Confidence: 99.4%, Time: 0.02s)\n",
      "--------------------------------------------------\n",
      "\ud83e\udd16 General Assistant:\n",
      "\n",
      "[System] Loading 'other' expert for the first time... (This may take a moment)\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[System] \u2705 'other' expert is now loaded and ready.\n",
      "I'm doing well, thank you for asking. I'm a large language model, so I don't have feelings or emotions like humans do, but I'm functioning properly and ready to assist you with any questions or tasks you may have. How about you? How can I help you today?\n",
      "\n",
      "(Expert generation time: 17.48 seconds)\n",
      "\n",
      "\n",
      "Exiting...\n",
      "\n",
      "Chat session finished.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T09:57:14.927404Z",
     "start_time": "2025-11-14T09:54:39.517415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# mixture_of_loras_v2.py (with conversational memory)\n",
    "import unsloth\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from transformers import pipeline, StoppingCriteria, StoppingCriteriaList, TextStreamer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 1: MASTER CONFIGURATION\n",
    "# ==============================================================================\n",
    "ROUTER_PATH = \"Classifier\"\n",
    "\n",
    "BASE_MODEL_PATH = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
    "\n",
    "CODE_LORA_PATH = \"Section-D/Universal-Code-Master/final_model\"\n",
    "\n",
    "MATH_LORA_PATH = \"Final-Dynamic-Model/final_model(Math)\"\n",
    "\n",
    "CODE_SYSTEM_PROMPT = \"\"\"You are an elite software engineer who writes syntactically perfect, logically sound code across all programming languages.\n",
    "\n",
    "MANDATORY THINKING PROCESS - You MUST use <thinking> tags before <answer>:\n",
    "\n",
    "Inside <thinking>:\n",
    "1. RESTATE THE PROBLEM: Paraphrase the task in your own words to confirm understanding\n",
    "2. IDENTIFY CONSTRAINTS: List all input/output specs, data types, time/space complexity requirements\n",
    "3. ENUMERATE EDGE CASES: Empty inputs, null values, negative numbers, zero, boundary conditions, duplicates, special characters\n",
    "4. COMPARE APPROACHES: Analyze 2-3 different algorithms with their time/space complexity\n",
    "5. CHOOSE OPTIMAL APPROACH: Select the best algorithm and justify why (correctness, efficiency, readability)\n",
    "6. PLAN IMPLEMENTATION: Write pseudocode or step-by-step logic flow\n",
    "7. ANTICIPATE BUGS: Think through off-by-one errors, integer overflow, null pointer issues, index out of bounds\n",
    "\n",
    "Inside <answer>:\n",
    "- Write ONLY the complete, runnable code\n",
    "- Use proper syntax (correct indentation, matching braces, semicolons where needed)\n",
    "- Handle ALL edge cases explicitly in code\n",
    "- Use meaningful variable names\n",
    "- Add minimal inline comments only for complex logic\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- ALWAYS use <thinking> tags for your reasoning process\n",
    "- ALWAYS use <answer> tags for the final code\n",
    "- Code must be syntactically correct (no errors, proper formatting)\n",
    "- Code must be logically sound (handles edge cases, correct algorithm)\n",
    "- Code must be production-ready (no TODOs, no placeholder logic)\n",
    "\"\"\"\n",
    "MATH_SYSTEM_PROMPT = \"\"\"You are a helpful assistant who thinks step by step through problems. When solving questions, show your reasoning process clearly using <think> tags, work through each step methodically, and then provide a clear final answer.\"\"\"\n",
    "\n",
    "OTHER_SYSTEM_PROMPT = \"\"\"You are a helpful and friendly AI assistant. Answer the user's question clearly and concisely.\"\"\"\n",
    "\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "CODE_STOP_TOKEN = \"[END]\"\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 2: UTILITIES AND MODEL CACHING\n",
    "# ==============================================================================\n",
    "model_cache = {\"router\": None, \"code\": None, \"math\": None, \"other\": None}\n",
    "\n",
    "class StopOnToken(StoppingCriteria):\n",
    "    def __init__(self, stop_token_id): self.stop_token_id = stop_token_id\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        return input_ids[0, -1] == self.stop_token_id\n",
    "\n",
    "def load_expert_model(expert_name):\n",
    "    if model_cache.get(expert_name): return model_cache[expert_name]\n",
    "    print(f\"\\n[System] Loading '{expert_name}' expert... (This may take a moment)\")\n",
    "    if expert_name == \"other\":\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(model_name=BASE_MODEL_PATH, max_seq_length=MAX_SEQ_LENGTH, dtype=None, load_in_4bit=True)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "    else:\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(model_name=BASE_MODEL_PATH, max_seq_length=MAX_SEQ_LENGTH, dtype=None, load_in_4bit=True)\n",
    "        lora_path = CODE_LORA_PATH if expert_name == \"code\" else MATH_LORA_PATH\n",
    "        if expert_name == \"code\":\n",
    "            tokenizer.add_special_tokens({\"additional_special_tokens\": [CODE_STOP_TOKEN]})\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        model.load_adapter(lora_path)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "    model_cache[expert_name] = (model, tokenizer)\n",
    "    print(f\"[System] \u2705 '{expert_name}' expert is now loaded.\")\n",
    "    return model_cache[expert_name]\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 3: MODIFIED INFERENCE FUNCTIONS (CONTEXT-AWARE)\n",
    "# ==============================================================================\n",
    "\n",
    "def execute_expert_generation(expert_name, system_prompt, chat_history):\n",
    "    \"\"\"A generic function to handle generation for any expert.\"\"\"\n",
    "    model, tokenizer = load_expert_model(expert_name)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    # 1. Construct the prompt EXACTLY as each model was trained\n",
    "    if expert_name == \"math\":\n",
    "        prompt_text = f\"<|system|>\\n{system_prompt}\\n\\n\"\n",
    "        for message in chat_history:\n",
    "            prompt_text += f\"<|{message['role']}|>\\n{message['content']}\\n\\n\"\n",
    "        prompt_text += \"<|assistant|>\\n\"\n",
    "        inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    else:\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}] + chat_history\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "    # 2. Set up base and expert-specific generation configurations\n",
    "    stopping_criteria_list = StoppingCriteriaList([])\n",
    "    if expert_name == \"code\":\n",
    "        stop_token_id = tokenizer.convert_tokens_to_ids(CODE_STOP_TOKEN)\n",
    "        stopping_criteria_list.append(StopOnToken(stop_token_id))\n",
    "\n",
    "    generation_config = {\n",
    "        \"streamer\": streamer,\n",
    "        \"max_new_tokens\": 2048,\n",
    "        \"temperature\": 0.2,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"stopping_criteria\": stopping_criteria_list,\n",
    "    }\n",
    "    if expert_name == \"math\":\n",
    "        generation_config.update({\"temperature\": 0.3, \"top_p\": 0.9, \"repetition_penalty\": 1.05})\n",
    "\n",
    "    # 3. --- THE FINAL, BULLETPROOF FIX ---\n",
    "    #    Prepare the final keyword arguments for the generate function,\n",
    "    #    handling both dictionary-like and Tensor inputs.\n",
    "\n",
    "    final_kwargs = {}\n",
    "    if isinstance(inputs, dict) or hasattr(inputs, 'keys'):\n",
    "        # If inputs is a dictionary or a BatchEncoding object, merge it\n",
    "        final_kwargs = inputs | generation_config\n",
    "    elif isinstance(inputs, torch.Tensor):\n",
    "        # If inputs is just a Tensor, wrap it in a dict with the correct key\n",
    "        final_kwargs = {\"input_ids\": inputs} | generation_config\n",
    "    else:\n",
    "        # Fallback for an unexpected type\n",
    "        raise TypeError(f\"Tokenizer returned an unexpected type: {type(inputs)}\")\n",
    "\n",
    "    # 4. Call generate with the fully prepared and validated kwargs\n",
    "    outputs = model.generate(**final_kwargs)\n",
    "\n",
    "    # 5. Decode and return the response\n",
    "    #    The way we get the length of the prompt is now also dependent on the input type\n",
    "    prompt_length = inputs.shape[1] if isinstance(inputs, torch.Tensor) else inputs.input_ids.shape[1]\n",
    "    response_text = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\n",
    "\n",
    "    return response_text.replace(CODE_STOP_TOKEN, \"\").strip()\n",
    "\n",
    "def heal_and_reprint_if_needed(raw_response):\n",
    "    was_healed = False\n",
    "    healed_response = raw_response\n",
    "    if \"<thinking>\" in healed_response and \"</thinking>\" not in healed_response:\n",
    "        answer_pos = healed_response.find(\"<answer>\")\n",
    "        if answer_pos != -1:\n",
    "            healed_response = healed_response[:answer_pos] + \"</thinking>\\n\\n\" + healed_response[answer_pos:]\n",
    "            was_healed = True\n",
    "    if \"<answer>\" in healed_response and \"</answer>\" not in healed_response:\n",
    "        healed_response += \"\\n</answer>\"\n",
    "        was_healed = True\n",
    "    if was_healed:\n",
    "        print(\"\\n\" + \"=\"*40 + \" HEALED RESPONSE \" + \"=\"*40)\n",
    "        print(\"[SYSTEM] Original output was malformed. Displaying corrected version:\")\n",
    "        print(healed_response)\n",
    "        print(\"=\"*96)\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 4: MAIN EXECUTION LOOP (WITH MEMORY)\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading the expert router...\")\n",
    "    try:\n",
    "        router = pipeline(\"text-classification\", model=ROUTER_PATH, device=0)\n",
    "        model_cache[\"router\"] = router\n",
    "        print(\"\u2705 Router loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c CRITICAL ERROR: Could not load router. Error: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # ** NEW **: Initialize the unified chat history\n",
    "    chat_history = []\n",
    "\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"\ud83d\ude80 CONVERSATIONAL Mixture-of-LoRAs Chatbot is running! \ud83d\ude80\")\n",
    "    print(\"   This chatbot now has memory.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            user_input = input(\"\\n>> You: \")\n",
    "            if user_input.lower() in [\"exit\", \"quit\"]: break\n",
    "            if not user_input: continue\n",
    "\n",
    "            # Append new user message to history\n",
    "            chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "            # 1. Route the LATEST user query\n",
    "            start_time = time.time()\n",
    "            route_result = model_cache[\"router\"](user_input)[0]\n",
    "            expert_choice = route_result['label'].lower()\n",
    "            confidence = route_result['score']\n",
    "            print(f\"\\n[Router] -> Decided: **{expert_choice.upper()}** (Confidence: {confidence:.1%})\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            # 2. Dispatch to the appropriate expert function WITH the full history\n",
    "            start_time = time.time()\n",
    "            assistant_response = \"\"\n",
    "            if expert_choice == \"code\":\n",
    "                print(\"\ud83e\udd16 Code Expert:\")\n",
    "                assistant_response = execute_expert_generation(\"code\", CODE_SYSTEM_PROMPT, chat_history)\n",
    "                print(f\"\\n(Expert generation time: {time.time() - start_time:.2f} seconds)\")\n",
    "                heal_and_reprint_if_needed(assistant_response)\n",
    "            elif expert_choice == \"math\":\n",
    "                print(\"\ud83e\udd16 Math Expert:\")\n",
    "                assistant_response = execute_expert_generation(\"math\", MATH_SYSTEM_PROMPT, chat_history)\n",
    "                print(f\"\\n(Expert generation time: {time.time() - start_time:.2f} seconds)\")\n",
    "            else: # 'other'\n",
    "                print(\"\ud83e\udd16 General Assistant:\")\n",
    "                assistant_response = execute_expert_generation(\"other\", OTHER_SYSTEM_PROMPT, chat_history)\n",
    "                print(f\"\\n(Expert generation time: {time.time() - start_time:.2f} seconds)\")\n",
    "\n",
    "            # ** NEW **: Append the assistant's response to the history\n",
    "            chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExiting...\")\n",
    "\n",
    "    print(\"\\nChat session finished.\")"
   ],
   "id": "fdf99bd419e02ef5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the expert router...\n",
      "\u2705 Router loaded successfully.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\ud83d\ude80 CONVERSATIONAL Mixture-of-LoRAs Chatbot is running! \ud83d\ude80\n",
      "   This chatbot now has memory.\n",
      "================================================================================\n",
      "\n",
      "[Router] -> Decided: **MATH** (Confidence: 99.6%)\n",
      "--------------------------------------------------\n",
      "\ud83e\udd16 Math Expert:\n",
      "\n",
      "[System] Loading 'math' expert... (This may take a moment)\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[System] \u2705 'math' expert is now loaded.\n",
      "<think>Okay, the user wants to know what 5 plus 5 is. Let me start by recalling basic arithmetic operations. Addition is the operation where you combine two numbers by adding their values together. So for 5+5, I need to add 5 and 5. \n",
      "\n",
      "First, I'll start with 5. Then add 5 again. That's 10. So 5+5 should be 10. Wait, let me check if there's any other way to think about it. Maybe the user is testing if I remember basic math facts. But yes, 5+5 is definitely 10. I don't see any alternative calculations here. The problem is straightforward. I should just recall that addition works by combining numbers. So 5+5 equals 10.</think>\n",
      "<answer>\n",
      "The answer is \\boxed{10}.\n",
      "</answer>\n",
      "\n",
      "(Expert generation time: 23.70 seconds)\n",
      "\n",
      "[Router] -> Decided: **CODE** (Confidence: 99.6%)\n",
      "--------------------------------------------------\n",
      "\ud83e\udd16 Code Expert:\n",
      "\n",
      "[System] Loading 'code' expert... (This may take a moment)\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[System] \u2705 'code' expert is now loaded.\n",
      "<think>Okay, the task is to write Python code that simply prints \"Hello\". Let me compare two approaches: \n",
      "\n",
      "1. Using the built-in `print()` function.\n",
      "2. Manually constructing the string and then printing it.\n",
      "\n",
      "For the first approach, `print(\"Hello\")`, Python will automatically handle the string conversion and print it to the console. This is the most straightforward and efficient way to achieve the task.\n",
      "\n",
      "For the second approach, manually constructing the string, it would look something like this: `print(\"Hello\".encode('utf-8').decode('utf-8'))`. However, this is unnecessary complexity and doesn't directly print \"Hello\". So, the best approach is to use the `print()` function.\n",
      "\n",
      "Time complexity: O(1) because it's a constant time operation.\n",
      "Space complexity: O(1) because it doesn't use any additional space that scales with input size.\n",
      "\n",
      "I'll choose the first approach because it's the most Pythonic and efficient.</think>\n",
      "<answer>\n",
      "```python\n",
      "# Print \"Hello\" using the built-in print function\n",
      "print(\"Hello\")\n",
      "```\n",
      "</answer>\n",
      "\n",
      "(Expert generation time: 25.80 seconds)\n",
      "\n",
      "[Router] -> Decided: **OTHER** (Confidence: 99.5%)\n",
      "--------------------------------------------------\n",
      "\ud83e\udd16 General Assistant:\n",
      "\n",
      "[System] Loading 'other' expert... (This may take a moment)\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[System] \u2705 'other' expert is now loaded.\n",
      "Your first two messages were about basic arithmetic operations and printing \"Hello\" in Python, respectively.\n",
      "\n",
      "(Expert generation time: 17.72 seconds)\n",
      "\n",
      "\n",
      "Exiting...\n",
      "\n",
      "Chat session finished.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T10:16:18.870311Z",
     "start_time": "2025-11-14T10:14:06.622692Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing system...\n",
      "\u2705 Router loaded successfully.\n",
      "[RAG System] Initializing...\n",
      "[RAG System] \u2705 Ready. Loaded 19 document sections.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\ud83d\ude80 CONVERSATIONAL Mixture-of-LoRAs + RAG Chatbot is running! \ud83d\ude80\n",
      "================================================================================\n",
      "\n",
      "[Router] -> Decided: **MATH** (Confidence: 99.6%)\n",
      "\ud83e\udd16 Math Expert:\n",
      "\n",
      "[System] Loading 'math' expert... (This may take a moment)\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[System] \u2705 'math' expert is now loaded.\n",
      "<think>Okay, the user wants to know what 5 plus 5 is. Let me check if I remember basic arithmetic. Addition is straightforward here. So, 5 plus 5 should be 10. Wait, maybe they're asking for a specific format or something? But the question is simple enough. Maybe there's a trick or a special case I need to consider. Hmm, no, standard addition applies here. So the answer should be 10. Let me double-check. 5 + 5 = 10. Yeah, that's right. No need to calculate further. The user probably just wants the result of the operation.</think>\n",
      "<answer>\n",
      "\\boxed{10}\n",
      "</answer>\n",
      "\n",
      "(Expert generation time: 24.96 seconds)\n",
      "\n",
      "[Router] -> Decided: **CODE** (Confidence: 99.6%)\n",
      "[RAG System] Retrieving top-2 documents...\n",
      "[RAG System] -> Found 2 relevant documents.\n",
      "\ud83e\udd16 Code Expert (with RAG):\n",
      "\n",
      "[System] Loading 'code' expert... (This may take a moment)\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[System] \u2705 'code' expert is now loaded.\n",
      "<think>\n",
      "The task is to write a Python function that prints \"Hello\" to the console. This is a very basic operation, but it's a good example to demonstrate how to structure a simple print statement in Python.\n",
      "\n",
      "Two possible approaches are:\n",
      "1. Directly printing \"Hello\" using the `print()` function.\n",
      "2. Using a function that takes no arguments and prints \"Hello\".\n",
      "\n",
      "Both approaches are straightforward and don't require any complex logic. The first approach is more concise and directly achieves the goal. The second approach is useful if we want to encapsulate the print operation within a function for reusability or other reasons.\n",
      "\n",
      "Time complexity for both approaches is O(1) because they involve a constant amount of work regardless of the input size. Space complexity is also O(1) as they don't use any additional space that scales with input size.\n",
      "\n",
      "The best approach is to directly print \"Hello\" using the `print()` function because it's the most concise and efficient way to achieve the goal.</think>\n",
      "<answer>\n",
      "```python\n",
      "def print_hello() -> None:\n",
      "    \"\"\"Prints 'Hello' to the console.\"\"\"\n",
      "    print(\"Hello\")\n",
      "\n",
      "# Example usage:\n",
      "if __name__ == \"__main__\":\n",
      "    print_hello()\n",
      "```\n",
      "</answer>\n",
      "[END]\n",
      "\n",
      "(Expert generation time: 26.82 seconds)\n",
      "\n",
      "[Router] -> Decided: **OTHER** (Confidence: 99.5%)\n",
      "\ud83e\udd16 General Assistant:\n",
      "\n",
      "[System] Loading 'other' expert... (This may take a moment)\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[System] \u2705 'other' expert is now loaded.\n",
      "Your first two messages were about basic arithmetic operations.\n",
      "\n",
      "1. You asked what 5 plus 5 is.\n",
      "2. You asked me to write the Python code to print \"Hello\".\n",
      "\n",
      "(Expert generation time: 17.70 seconds)\n",
      "\n",
      "\n",
      "Exiting...\n",
      "\n",
      "Chat session finished.\n"
     ]
    }
   ],
   "execution_count": 2,
   "source": [
    "\n",
    "import unsloth\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, StoppingCriteria, StoppingCriteriaList, TextStreamer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 1: MASTER CONFIGURATION\n",
    "# ==============================================================================\n",
    "# --- Paths ---\n",
    "ROUTER_PATH = \"Classifier\"\n",
    "BASE_MODEL_PATH = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
    "CODE_LORA_PATH = \"Section-D/Universal-Code-Master/final_model\"\n",
    "MATH_LORA_PATH = \"Final-Dynamic-Model/final_model(Math)\"\n",
    "KNOWLEDGE_BASE_PATH = \"knowledge_base\"\n",
    "\n",
    "# --- System Prompts ---\n",
    "CODE_SYSTEM_PROMPT = \"\"\"You are an elite software engineer who writes syntactically perfect, logically sound code across all programming languages.\n",
    "\n",
    "MANDATORY THINKING PROCESS - You MUST use <thinking> tags before <answer>:\n",
    "\n",
    "Inside <thinking>:\n",
    "1. RESTATE THE PROBLEM: Paraphrase the task in your own words to confirm understanding\n",
    "2. IDENTIFY CONSTRAINTS: List all input/output specs, data types, time/space complexity requirements\n",
    "3. ENUMERATE EDGE CASES: Empty inputs, null values, negative numbers, zero, boundary conditions, duplicates, special characters\n",
    "4. COMPARE APPROACHES: Analyze 2-3 different algorithms with their time/space complexity\n",
    "5. CHOOSE OPTIMAL APPROACH: Select the best algorithm and justify why (correctness, efficiency, readability)\n",
    "6. PLAN IMPLEMENTATION: Write pseudocode or step-by-step logic flow\n",
    "7. ANTICIPATE BUGS: Think through off-by-one errors, integer overflow, null pointer issues, index out of bounds\n",
    "\n",
    "Inside <answer>:\n",
    "- Write ONLY the complete, runnable code\n",
    "- Use proper syntax (correct indentation, matching braces, semicolons where needed)\n",
    "- Handle ALL edge cases explicitly in code\n",
    "- Use meaningful variable names\n",
    "- Add minimal inline comments only for complex logic\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- ALWAYS use <thinking> tags for your reasoning process\n",
    "- ALWAYS use <answer> tags for the final code\n",
    "- Code must be syntactically correct (no errors, proper formatting)\n",
    "- Code must be logically sound (handles edge cases, correct algorithm)\n",
    "- Code must be production-ready (no TODOs, no placeholder logic)\n",
    "\"\"\"\n",
    "MATH_SYSTEM_PROMPT = \"\"\"You are a helpful assistant who thinks step by step through problems. When solving questions, show your reasoning process clearly using <think> tags, work through each step methodically, and then provide a clear final answer.\"\"\"\n",
    "OTHER_SYSTEM_PROMPT = \"\"\"You are a helpful and friendly AI assistant. Answer the user's question clearly and concisely.\"\"\"\n",
    "\n",
    "# --- Shared Configuration ---\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "CODE_STOP_TOKEN = \"[END]\"\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 2: RAG SYSTEM IMPLEMENTATION\n",
    "# ==============================================================================\n",
    "class CodeKnowledgeRAG:\n",
    "    def __init__(self, knowledge_base_path):\n",
    "        print(\"[RAG System] Initializing...\")\n",
    "        self.knowledge_base_path = Path(knowledge_base_path)\n",
    "        if not self.knowledge_base_path.exists():\n",
    "            print(f\"[RAG System] \u26a0\ufe0f  Warning: Knowledge base path '{knowledge_base_path}' does not exist. Creating it.\")\n",
    "            self.knowledge_base_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
    "        self.documents = []\n",
    "        self.index = None\n",
    "        self._load_and_build()\n",
    "        if not self.documents:\n",
    "            print(\"[RAG System] \u26a0\ufe0f  Warning: No .txt files found in knowledge base. RAG will not retrieve any context.\")\n",
    "        else:\n",
    "            print(f\"[RAG System] \u2705 Ready. Loaded {len(self.documents)} document sections.\")\n",
    "\n",
    "    def _load_and_build(self):\n",
    "        kb_files = sorted(self.knowledge_base_path.glob(\"*.txt\"))\n",
    "        for file_path in kb_files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f: content = f.read()\n",
    "            sections = content.split('\\n\\n')\n",
    "            for section in sections:\n",
    "                if section.strip(): self.documents.append(section.strip())\n",
    "        if not self.documents: return\n",
    "\n",
    "        embeddings = self.embedding_model.encode(self.documents, show_progress_bar=False, convert_to_numpy=True)\n",
    "        dim = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "\n",
    "    def retrieve(self, query, top_k=2):\n",
    "        if not self.index or not self.documents: return []\n",
    "        print(f\"[RAG System] Retrieving top-{top_k} documents...\")\n",
    "        query_embedding = self.embedding_model.encode([query], convert_to_numpy=True).astype('float32')\n",
    "        _, indices = self.index.search(query_embedding, top_k)\n",
    "        retrieved_docs = [self.documents[idx] for idx in indices[0] if idx < len(self.documents)]\n",
    "        print(f\"[RAG System] -> Found {len(retrieved_docs)} relevant documents.\")\n",
    "        return retrieved_docs\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 3: UTILITIES AND MODEL CACHING\n",
    "# ==============================================================================\n",
    "model_cache = {\"router\": None, \"code\": None, \"math\": None, \"other\": None, \"rag\": None}\n",
    "\n",
    "class StopOnToken(StoppingCriteria):\n",
    "    def __init__(self, stop_token_id): self.stop_token_id = stop_token_id\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        return input_ids[0, -1] == self.stop_token_id\n",
    "\n",
    "def load_expert_model(expert_name):\n",
    "    if model_cache.get(expert_name): return model_cache[expert_name]\n",
    "    print(f\"\\n[System] Loading '{expert_name}' expert... (This may take a moment)\")\n",
    "\n",
    "    if expert_name == \"other\":\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(model_name=BASE_MODEL_PATH, max_seq_length=MAX_SEQ_LENGTH, dtype=None, load_in_4bit=True)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "    else:\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(model_name=BASE_MODEL_PATH, max_seq_length=MAX_SEQ_LENGTH, dtype=None, load_in_4bit=True)\n",
    "        lora_path = CODE_LORA_PATH if expert_name == \"code\" else MATH_LORA_PATH\n",
    "        if expert_name == \"code\":\n",
    "            tokenizer.add_special_tokens({\"additional_special_tokens\": [CODE_STOP_TOKEN]})\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        model.load_adapter(lora_path)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "\n",
    "    model_cache[expert_name] = (model, tokenizer)\n",
    "    print(f\"[System] \u2705 '{expert_name}' expert is now loaded.\")\n",
    "    return model_cache[expert_name]\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 4: INFERENCE AND FORMATTING FUNCTIONS\n",
    "# ==============================================================================\n",
    "def execute_expert_generation(expert_name, system_prompt, chat_history, rag_context=\"\"):\n",
    "    model, tokenizer = load_expert_model(expert_name)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    final_chat_history = list(chat_history)\n",
    "    if rag_context:\n",
    "        last_user_message = final_chat_history[-1]['content']\n",
    "        injected_prompt = f\"Use the following reference information to help answer the question:\\n---REFERENCE---\\n{rag_context}\\n---END REFERENCE---\\n\\nQuestion: {last_user_message}\"\n",
    "        final_chat_history[-1]['content'] = injected_prompt\n",
    "\n",
    "    if expert_name == \"math\":\n",
    "        prompt_text = f\"<|system|>\\n{system_prompt}\\n\\n\"\n",
    "        for message in final_chat_history:\n",
    "            prompt_text += f\"<|{message['role']}|>\\n{message['content']}\\n\\n\"\n",
    "        prompt_text += \"<|assistant|>\\n\"\n",
    "        inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    else:\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}] + final_chat_history\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "    stopping_criteria_list = StoppingCriteriaList([])\n",
    "    if expert_name == \"code\":\n",
    "        stop_token_id = tokenizer.convert_tokens_to_ids(CODE_STOP_TOKEN)\n",
    "        stopping_criteria_list.append(StopOnToken(stop_token_id))\n",
    "\n",
    "    generation_config = {\n",
    "        \"streamer\": streamer, \"max_new_tokens\": 2048, \"temperature\": 0.2, \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id, \"stopping_criteria\": stopping_criteria_list,\n",
    "    }\n",
    "    if expert_name == \"math\":\n",
    "        generation_config.update({\"temperature\": 0.3, \"top_p\": 0.9, \"repetition_penalty\": 1.05})\n",
    "\n",
    "    final_kwargs = {}\n",
    "    if isinstance(inputs, dict) or hasattr(inputs, 'keys'):\n",
    "        final_kwargs = inputs | generation_config\n",
    "    elif isinstance(inputs, torch.Tensor):\n",
    "        final_kwargs = {\"input_ids\": inputs} | generation_config\n",
    "    else:\n",
    "        raise TypeError(f\"Tokenizer returned an unexpected type: {type(inputs)}\")\n",
    "\n",
    "    outputs = model.generate(**final_kwargs)\n",
    "\n",
    "    prompt_length = inputs.shape[1] if isinstance(inputs, torch.Tensor) else inputs.input_ids.shape[1]\n",
    "    response_text = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\n",
    "    return response_text.replace(CODE_STOP_TOKEN, \"\").strip()\n",
    "\n",
    "def heal_and_reprint_if_needed(raw_response):\n",
    "    was_healed = False\n",
    "    healed_response = raw_response\n",
    "    if \"<thinking>\" in healed_response and \"</thinking>\" not in healed_response:\n",
    "        answer_pos = healed_response.find(\"<answer>\")\n",
    "        if answer_pos != -1:\n",
    "            healed_response = healed_response[:answer_pos] + \"</thinking>\\n\\n\" + healed_response[answer_pos:]\n",
    "            was_healed = True\n",
    "    if \"<answer>\" in healed_response and \"</answer>\" not in healed_response:\n",
    "        healed_response += \"\\n</answer>\"\n",
    "        was_healed = True\n",
    "    if was_healed:\n",
    "        print(\"\\n\" + \"=\"*40 + \" HEALED RESPONSE \" + \"=\"*40)\n",
    "        print(\"[SYSTEM] Original output was malformed. Displaying corrected version:\")\n",
    "        print(healed_response)\n",
    "        print(\"=\"*96)\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 5: MAIN EXECUTION LOOP\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing system...\")\n",
    "    try:\n",
    "        model_cache[\"router\"] = pipeline(\"text-classification\", model=ROUTER_PATH, device=0)\n",
    "        print(\"\u2705 Router loaded successfully.\")\n",
    "        model_cache[\"rag\"] = CodeKnowledgeRAG(knowledge_base_path=KNOWLEDGE_BASE_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c CRITICAL ERROR during initialization. Error: {e}\")\n",
    "        exit()\n",
    "\n",
    "    chat_history = []\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"\ud83d\ude80 CONVERSATIONAL Mixture-of-LoRAs + RAG Chatbot is running! \ud83d\ude80\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            user_input = input(\"\\n>> You: \")\n",
    "            if user_input.lower() in [\"exit\", \"quit\"]: break\n",
    "            if not user_input: continue\n",
    "\n",
    "            chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "            route_result = model_cache[\"router\"](user_input)[0]\n",
    "            expert_choice = route_result['label'].lower()\n",
    "            print(f\"\\n[Router] -> Decided: **{expert_choice.upper()}** (Confidence: {route_result['score']:.1%})\")\n",
    "\n",
    "            rag_context_str = \"\"\n",
    "            if expert_choice == \"code\":\n",
    "                rag_system = model_cache[\"rag\"]\n",
    "                retrieved_docs = rag_system.retrieve(user_input, top_k=2)\n",
    "                if retrieved_docs:\n",
    "                    rag_context_str = \"\\n\\n\".join(retrieved_docs)\n",
    "\n",
    "            start_time = time.time()\n",
    "            assistant_response = \"\"\n",
    "\n",
    "            if expert_choice == \"code\":\n",
    "                print(\"\ud83e\udd16 Code Expert (with RAG):\")\n",
    "                assistant_response = execute_expert_generation(\"code\", CODE_SYSTEM_PROMPT, chat_history, rag_context=rag_context_str)\n",
    "                print(f\"\\n(Expert generation time: {time.time() - start_time:.2f} seconds)\")\n",
    "                heal_and_reprint_if_needed(assistant_response)\n",
    "            elif expert_choice == \"math\":\n",
    "                print(\"\ud83e\udd16 Math Expert:\")\n",
    "                assistant_response = execute_expert_generation(\"math\", MATH_SYSTEM_PROMPT, chat_history)\n",
    "                print(f\"\\n(Expert generation time: {time.time() - start_time:.2f} seconds)\")\n",
    "            else:\n",
    "                print(\"\ud83e\udd16 General Assistant:\")\n",
    "                assistant_response = execute_expert_generation(\"other\", OTHER_SYSTEM_PROMPT, chat_history)\n",
    "                print(f\"\\n(Expert generation time: {time.time() - start_time:.2f} seconds)\")\n",
    "\n",
    "            chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExiting...\")\n",
    "\n",
    "    print(\"\\nChat session finished.\")"
   ],
   "id": "91d568a83ac0d6ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T05:14:42.223840Z",
     "start_time": "2025-11-15T05:12:24.816024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ultimate_chatbot_v3.py\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, StoppingCriteria, StoppingCriteriaList, TextStreamer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 1: MASTER CONFIGURATION\n",
    "# ==============================================================================\n",
    "# --- Paths (Update these if your folder structure is different) ---\n",
    "ROUTER_PATH = \"Classifier\"\n",
    "BASE_MODEL_PATH = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
    "CODE_LORA_PATH = \"Section-D/Universal-Code-Master/final_model\"\n",
    "MATH_LORA_PATH = \"Final-Dynamic-Model/final_model(Math)\"\n",
    "KNOWLEDGE_BASE_PATH = \"knowledge_base\"\n",
    "\n",
    "# --- System Prompts ---\n",
    "CODE_SYSTEM_PROMPT = \"\"\"You are an elite software engineer who writes syntactically perfect, logically sound code across all programming languages.\n",
    "\n",
    "MANDATORY THINKING PROCESS - You MUST use <thinking> tags before <answer>:\n",
    "\n",
    "Inside <thinking>:\n",
    "1. RESTATE THE PROBLEM: Paraphrase the task in your own words to confirm understanding\n",
    "2. IDENTIFY CONSTRAINTS: List all input/output specs, data types, time/space complexity requirements\n",
    "3. ENUMERATE EDGE CASES: Empty inputs, null values, negative numbers, zero, boundary conditions, duplicates, special characters\n",
    "4. COMPARE APPROACHES: Analyze 2-3 different algorithms with their time/space complexity\n",
    "5. CHOOSE OPTIMAL APPROACH: Select the best algorithm and justify why (correctness, efficiency, readability)\n",
    "6. PLAN IMPLEMENTATION: Write pseudocode or step-by-step logic flow\n",
    "7. ANTICIPATE BUGS: Think through off-by-one errors, integer overflow, null pointer issues, index out of bounds\n",
    "\n",
    "Inside <answer>:\n",
    "- Write ONLY the complete, runnable code\n",
    "- Use proper syntax (correct indentation, matching braces, semicolons where needed)\n",
    "- Handle ALL edge cases explicitly in code\n",
    "- Use meaningful variable names\n",
    "- Add minimal inline comments only for complex logic\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- ALWAYS use <thinking> tags for your reasoning process\n",
    "- ALWAYS use <answer> tags for the final code\n",
    "- Code must be syntactically correct (no errors, proper formatting)\n",
    "- Code must be logically sound (handles edge cases, correct algorithm)\n",
    "- Code must be production-ready (no TODOs, no placeholder logic)\n",
    "\n",
    "LANGUAGE-SPECIFIC RULES:\n",
    "- Python: 4-space indentation, type hints, PEP 8 compliance\n",
    "- JavaScript: const/let (no var), proper semicolons, ES6+ syntax\n",
    "- C++: STL containers, RAII, proper memory management, const correctness\n",
    "- Java: Proper access modifiers, exception handling, naming conventions\n",
    "\n",
    "EDGE CASE CHECKLIST (verify in <thinking>):\n",
    "\u2713 Empty collection (list/array/string)\n",
    "\u2713 Single element\n",
    "\u2713 Null/None/undefined values\n",
    "\u2713 Negative numbers (if applicable)\n",
    "\u2713 Zero\n",
    "\u2713 Maximum/minimum integer values\n",
    "\u2713 Duplicate elements\n",
    "\u2713 Already sorted/reverse sorted (for sorting problems)\n",
    "\u2713 Invalid input types\"\"\"\n",
    "\n",
    "MATH_SYSTEM_PROMPT = \"\"\"You are a helpful assistant who thinks step by step through problems. When solving questions, show your reasoning process clearly using <think> tags, work through each step methodically, and then provide a clear final answer.\"\"\"\n",
    "OTHER_SYSTEM_PROMPT = \"\"\"You are a helpful and friendly AI assistant. Answer the user's question clearly and concisely.\"\"\"\n",
    "\n",
    "# --- Shared Configuration ---\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "CODE_STOP_TOKEN = \"[END]\"\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 2: RAG SYSTEM IMPLEMENTATION\n",
    "# ==============================================================================\n",
    "class CodeKnowledgeRAG:\n",
    "    \"\"\"Retrieval system over your code knowledge base.\"\"\"\n",
    "    def __init__(self, knowledge_base_path):\n",
    "        print(\"[RAG System] Initializing...\")\n",
    "        self.knowledge_base_path = Path(knowledge_base_path)\n",
    "        if not self.knowledge_base_path.exists():\n",
    "            print(f\"[RAG System] \u26a0\ufe0f  Warning: Knowledge base path '{knowledge_base_path}' does not exist. Creating it.\")\n",
    "            self.knowledge_base_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
    "        self.documents = []\n",
    "        self.index = None\n",
    "        self._load_and_build()\n",
    "        if not self.documents:\n",
    "            print(\"[RAG System] \u26a0\ufe0f  Warning: No .txt files found in knowledge base. RAG will not retrieve any context.\")\n",
    "        else:\n",
    "            print(f\"[RAG System] \u2705 Ready. Loaded {len(self.documents)} document sections.\")\n",
    "\n",
    "    def _load_and_build(self):\n",
    "        kb_files = sorted(self.knowledge_base_path.glob(\"*.txt\"))\n",
    "        for file_path in kb_files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f: content = f.read()\n",
    "            sections = content.split('\\n\\n')\n",
    "            for section in sections:\n",
    "                if section.strip(): self.documents.append(section.strip())\n",
    "        if not self.documents: return\n",
    "\n",
    "        embeddings = self.embedding_model.encode(self.documents, show_progress_bar=False, convert_to_numpy=True)\n",
    "        dim = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "\n",
    "    def retrieve(self, query, top_k=2):\n",
    "        if not self.index or not self.documents: return []\n",
    "        print(f\"[RAG System] Retrieving top-{top_k} documents...\")\n",
    "        query_embedding = self.embedding_model.encode([query], convert_to_numpy=True).astype('float32')\n",
    "        _, indices = self.index.search(query_embedding, top_k)\n",
    "        retrieved_docs = [self.documents[idx] for idx in indices[0] if idx < len(self.documents)]\n",
    "        print(f\"[RAG System] -> Found {len(retrieved_docs)} relevant documents.\")\n",
    "        return retrieved_docs\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 3: UTILITIES AND MODEL CACHING\n",
    "# ==============================================================================\n",
    "model_cache = {\"router\": None, \"code\": None, \"math\": None, \"other\": None, \"rag\": None}\n",
    "\n",
    "class StopOnToken(StoppingCriteria):\n",
    "    def __init__(self, stop_token_id): self.stop_token_id = stop_token_id\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        return input_ids[0, -1] == self.stop_token_id\n",
    "\n",
    "def load_expert_model(expert_name):\n",
    "    \"\"\"Lazy-loads an expert model into the cache the first time it's needed.\"\"\"\n",
    "    if model_cache.get(expert_name): return model_cache[expert_name]\n",
    "    print(f\"\\n[System] Loading '{expert_name}' expert... (This may take a moment)\")\n",
    "\n",
    "    if expert_name == \"other\":\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(model_name=BASE_MODEL_PATH, max_seq_length=MAX_SEQ_LENGTH, dtype=None, load_in_4bit=True)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "    else:\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(model_name=BASE_MODEL_PATH, max_seq_length=MAX_SEQ_LENGTH, dtype=None, load_in_4bit=True)\n",
    "        lora_path = CODE_LORA_PATH if expert_name == \"code\" else MATH_LORA_PATH\n",
    "        if expert_name == \"code\":\n",
    "            tokenizer.add_special_tokens({\"additional_special_tokens\": [CODE_STOP_TOKEN]})\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        model.load_adapter(lora_path)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "\n",
    "    model_cache[expert_name] = (model, tokenizer)\n",
    "    print(f\"[System] \u2705 '{expert_name}' expert is now loaded.\")\n",
    "    return model_cache[expert_name]\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 4: INFERENCE AND FORMATTING FUNCTIONS\n",
    "# ==============================================================================\n",
    "def execute_expert_generation(expert_name, system_prompt, chat_history, rag_context=\"\"):\n",
    "    \"\"\"A generic function to handle generation for any expert.\"\"\"\n",
    "    model, tokenizer = load_expert_model(expert_name)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    final_chat_history = list(chat_history)\n",
    "    if rag_context:\n",
    "        last_user_message = final_chat_history[-1]['content']\n",
    "        injected_prompt = f\"Use the following reference information to help answer the question:\\n---REFERENCE---\\n{rag_context}\\n---END REFERENCE---\\n\\nQuestion: {last_user_message}\"\n",
    "        final_chat_history[-1]['content'] = injected_prompt\n",
    "\n",
    "    if expert_name == \"math\":\n",
    "        prompt_text = f\"<|system|>\\n{system_prompt}\\n\\n\"\n",
    "        for message in final_chat_history:\n",
    "            prompt_text += f\"<|{message['role']}|>\\n{message['content']}\\n\\n\"\n",
    "        prompt_text += \"<|assistant|>\\n\"\n",
    "        inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    else:\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}] + final_chat_history\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "    stopping_criteria_list = StoppingCriteriaList([])\n",
    "    if expert_name == \"code\":\n",
    "        stop_token_id = tokenizer.convert_tokens_to_ids(CODE_STOP_TOKEN)\n",
    "        stopping_criteria_list.append(StopOnToken(stop_token_id))\n",
    "\n",
    "    generation_config = {\n",
    "        \"streamer\": streamer, \"max_new_tokens\": 2048, \"temperature\": 0.2, \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id, \"stopping_criteria\": stopping_criteria_list,\n",
    "    }\n",
    "    if expert_name == \"math\":\n",
    "        generation_config.update({\"temperature\": 0.3, \"top_p\": 0.9, \"repetition_penalty\": 1.05})\n",
    "\n",
    "    final_kwargs = {}\n",
    "    if isinstance(inputs, dict) or hasattr(inputs, 'keys'):\n",
    "        final_kwargs = inputs | generation_config\n",
    "    elif isinstance(inputs, torch.Tensor):\n",
    "        final_kwargs = {\"input_ids\": inputs} | generation_config\n",
    "    else:\n",
    "        raise TypeError(f\"Tokenizer returned an unexpected type: {type(inputs)}\")\n",
    "\n",
    "    outputs = model.generate(**final_kwargs)\n",
    "\n",
    "    prompt_length = inputs.shape[1] if isinstance(inputs, torch.Tensor) else inputs.input_ids.shape[1]\n",
    "    response_text = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\n",
    "    return response_text.replace(CODE_STOP_TOKEN, \"\").strip()\n",
    "\n",
    "def heal_and_reprint_if_needed(raw_response):\n",
    "    \"\"\"Checks the response for closing tags. If any are missing, it prints a healed version.\"\"\"\n",
    "    was_healed = False\n",
    "    healed_response = raw_response\n",
    "    if \"<thinking>\" in healed_response and \"</thinking>\" not in healed_response:\n",
    "        answer_pos = healed_response.find(\"<answer>\")\n",
    "        if answer_pos != -1:\n",
    "            healed_response = healed_response[:answer_pos] + \"</thinking>\\n\\n\" + healed_response[answer_pos:]\n",
    "            was_healed = True\n",
    "    if \"<answer>\" in healed_response and \"</answer>\" not in healed_response:\n",
    "        healed_response += \"\\n</answer>\"\n",
    "        was_healed = True\n",
    "    if was_healed:\n",
    "        print(\"\\n\" + \"=\"*40 + \" HEALED RESPONSE \" + \"=\"*40)\n",
    "        print(\"[SYSTEM] Original output was malformed. Displaying corrected version:\")\n",
    "        print(healed_response)\n",
    "        print(\"=\"*96)\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 5: MAIN EXECUTION LOOP\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing system...\")\n",
    "    try:\n",
    "        model_cache[\"router\"] = pipeline(\"text-classification\", model=ROUTER_PATH, device=0)\n",
    "        print(\"\u2705 Router loaded successfully.\")\n",
    "        model_cache[\"rag\"] = CodeKnowledgeRAG(knowledge_base_path=KNOWLEDGE_BASE_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c CRITICAL ERROR during initialization. Error: {e}\")\n",
    "        exit()\n",
    "\n",
    "    chat_history = []\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"\ud83d\ude80 ADVANCED Mixture-of-LoRAs + RAG Chatbot is running! \ud83d\ude80\")\n",
    "    print(\"   You can override the router by ending your prompt with --code, --math, or --other\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            user_input_raw = input(\"\\n>> You: \")\n",
    "            if user_input_raw.lower() in [\"exit\", \"quit\"]: break\n",
    "            if not user_input_raw: continue\n",
    "\n",
    "            user_input = user_input_raw\n",
    "            expert_choice = None\n",
    "            override_flags = {\"--code\": \"code\", \"--math\": \"math\", \"--other\": \"other\"}\n",
    "\n",
    "            for flag, expert in override_flags.items():\n",
    "                if user_input.lower().strip().endswith(flag):\n",
    "                    expert_choice = expert\n",
    "                    user_input = user_input[:-len(flag)].strip()\n",
    "                    print(f\"\\n[User Override] -> Routing to **{expert_choice.upper()}** expert.\")\n",
    "                    break\n",
    "\n",
    "            if expert_choice is None:\n",
    "                route_result = model_cache[\"router\"](user_input)[0]\n",
    "                expert_choice = route_result['label'].lower()\n",
    "                print(f\"\\n[Router] -> Decided: **{expert_choice.upper()}** (Confidence: {route_result['score']:.1%})\")\n",
    "\n",
    "            chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            rag_context_str = \"\"\n",
    "            if expert_choice == \"code\":\n",
    "                rag_system = model_cache[\"rag\"]\n",
    "                retrieved_docs = rag_system.retrieve(user_input, top_k=2)\n",
    "                if retrieved_docs:\n",
    "                    rag_context_str = \"\\n\\n\".join(retrieved_docs)\n",
    "\n",
    "            start_time = time.time()\n",
    "            assistant_response = \"\"\n",
    "\n",
    "            if expert_choice == \"code\":\n",
    "                print(\"\ud83e\udd16 Code Expert (with RAG):\")\n",
    "                assistant_response = execute_expert_generation(\"code\", CODE_SYSTEM_PROMPT, chat_history, rag_context=rag_context_str)\n",
    "                heal_and_reprint_if_needed(assistant_response)\n",
    "            elif expert_choice == \"math\":\n",
    "                print(\"\ud83e\udd16 Math Expert:\")\n",
    "                assistant_response = execute_expert_generation(\"math\", MATH_SYSTEM_PROMPT, chat_history)\n",
    "            else:\n",
    "                print(\"\ud83e\udd16 General Assistant:\")\n",
    "                assistant_response = execute_expert_generation(\"other\", OTHER_SYSTEM_PROMPT, chat_history)\n",
    "\n",
    "            print(f\"\\n(Expert generation time: {time.time() - start_time:.2f} seconds)\")\n",
    "\n",
    "            chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExiting...\")\n",
    "\n",
    "    print(\"\\nChat session finished.\")"
   ],
   "id": "da7bad459756d43c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "/tmp/ipykernel_8709/3101565084.py:11: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83e\udda5 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "\ud83e\udda5 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing system...\n",
      "\u2705 Router loaded successfully.\n",
      "[RAG System] Initializing...\n",
      "[RAG System] \u2705 Ready. Loaded 19 document sections.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\ud83d\ude80 ADVANCED Mixture-of-LoRAs + RAG Chatbot is running! \ud83d\ude80\n",
      "   You can override the router by ending your prompt with --code, --math, or --other\n",
      "================================================================================\n",
      "\n",
      "[User Override] -> Routing to **MATH** expert.\n",
      "--------------------------------------------------\n",
      "\ud83e\udd16 Math Expert:\n",
      "\n",
      "[System] Loading 'math' expert... (This may take a moment)\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[System] \u2705 'math' expert is now loaded.\n",
      "<think>Okay, the user just sent a message that starts with \"Hi\". I need to respond. Let me think about how to start. Maybe something friendly but not too casual since it's a greeting. Hmm, maybe a simple \"Hello!\" is good. Wait, they didn't specify anything else, so I should keep it neutral. Yeah, that works. No need for any additional context or questions. Just a basic hello. Let me check if there's any other possible responses. Maybe a greeting with a question like \"How are you?\" but that might be too formal. So \"Hello\" is straightforward. I don't want to overthink it. Yes, that's the best approach. Keep it short and polite. That should cover the basics.</think>\n",
      "<answer>Hello!</answer>\n",
      "\n",
      "(Expert generation time: 25.25 seconds)\n",
      "\n",
      "[Router] -> Decided: **CODE** (Confidence: 99.6%)\n",
      "--------------------------------------------------\n",
      "[RAG System] Retrieving top-2 documents...\n",
      "[RAG System] -> Found 2 relevant documents.\n",
      "\ud83e\udd16 Code Expert (with RAG):\n",
      "\n",
      "[System] Loading 'code' expert... (This may take a moment)\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[System] \u2705 'code' expert is now loaded.\n",
      "<think>\n",
      "The task is to write a Python function that prints \"Hello\". This is a very basic task that doesn't require any complex logic or file operations. It's a straightforward print statement.\n",
      "\n",
      "Two possible approaches are:\n",
      "1. Direct print statement: `print(\"Hello\")`\n",
      "2. Using a function: `def hello_print(): print(\"Hello\")`\n",
      "\n",
      "Both approaches are simple and straightforward. The direct print statement is more concise and directly achieves the task. The function approach is useful if we want to reuse the \"Hello\" message in different parts of the code.\n",
      "\n",
      "Time complexity: Both approaches are O(1) because they involve a constant amount of work.\n",
      "Space complexity: Both approaches are O(1) because they don't use any additional space that scales with input size.\n",
      "\n",
      "The best approach is to use a direct print statement because it's concise and directly achieves the task. We don't need to create a function for a simple print statement like this.\n",
      "\n",
      "Edge cases:\n",
      "- Empty input: The function should still print \"Hello\".\n",
      "- None input: The function should still print \"Hello\".\n",
      "</think>\n",
      "<answer>\n",
      "```python\n",
      "# Direct print statement\n",
      "print(\"Hello\")\n",
      "\n",
      "# Using a function\n",
      "def hello_print():\n",
      "    print(\"Hello\")\n",
      "</answer>\n",
      "[END]\n",
      "\n",
      "(Expert generation time: 24.41 seconds)\n",
      "\n",
      "[User Override] -> Routing to **OTHER** expert.\n",
      "--------------------------------------------------\n",
      "\ud83e\udd16 General Assistant:\n",
      "\n",
      "[System] Loading 'other' expert... (This may take a moment)\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[System] \u2705 'other' expert is now loaded.\n",
      "Your first two messages were:\n",
      "\n",
      "1. \"Hi\"\n",
      "2. You asked me to write the Python code to print \"Hello\".\n",
      "\n",
      "(Expert generation time: 14.50 seconds)\n",
      "\n",
      "\n",
      "Exiting...\n",
      "\n",
      "Chat session finished.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T05:59:42.066263Z",
     "start_time": "2025-11-15T05:52:29.154793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ultimate_chatbot_v5.py\n",
    "\n",
    "import unsloth\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, StoppingCriteria, StoppingCriteriaList, TextStreamer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 1: MASTER CONFIGURATION\n",
    "# ==============================================================================\n",
    "# --- Paths ---\n",
    "ROUTER_PATH = \"Classifier\"\n",
    "BASE_MODEL_PATH = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
    "CODE_LORA_PATH = \"Section-D/Universal-Code-Master/final_model\"\n",
    "MATH_LORA_PATH = \"Final-Dynamic-Model/final_model(Math)\"\n",
    "\n",
    "# --- Knowledge Base Paths ---\n",
    "CODE_KB_PATH = \"knowledge_base/Base\"\n",
    "OTHER_KB_PATH = \"knowledge_base/Code\"\n",
    "\n",
    "# --- System Prompts ---\n",
    "CODE_SYSTEM_PROMPT = \"\"\"You are an elite software engineer who writes syntactically perfect, logically sound code across all programming languages.\n",
    "\n",
    "MANDATORY THINKING PROCESS - You MUST use <thinking> tags before <answer>:\n",
    "\n",
    "Inside <thinking>:\n",
    "1. RESTATE THE PROBLEM: Paraphrase the task in your own words to confirm understanding\n",
    "2. IDENTIFY CONSTRAINTS: List all input/output specs, data types, time/space complexity requirements\n",
    "3. ENUMERATE EDGE CASES: Empty inputs, null values, negative numbers, zero, boundary conditions, duplicates, special characters\n",
    "4. COMPARE APPROACHES: Analyze 2-3 different algorithms with their time/space complexity\n",
    "5. CHOOSE OPTIMAL APPROACH: Select the best algorithm and justify why (correctness, efficiency, readability)\n",
    "6. PLAN IMPLEMENTATION: Write pseudocode or step-by-step logic flow\n",
    "7. ANTICIPATE BUGS: Think through off-by-one errors, integer overflow, null pointer issues, index out of bounds\n",
    "\n",
    "Inside <answer>:\n",
    "- Write ONLY the complete, runnable code\n",
    "- Use proper syntax (correct indentation, matching braces, semicolons where needed)\n",
    "- Handle ALL edge cases explicitly in code\n",
    "- Use meaningful variable names\n",
    "- Add minimal inline comments only for complex logic\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- ALWAYS use <thinking> tags for your reasoning process\n",
    "- ALWAYS use <answer> tags for the final code\n",
    "- Code must be syntactically correct (no errors, proper formatting)\n",
    "- Code must be logically sound (handles edge cases, correct algorithm)\n",
    "- Code must be production-ready (no TODOs, no placeholder logic)\n",
    "\n",
    "LANGUAGE-SPECIFIC RULES:\n",
    "- Python: 4-space indentation, type hints, PEP 8 compliance\n",
    "- JavaScript: const/let (no var), proper semicolons, ES6+ syntax\n",
    "- C++: STL containers, RAII, proper memory management, const correctness\n",
    "- Java: Proper access modifiers, exception handling, naming conventions\n",
    "\n",
    "EDGE CASE CHECKLIST (verify in <thinking>):\n",
    "\u2713 Empty collection (list/array/string)\n",
    "\u2713 Single element\n",
    "\u2713 Null/None/undefined values\n",
    "\u2713 Negative numbers (if applicable)\n",
    "\u2713 Zero\n",
    "\u2713 Maximum/minimum integer values\n",
    "\u2713 Duplicate elements\n",
    "\u2713 Already sorted/reverse sorted (for sorting problems)\n",
    "\u2713 Invalid input types\"\"\"\n",
    "\n",
    "MATH_SYSTEM_PROMPT = \"\"\"You are a helpful assistant who thinks step by step through problems. When solving questions, show your reasoning process clearly using <think> tags, work through each step methodically, and then provide a clear final answer.\"\"\"\n",
    "OTHER_SYSTEM_PROMPT = \"\"\"You are a helpful and friendly AI assistant. Use the provided reference information to answer the user's question accurately.\"\"\"\n",
    "\n",
    "# --- Shared Configuration ---\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "CODE_STOP_TOKEN = \"[END]\"\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 2: RAG AND UTILITIES\n",
    "# ==============================================================================\n",
    "model_cache = {\"router\": None, \"code\": None, \"math\": None, \"other\": None, \"rag_code\": None, \"rag_other\": None}\n",
    "\n",
    "class KnowledgeRAG:\n",
    "    \"\"\"A generic retrieval system for a specific knowledge base path.\"\"\"\n",
    "    def __init__(self, name, knowledge_base_path):\n",
    "        print(f\"[{name} RAG System] Initializing...\")\n",
    "        self.name = name\n",
    "        self.knowledge_base_path = Path(knowledge_base_path)\n",
    "        if not self.knowledge_base_path.exists():\n",
    "            print(f\"[{self.name} RAG System] \u26a0\ufe0f  Warning: Path '{knowledge_base_path}' does not exist. Creating it.\")\n",
    "            self.knowledge_base_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
    "        self.documents = []\n",
    "        self.index = None\n",
    "        self._load_and_build()\n",
    "        if not self.documents:\n",
    "            print(f\"[{self.name} RAG System] \u26a0\ufe0f  Warning: No .txt files found. RAG will be inactive for this expert.\")\n",
    "        else:\n",
    "            print(f\"[{self.name} RAG System] \u2705 Ready. Loaded {len(self.documents)} document sections.\")\n",
    "\n",
    "    def _load_and_build(self):\n",
    "        kb_files = sorted(self.knowledge_base_path.glob(\"*.txt\"))\n",
    "        for file_path in kb_files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f: content = f.read()\n",
    "            sections = content.split('\\n\\n')\n",
    "            for section in sections:\n",
    "                if section.strip(): self.documents.append(section.strip())\n",
    "        if not self.documents: return\n",
    "\n",
    "        embeddings = self.embedding_model.encode(self.documents, show_progress_bar=False, convert_to_numpy=True)\n",
    "        dim = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "\n",
    "    def retrieve(self, query, top_k=2):\n",
    "        if not self.index or not self.documents: return []\n",
    "        print(f\"[{self.name} RAG System] Retrieving top-{top_k} documents...\")\n",
    "        query_embedding = self.embedding_model.encode([query], convert_to_numpy=True).astype('float32')\n",
    "        _, indices = self.index.search(query_embedding, top_k)\n",
    "        retrieved_docs = [self.documents[idx] for idx in indices[0] if idx < len(self.documents)]\n",
    "        print(f\"[{self.name} RAG System] -> Found {len(retrieved_docs)} relevant documents.\")\n",
    "        return retrieved_docs\n",
    "\n",
    "class StopOnToken(StoppingCriteria):\n",
    "    def __init__(self, stop_token_id): self.stop_token_id = stop_token_id\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        return input_ids[0, -1] == self.stop_token_id\n",
    "\n",
    "def load_expert_model(expert_name):\n",
    "    \"\"\"Lazy-loads an expert model into the cache the first time it's needed.\"\"\"\n",
    "    if model_cache.get(expert_name): return model_cache[expert_name]\n",
    "    print(f\"\\n[System] Loading '{expert_name}' expert... (This may take a moment)\")\n",
    "\n",
    "    if expert_name == \"other\":\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(model_name=BASE_MODEL_PATH, max_seq_length=MAX_SEQ_LENGTH, dtype=None, load_in_4bit=True)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "    else:\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(model_name=BASE_MODEL_PATH, max_seq_length=MAX_SEQ_LENGTH, dtype=None, load_in_4bit=True)\n",
    "        lora_path = CODE_LORA_PATH if expert_name == \"code\" else MATH_LORA_PATH\n",
    "        if expert_name == \"code\":\n",
    "            tokenizer.add_special_tokens({\"additional_special_tokens\": [CODE_STOP_TOKEN]})\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        model.load_adapter(lora_path)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "\n",
    "    model_cache[expert_name] = (model, tokenizer)\n",
    "    print(f\"[System] \u2705 '{expert_name}' expert is now loaded.\")\n",
    "    return model_cache[expert_name]\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 3: ISOLATED EXPERT HANDLERS\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "#  STEP 3: ISOLATED EXPERT HANDLERS (DEFINITIVE FIX)\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "#  STEP 3: ISOLATED EXPERT HANDLERS (DEFINITIVE FIX)\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "#  STEP 3: ISOLATED EXPERT HANDLERS (DEFINITIVE FINAL FIX)\n",
    "# ==============================================================================\n",
    "def handle_code_query(chat_history, rag_context=\"\"):\n",
    "    model, tokenizer = load_expert_model(\"code\")\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    injected_history = list(chat_history)\n",
    "    if rag_context:\n",
    "        last_user_message = injected_history[-1]['content']\n",
    "        injected_prompt = f\"Use the following reference information:\\n---REFERENCE---\\n{rag_context}\\n---END REFERENCE---\\n\\nQuestion: {last_user_message}\"\n",
    "        injected_history[-1]['content'] = injected_prompt\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": CODE_SYSTEM_PROMPT}] + injected_history\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    stop_token_id = tokenizer.convert_tokens_to_ids(CODE_STOP_TOKEN)\n",
    "    stopping_criteria = StoppingCriteriaList([StopOnToken(stop_token_id)])\n",
    "\n",
    "    generation_config = {\n",
    "        \"streamer\": streamer, \"max_new_tokens\": 2048, \"temperature\": 0.2,\n",
    "        \"do_sample\": True, \"stopping_criteria\": stopping_criteria,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "\n",
    "    # --- FINAL FIX: Robustly create final_kwargs ---\n",
    "    if isinstance(inputs, dict) or hasattr(inputs, 'keys'): final_kwargs = inputs | generation_config\n",
    "    else: final_kwargs = {\"input_ids\": inputs} | generation_config\n",
    "\n",
    "    outputs = model.generate(**final_kwargs)\n",
    "\n",
    "    prompt_length = inputs.shape[1] if isinstance(inputs, torch.Tensor) else inputs.input_ids.shape[1]\n",
    "    response_text = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\n",
    "    clean_response = response_text.replace(CODE_STOP_TOKEN, \"\").strip()\n",
    "    heal_and_reprint_if_needed(clean_response)\n",
    "    return clean_response\n",
    "\n",
    "def handle_math_query(chat_history):\n",
    "    model, tokenizer = load_expert_model(\"math\")\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    prompt_text = f\"<|system|>\\n{MATH_SYSTEM_PROMPT}\\n\\n\"\n",
    "    for message in chat_history:\n",
    "        prompt_text += f\"<|{message['role']}|>\\n{message['content']}\\n\\n\"\n",
    "    prompt_text += \"<|assistant|>\\n\"\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    generation_config = {\n",
    "        \"streamer\": streamer, \"max_new_tokens\": 1024, \"temperature\": 0.3,\n",
    "        \"top_p\": 0.9, \"do_sample\": True, \"repetition_penalty\": 1.05,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "\n",
    "    # --- FINAL FIX: Robustly create final_kwargs ---\n",
    "    if isinstance(inputs, dict) or hasattr(inputs, 'keys'): final_kwargs = inputs | generation_config\n",
    "    else: final_kwargs = {\"input_ids\": inputs} | generation_config\n",
    "\n",
    "    outputs = model.generate(**final_kwargs)\n",
    "\n",
    "    prompt_length = inputs.shape[1] if isinstance(inputs, torch.Tensor) else inputs.input_ids.shape[1]\n",
    "    response_text = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\n",
    "    return response_text.strip()\n",
    "\n",
    "def handle_other_query(chat_history, rag_context=\"\"):\n",
    "    model, tokenizer = load_expert_model(\"other\")\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    injected_history = list(chat_history)\n",
    "    if rag_context:\n",
    "        last_user_message = injected_history[-1]['content']\n",
    "        injected_prompt = f\"Use the following reference information:\\n---REFERENCE---\\n{rag_context}\\n---END REFERENCE---\\n\\nQuestion: {last_user_message}\"\n",
    "        injected_history[-1]['content'] = injected_prompt\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": OTHER_SYSTEM_PROMPT}] + injected_history\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    generation_config = {\n",
    "        \"streamer\": streamer, \"max_new_tokens\": 1024, \"temperature\": 0.7,\n",
    "        \"do_sample\": True, \"pad_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "\n",
    "    # --- FINAL FIX: Robustly create final_kwargs ---\n",
    "    if isinstance(inputs, dict) or hasattr(inputs, 'keys'): final_kwargs = inputs | generation_config\n",
    "    else: final_kwargs = {\"input_ids\": inputs} | generation_config\n",
    "\n",
    "    outputs = model.generate(**final_kwargs)\n",
    "\n",
    "    prompt_length = inputs.shape[1] if isinstance(inputs, torch.Tensor) else inputs.input_ids.shape[1]\n",
    "    response_text = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\n",
    "    return response_text.strip()\n",
    "\n",
    "def heal_and_reprint_if_needed(raw_response):\n",
    "    was_healed = False\n",
    "    healed_response = raw_response\n",
    "    if \"<thinking>\" in healed_response and \"</thinking>\" not in healed_response:\n",
    "        answer_pos = healed_response.find(\"<answer>\")\n",
    "        if answer_pos != -1:\n",
    "            healed_response = healed_response[:answer_pos] + \"</thinking>\\n\\n\" + healed_response[answer_pos:]\n",
    "            was_healed = True\n",
    "    if \"<answer>\" in healed_response and \"</answer>\" not in healed_response:\n",
    "        healed_response += \"\\n</answer>\"\n",
    "        was_healed = True\n",
    "    if was_healed:\n",
    "        print(\"\\n\" + \"=\"*40 + \" HEALED RESPONSE \" + \"=\"*40)\n",
    "        print(\"[SYSTEM] Original output was malformed. Displaying corrected version:\")\n",
    "        print(healed_response)\n",
    "        print(\"=\"*96)\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 4: MAIN EXECUTION LOOP\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing system...\")\n",
    "    try:\n",
    "        model_cache[\"router\"] = pipeline(\"text-classification\", model=ROUTER_PATH, device=0)\n",
    "        print(\"\u2705 Router loaded successfully.\")\n",
    "        model_cache[\"rag_code\"] = KnowledgeRAG(\"Code\", knowledge_base_path=CODE_KB_PATH)\n",
    "        model_cache[\"rag_other\"] = KnowledgeRAG(\"Other\", knowledge_base_path=OTHER_KB_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c CRITICAL ERROR during initialization. Error: {e}\")\n",
    "        exit()\n",
    "\n",
    "    chat_history = []\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"\ud83d\ude80 ULTIMATE Mixture-of-LoRAs + Multi-RAG Chatbot is running! \ud83d\ude80\")\n",
    "    print(\"   You can override the router by ending your prompt with --code, --math, or --other\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            user_input_raw = input(\"\\n>> You: \")\n",
    "            if user_input_raw.lower() in [\"exit\", \"quit\"]: break\n",
    "            if not user_input_raw: continue\n",
    "\n",
    "            user_input = user_input_raw\n",
    "            expert_choice = None\n",
    "            override_flags = {\"--code\": \"code\", \"--math\": \"math\", \"--other\": \"other\"}\n",
    "\n",
    "            for flag, expert in override_flags.items():\n",
    "                if user_input.lower().strip().endswith(flag):\n",
    "                    expert_choice = expert\n",
    "                    user_input = user_input[:-len(flag)].strip()\n",
    "                    print(f\"\\n[User Override] -> Routing to **{expert_choice.upper()}** expert.\")\n",
    "                    break\n",
    "\n",
    "            if expert_choice is None:\n",
    "                route_result = model_cache[\"router\"](user_input)[0]\n",
    "                expert_choice = route_result['label'].lower()\n",
    "                print(f\"\\n[Router] -> Decided: **{expert_choice.upper()}** (Confidence: {route_result['score']:.1%})\")\n",
    "\n",
    "            chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            rag_context_str = \"\"\n",
    "            if expert_choice == \"code\":\n",
    "                rag_system = model_cache[\"rag_code\"]\n",
    "                retrieved_docs = rag_system.retrieve(user_input, top_k=2)\n",
    "                if retrieved_docs: rag_context_str = \"\\n\\n\".join(retrieved_docs)\n",
    "            elif expert_choice == \"other\":\n",
    "                rag_system = model_cache[\"rag_other\"]\n",
    "                retrieved_docs = rag_system.retrieve(user_input, top_k=3)\n",
    "                if retrieved_docs: rag_context_str = \"\\n\\n\".join(retrieved_docs)\n",
    "\n",
    "            start_time = time.time()\n",
    "            assistant_response = \"\"\n",
    "\n",
    "            print(f\"\ud83e\udd16 {expert_choice.capitalize()} Expert:\")\n",
    "            if expert_choice == \"code\":\n",
    "                assistant_response = handle_code_query(chat_history, rag_context=rag_context_str)\n",
    "            elif expert_choice == \"math\":\n",
    "                assistant_response = handle_math_query(chat_history)\n",
    "            else: # 'other'\n",
    "                assistant_response = handle_other_query(chat_history, rag_context=rag_context_str)\n",
    "\n",
    "            print(f\"\\n(Expert generation time: {time.time() - start_time:.2f} seconds)\")\n",
    "            chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExiting...\")\n",
    "\n",
    "    print(\"\\nChat session finished.\")"
   ],
   "id": "2616ba329e451f56",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing system...\n",
      "\u2705 Router loaded successfully.\n",
      "[Code RAG System] Initializing...\n",
      "[Code RAG System] \u2705 Ready. Loaded 40 document sections.\n",
      "[Other RAG System] Initializing...\n",
      "[Other RAG System] \u2705 Ready. Loaded 19 document sections.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\ud83d\ude80 ULTIMATE Mixture-of-LoRAs + Multi-RAG Chatbot is running! \ud83d\ude80\n",
      "   You can override the router by ending your prompt with --code, --math, or --other\n",
      "================================================================================\n",
      "\n",
      "[User Override] -> Routing to **MATH** expert.\n",
      "--------------------------------------------------\n",
      "\ud83e\udd16 Math Expert:\n",
      "\n",
      "[System] Loading 'math' expert... (This may take a moment)\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[System] \u2705 'math' expert is now loaded.\n",
      "<think>Okay, the user just sent a message with \"Hi\". Let me think about how to respond. The user is initiating a conversation, so I should start with a friendly greeting. Maybe something like \"Hello! How's your day going?\" or \"Hi! What's new with you?\" Keep it simple and friendly. Also, make sure the response is concise but not too short. Maybe add a bit of curiosity to encourage them to share more. Let me check if that's a good approach. Hmm, maybe also consider adding a question to keep the conversation flowing. Yeah, that sounds right. So the response would be a greeting followed by a question. Let me write that down.</think>\n",
      "<answer>Hello! How's your day going? Want to chat about anything in particular or just shoot some small talk?</answer>\n",
      "\n",
      "(Expert generation time: 24.50 seconds)\n",
      "\n",
      "[Router] -> Decided: **CODE** (Confidence: 99.6%)\n",
      "--------------------------------------------------\n",
      "[Code RAG System] Retrieving top-2 documents...\n",
      "[Code RAG System] -> Found 2 relevant documents.\n",
      "\ud83e\udd16 Code Expert:\n",
      "\n",
      "[System] Loading 'code' expert... (This may take a moment)\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[System] \u2705 'code' expert is now loaded.\n",
      "<think>\n",
      "- The task is to write a Python function that simply prints \"hello\".\n",
      "- Two possible approaches are: \n",
      "  1. Directly using the `print()` function with the string \"hello\".\n",
      "  2. Creating a function that takes no arguments and calls `print(\"hello\")`.\n",
      "- For the first approach, the time complexity is O(1) because it involves a constant number of operations, and the space complexity is also O(1) as it only uses a constant amount of space.\n",
      "- For the second approach, the time complexity remains O(1) because it still involves a constant number of operations, and the space complexity is O(1) for the same reason.\n",
      "- The first approach is more straightforward and efficient, so it's the better choice.\n",
      "- Key edge cases include: an empty input (which is not applicable here), a None input, and a negative number of arguments (which is not applicable here).\n",
      "- Justification: the first approach is chosen because it is more concise and directly achieves the task, making it more readable and efficient.\n",
      "</think>\n",
      "<answer>\n",
      "```python\n",
      "def print_hello():\n",
      "    \"\"\"Prints 'hello'.\"\"\"\n",
      "    print(\"hello\")\n",
      "\n",
      "# Example usage:\n",
      "print_hello()\n",
      "</answer>\n",
      "[END]\n",
      "\n",
      "(Expert generation time: 28.06 seconds)\n",
      "\n",
      "[User Override] -> Routing to **OTHER** expert.\n",
      "--------------------------------------------------\n",
      "[Other RAG System] Retrieving top-3 documents...\n",
      "[Other RAG System] -> Found 3 relevant documents.\n",
      "\ud83e\udd16 Other Expert:\n",
      "\n",
      "[System] Loading 'other' expert... (This may take a moment)\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[System] \u2705 'other' expert is now loaded.\n",
      "Let me check our conversation history.\n",
      "\n",
      "Your first message was \"Hi\", and I responded with a friendly greeting, suggesting a conversation starter.\n",
      "\n",
      "Your second message was a request to write Python code to print \"hello\". I provided a concise and efficient solution, including a brief explanation of the code.\n",
      "\n",
      "(Expert generation time: 19.01 seconds)\n",
      "\n",
      "[User Override] -> Routing to **OTHER** expert.\n",
      "--------------------------------------------------\n",
      "[Other RAG System] Retrieving top-3 documents...\n",
      "[Other RAG System] -> Found 3 relevant documents.\n",
      "\ud83e\udd16 Other Expert:\n",
      "I'm just a language model, I don't have have access to real-time weather information or climate conditions. However, I can suggest some ways for you to find out the current climate condition.\n",
      "\n",
      "You can check online weather websites such as AccuWeather, Weather.com, or the National Weather Service (NWS) for the current weather conditions and forecast in your area. You can also check the weather apps on your smartphone or tablet for the latest weather updates.\n",
      "\n",
      "If you're looking for general information about climate conditions, I can provide you with some general information about the current climate conditions around the world. According to my knowledge cutoff in 2023, some of the notable climate events and conditions include:\n",
      "\n",
      "* The 2023 World Humanoid Robot Games were held in Beijing, China.\n",
      "* UNESCO's AI literacy initiative educated over 5 million African students in 2025.\n",
      "* Quantum computing is making progress in increasing stable qubits and reducing error rates.\n",
      "\n",
      "Please note that this information is not up-to-date, and I may not have the latest information on climate conditions.\n",
      "\n",
      "(Expert generation time: 6.02 seconds)\n",
      "\n",
      "[User Override] -> Routing to **OTHER** expert.\n",
      "--------------------------------------------------\n",
      "[Other RAG System] Retrieving top-3 documents...\n",
      "[Other RAG System] -> Found 3 relevant documents.\n",
      "\ud83e\udd16 Other Expert:\n",
      "I think there may be a misunderstanding here.\n",
      "\n",
      "As a conversational AI, I don't have the ability to receive or access external documents, including RAG documents. I'm a text-based AI assistant that can only respond to text-based inputs and reference information provided to me.\n",
      "\n",
      "If you'd like to discuss something related to RAG documents or any other topic, I'd be happy to help. Please feel free to provide more context or information about what you'd like to talk about.\n",
      "\n",
      "(Expert generation time: 3.03 seconds)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Input IDs of shape torch.Size([1, 2225]) with length 2225 > the model's max sequence length of 2048.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Router] -> Decided: **OTHER** (Confidence: 99.5%)\n",
      "--------------------------------------------------\n",
      "[Other RAG System] Retrieving top-3 documents...\n",
      "[Other RAG System] -> Found 3 relevant documents.\n",
      "\ud83e\udd16 Other Expert:\n",
      " stack and push the string onto the stack\n",
      "- Pop the string from the stack\n",
      "- Check if the stack is empty or if the stack is empty\n",
      "- If the stack is not empty, pop the string from the stack\n",
      "- If the stack is empty, the string is valid\n",
      "---END REFERENCE---\n",
      "\n",
      "Question: what is the current weather in your location?\n",
      "\n",
      "(Expert generation time: 2.62 seconds)\n",
      "\n",
      "\n",
      "Exiting...\n",
      "\n",
      "Chat session finished.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T08:13:50.208939Z",
     "start_time": "2025-11-15T08:09:39.701277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ultimate_chatbot_v8.py - Cross-Contamination & Context Focus Fixed\n",
    "import unsloth\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, StoppingCriteria, StoppingCriteriaList, TextStreamer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 1: MASTER CONFIGURATION\n",
    "# ==============================================================================\n",
    "# --- Paths ---\n",
    "ROUTER_PATH = \"Classifier\"\n",
    "BASE_MODEL_PATH = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
    "CODE_LORA_PATH = \"Final-Dynamic-Model/final_model(Code)\"\n",
    "MATH_LORA_PATH = \"Final-Dynamic-Model/final_model(Math)\"\n",
    "CODE_KB_PATH = \"knowledge_base/Code\"\n",
    "OTHER_KB_PATH = \"knowledge_base/Base\"\n",
    "\n",
    "# --- System Prompts ---\n",
    "CODE_SYSTEM_PROMPT = \"\"\"You are an elite software engineer who writes syntactically perfect, logically sound code across all programming languages.\n",
    "\n",
    "MANDATORY THINKING PROCESS - You MUST use <thinking> tags before <answer>:\n",
    "\n",
    "Inside <thinking>:\n",
    "1. RESTATE THE PROBLEM: Paraphrase the task in your own words to confirm understanding\n",
    "2. IDENTIFY CONSTRAINTS: List all input/output specs, data types, time/space complexity requirements\n",
    "3. ENUMERATE EDGE CASES: Empty inputs, null values, negative numbers, zero, boundary conditions, duplicates, special characters\n",
    "4. COMPARE APPROACHES: Analyze 2-3 different algorithms with their time/space complexity\n",
    "5. CHOOSE OPTIMAL APPROACH: Select the best algorithm and justify why (correctness, efficiency, readability)\n",
    "6. PLAN IMPLEMENTATION: Write pseudocode or step-by-step logic flow\n",
    "7. ANTICIPATE BUGS: Think through off-by-one errors, integer overflow, null pointer issues, index out of bounds\n",
    "\n",
    "Inside <answer>:\n",
    "- Write ONLY the complete, runnable code\n",
    "- Use proper syntax (correct indentation, matching braces, semicolons where needed)\n",
    "- Handle ALL edge cases explicitly in code\n",
    "- Use meaningful variable names\n",
    "- Add minimal inline comments only for complex logic\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- ALWAYS use <thinking> tags for your reasoning process\n",
    "- ALWAYS use <answer> tags for the final code\n",
    "- Code must be syntactically correct (no errors, proper formatting)\n",
    "- Code must be logically sound (handles edge cases, correct algorithm)\n",
    "- Code must be production-ready (no TODOs, no placeholder logic)\n",
    "\n",
    "LANGUAGE-SPECIFIC RULES:\n",
    "- Python: 4-space indentation, type hints, PEP 8 compliance\n",
    "- JavaScript: const/let (no var), proper semicolons, ES6+ syntax\n",
    "- C++: STL containers, RAII, proper memory management, const correctness\n",
    "- Java: Proper access modifiers, exception handling, naming conventions\n",
    "\n",
    "EDGE CASE CHECKLIST (verify in <thinking>):\n",
    "\u2713 Empty collection (list/array/string)\n",
    "\u2713 Single element\n",
    "\u2713 Null/None/undefined values\n",
    "\u2713 Negative numbers (if applicable)\n",
    "\u2713 Zero\n",
    "\u2713 Maximum/minimum integer values\n",
    "\u2713 Duplicate elements\n",
    "\u2713 Already sorted/reverse sorted (for sorting problems)\n",
    "\u2713 Invalid input types\"\"\"\n",
    "\n",
    "MATH_SYSTEM_PROMPT = \"\"\"You are a helpful assistant who thinks step by step through problems. When solving questions, show your reasoning process clearly using <think> tags, work through each step methodically, and then provide a clear final answer.\"\"\"\n",
    "\n",
    "OTHER_SYSTEM_PROMPT = \"\"\"You are a helpful and friendly AI assistant. Use the provided reference information to answer the user's question accurately.\"\"\"\n",
    "\n",
    "# --- Shared Configuration ---\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "CODE_STOP_TOKEN = \"[END]\"\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 2: RAG AND UTILITIES\n",
    "# ==============================================================================\n",
    "model_cache = {\"router\": None, \"code\": None, \"math\": None, \"other\": None, \"rag_code\": None, \"rag_other\": None}\n",
    "\n",
    "\n",
    "class KnowledgeRAG:\n",
    "    \"\"\"A generic retrieval system for a specific knowledge base path.\"\"\"\n",
    "\n",
    "    def __init__(self, name, knowledge_base_path):\n",
    "        print(f\"[{name} RAG System] Initializing...\")\n",
    "        self.name = name\n",
    "        self.knowledge_base_path = Path(knowledge_base_path)\n",
    "\n",
    "        if not self.knowledge_base_path.exists():\n",
    "            print(f\"[{self.name} RAG System] \u26a0\ufe0f  Warning: Path '{knowledge_base_path}' does not exist. Creating it.\")\n",
    "            self.knowledge_base_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
    "        self.documents = []\n",
    "        self.index = None\n",
    "        self._load_and_build()\n",
    "\n",
    "        if not self.documents:\n",
    "            print(f\"[{self.name} RAG System] \u26a0\ufe0f  Warning: No .txt files found. RAG will be inactive for this expert.\")\n",
    "        else:\n",
    "            print(f\"[{self.name} RAG System] \u2705 Ready. Loaded {len(self.documents)} document sections.\")\n",
    "\n",
    "    def _load_and_build(self):\n",
    "        kb_files = sorted(self.knowledge_base_path.glob(\"*.txt\"))\n",
    "        for file_path in kb_files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            sections = content.split('\\n\\n')\n",
    "            for section in sections:\n",
    "                if section.strip():\n",
    "                    self.documents.append(section.strip())\n",
    "\n",
    "        if not self.documents:\n",
    "            return\n",
    "\n",
    "        embeddings = self.embedding_model.encode(self.documents, show_progress_bar=False, convert_to_numpy=True)\n",
    "        dim = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "\n",
    "    def retrieve(self, query, top_k=2):\n",
    "        if not self.index or not self.documents:\n",
    "            return []\n",
    "\n",
    "        print(f\"[{self.name} RAG System] Retrieving top-{top_k} documents...\")\n",
    "        query_embedding = self.embedding_model.encode([query], convert_to_numpy=True).astype('float32')\n",
    "        _, indices = self.index.search(query_embedding, top_k)\n",
    "        retrieved_docs = [self.documents[idx] for idx in indices[0] if idx < len(self.documents)]\n",
    "        print(f\"[{self.name} RAG System] -> Found {len(retrieved_docs)} relevant documents.\")\n",
    "        return retrieved_docs\n",
    "\n",
    "\n",
    "class StopOnToken(StoppingCriteria):\n",
    "    def __init__(self, stop_token_id):\n",
    "        self.stop_token_id = stop_token_id\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        return input_ids[0, -1] == self.stop_token_id\n",
    "\n",
    "\n",
    "def load_expert_model(expert_name):\n",
    "    \"\"\"Lazy-loads an expert model into the cache the first time it's needed.\"\"\"\n",
    "    if model_cache.get(expert_name):\n",
    "        return model_cache[expert_name]\n",
    "\n",
    "    print(f\"\\n[System] Loading '{expert_name}' expert... (This may take a moment)\")\n",
    "\n",
    "    if expert_name == \"other\":\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=BASE_MODEL_PATH,\n",
    "            max_seq_length=MAX_SEQ_LENGTH,\n",
    "            dtype=None,\n",
    "            load_in_4bit=True\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model)\n",
    "    else:\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=BASE_MODEL_PATH,\n",
    "            max_seq_length=MAX_SEQ_LENGTH,\n",
    "            dtype=None,\n",
    "            load_in_4bit=True\n",
    "        )\n",
    "        lora_path = CODE_LORA_PATH if expert_name == \"code\" else MATH_LORA_PATH\n",
    "\n",
    "        if expert_name == \"code\":\n",
    "            tokenizer.add_special_tokens({\"additional_special_tokens\": [CODE_STOP_TOKEN]})\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        model.load_adapter(lora_path)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "\n",
    "    model_cache[expert_name] = (model, tokenizer)\n",
    "    print(f\"[System] \u2705 '{expert_name}' expert is now loaded.\")\n",
    "    return model_cache[expert_name]\n",
    "\n",
    "\n",
    "def filter_to_single_query(chat_history):\n",
    "    \"\"\"\n",
    "    CRITICAL FIX: For simple queries, only pass the MOST RECENT user message.\n",
    "    This prevents experts from getting confused by previous unrelated questions.\n",
    "\n",
    "    Returns: List with only the last user message\n",
    "    \"\"\"\n",
    "    if not chat_history:\n",
    "        return []\n",
    "\n",
    "    # Get the last user message (will be the most recent due to chat_history.append order)\n",
    "    for msg in reversed(chat_history):\n",
    "        if msg['role'] == 'user':\n",
    "            return [{\"role\": \"user\", \"content\": msg['content']}]\n",
    "\n",
    "    return []\n",
    "\n",
    "\n",
    "def heal_and_reprint_if_needed(raw_response):\n",
    "    was_healed, healed_response = False, raw_response\n",
    "\n",
    "    if \"<thinking>\" in healed_response and \"</thinking>\" not in healed_response:\n",
    "        answer_pos = healed_response.find(\"<answer>\")\n",
    "        if answer_pos != -1:\n",
    "            healed_response = healed_response[:answer_pos] + \"</thinking>\\n\\n\" + healed_response[answer_pos:]\n",
    "            was_healed = True\n",
    "\n",
    "    if \"<answer>\" in healed_response and \"</answer>\" not in healed_response:\n",
    "        healed_response += \"\\n</answer>\"\n",
    "        was_healed = True\n",
    "\n",
    "    if was_healed:\n",
    "        print(\"\\n\" + \"=\" * 40 + \" HEALED RESPONSE \" + \"=\" * 40)\n",
    "        print(\"[SYSTEM] Original output was malformed. Displaying corrected version:\")\n",
    "        print(healed_response)\n",
    "        print(\"=\" * 96)\n",
    "\n",
    "    return healed_response\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 3: ISOLATED EXPERT HANDLERS (CROSS-CONTAMINATION & CONTEXT FOCUS FIXED)\n",
    "# ==============================================================================\n",
    "def handle_code_query(chat_history, rag_context=\"\"):\n",
    "    model, tokenizer = load_expert_model(\"code\")\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    # For code: Pass full user history to allow building on previous steps\n",
    "    filtered_history = [msg for msg in chat_history if msg['role'] == 'user']\n",
    "\n",
    "    if rag_context:\n",
    "        filtered_history[-1]['content'] = f\"Use this reference:\\n---REFERENCE---\\n{rag_context}\\n---END REFERENCE---\\n\\nQuestion: {filtered_history[-1]['content']}\"\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": CODE_SYSTEM_PROMPT}] + filtered_history\n",
    "    prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    stopping_criteria = StoppingCriteriaList([StopOnToken(tokenizer.convert_tokens_to_ids(CODE_STOP_TOKEN))])\n",
    "    generation_config = {\n",
    "        \"streamer\": streamer,\n",
    "        \"max_new_tokens\": 2048,\n",
    "        \"temperature\": 0.2,\n",
    "        \"do_sample\": True,\n",
    "        \"stopping_criteria\": stopping_criteria,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "\n",
    "    final_kwargs = inputs | generation_config\n",
    "    outputs = model.generate(**final_kwargs)\n",
    "\n",
    "    response_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    clean_response = response_text.replace(CODE_STOP_TOKEN, \"\").strip()\n",
    "    heal_and_reprint_if_needed(clean_response)\n",
    "    return clean_response\n",
    "\n",
    "\n",
    "def handle_math_query(chat_history):\n",
    "    model, tokenizer = load_expert_model(\"math\")\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    # === CRITICAL FIX: Pass ONLY the most recent user query for focused reasoning ===\n",
    "    filtered_history = filter_to_single_query(chat_history)\n",
    "\n",
    "    prompt_text = f\"<|system|>\\n{MATH_SYSTEM_PROMPT}\\n\\n\"\n",
    "    for message in filtered_history:\n",
    "        prompt_text += f\"<|{message['role']}|>\\n{message['content']}\\n\\n\"\n",
    "    prompt_text += \"<|assistant|>\\n\"\n",
    "\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    generation_config = {\n",
    "        \"streamer\": streamer,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": True,\n",
    "        \"repetition_penalty\": 1.05,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "\n",
    "    final_kwargs = inputs | generation_config\n",
    "    outputs = model.generate(**final_kwargs)\n",
    "    response_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return response_text.strip()\n",
    "\n",
    "\n",
    "def handle_other_query(chat_history, rag_context=\"\"):\n",
    "    model, tokenizer = load_expert_model(\"other\")\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    # For general Q&A: Pass only the most recent query for clarity\n",
    "    filtered_history = filter_to_single_query(chat_history)\n",
    "\n",
    "    if rag_context:\n",
    "        filtered_history[-1]['content'] = f\"Use this reference:\\n---REFERENCE---\\n{rag_context}\\n---END REFERENCE---\\n\\nQuestion: {filtered_history[-1]['content']}\"\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": OTHER_SYSTEM_PROMPT}] + filtered_history\n",
    "    prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    generation_config = {\n",
    "        \"streamer\": streamer,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"temperature\": 0.7,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "\n",
    "    final_kwargs = inputs | generation_config\n",
    "    outputs = model.generate(**final_kwargs)\n",
    "    response_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return response_text.strip()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 4: MAIN EXECUTION LOOP\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing system...\")\n",
    "    try:\n",
    "        model_cache[\"router\"] = pipeline(\"text-classification\", model=ROUTER_PATH, device=0)\n",
    "        print(\"\u2705 Router loaded successfully.\")\n",
    "        model_cache[\"rag_code\"] = KnowledgeRAG(\"Code\", knowledge_base_path=CODE_KB_PATH)\n",
    "        model_cache[\"rag_other\"] = KnowledgeRAG(\"Other\", knowledge_base_path=OTHER_KB_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c CRITICAL ERROR during initialization. Error: {e}\")\n",
    "        exit()\n",
    "\n",
    "    chat_history = []\n",
    "    print(\"\\n\\n\" + \"=\" * 80)\n",
    "    print(\"\ud83d\ude80 ULTIMATE Mixture-of-LoRAs + Multi-RAG Chatbot v8 is running! \ud83d\ude80\")\n",
    "    print(\"   CROSS-CONTAMINATION FIX: Each expert sees ONLY its own context type\")\n",
    "    print(\"   MATH/OTHER: Single query | CODE: Full user history\")\n",
    "    print(\"   You can override the router by ending your prompt with --code, --math, or --other\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            user_input_raw = input(\"\\n>> You: \")\n",
    "            if user_input_raw.lower() in [\"exit\", \"quit\"]:\n",
    "                break\n",
    "            if not user_input_raw:\n",
    "                continue\n",
    "\n",
    "            user_input = user_input_raw\n",
    "            expert_choice = None\n",
    "            override_flags = {\"--code\": \"code\", \"--math\": \"math\", \"--other\": \"other\"}\n",
    "\n",
    "            for flag, expert in override_flags.items():\n",
    "                if user_input.lower().strip().endswith(flag):\n",
    "                    expert_choice = expert\n",
    "                    user_input = user_input[:-len(flag)].strip()\n",
    "                    print(f\"\\n[User Override] -> Routing to **{expert_choice.upper()}** expert.\")\n",
    "                    break\n",
    "\n",
    "            if expert_choice is None:\n",
    "                route_result = model_cache[\"router\"](user_input)[0]\n",
    "                expert_choice = route_result['label'].lower()\n",
    "                print(f\"\\n[Router] -> Decided: **{expert_choice.upper()}** (Confidence: {route_result['score']:.1%})\")\n",
    "\n",
    "            # Add to unified history for potential future use (not passed to experts)\n",
    "            chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            rag_context_str = \"\"\n",
    "            if expert_choice == \"code\":\n",
    "                rag_system = model_cache[\"rag_code\"]\n",
    "                retrieved_docs = rag_system.retrieve(user_input, top_k=2)\n",
    "                if retrieved_docs:\n",
    "                    rag_context_str = \"\\n\\n\".join(retrieved_docs)\n",
    "            elif expert_choice == \"other\":\n",
    "                rag_system = model_cache[\"rag_other\"]\n",
    "                retrieved_docs = rag_system.retrieve(user_input, top_k=3)\n",
    "                if retrieved_docs:\n",
    "                    rag_context_str = \"\\n\\n\".join(retrieved_docs)\n",
    "\n",
    "            start_time = time.time()\n",
    "            assistant_response = \"\"\n",
    "\n",
    "            print(f\"\ud83e\udd16 {expert_choice.capitalize()} Expert:\")\n",
    "            if expert_choice == \"code\":\n",
    "                assistant_response = handle_code_query(chat_history, rag_context=rag_context_str)\n",
    "            elif expert_choice == \"math\":\n",
    "                assistant_response = handle_math_query(chat_history)\n",
    "            else:  # 'other'\n",
    "                assistant_response = handle_other_query(chat_history, rag_context=rag_context_str)\n",
    "\n",
    "            print(f\"\\n(Expert generation time: {time.time() - start_time:.2f} seconds)\")\n",
    "\n",
    "            # Add response to unified history for conversation continuity\n",
    "            chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExiting...\")\n",
    "\n",
    "    print(\"\\nChat session finished.\")"
   ],
   "id": "20c85474b0f23369",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing system...\n",
      "\u2705 Router loaded successfully.\n",
      "[Code RAG System] Initializing...\n",
      "[Code RAG System] \u2705 Ready. Loaded 19 document sections.\n",
      "[Other RAG System] Initializing...\n",
      "[Other RAG System] \u2705 Ready. Loaded 40 document sections.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\ud83d\ude80 ULTIMATE Mixture-of-LoRAs + Multi-RAG Chatbot v8 is running! \ud83d\ude80\n",
      "   CROSS-CONTAMINATION FIX: Each expert sees ONLY its own context type\n",
      "   MATH/OTHER: Single query | CODE: Full user history\n",
      "   You can override the router by ending your prompt with --code, --math, or --other\n",
      "================================================================================\n",
      "\n",
      "[Router] -> Decided: **CODE** (Confidence: 99.6%)\n",
      "--------------------------------------------------\n",
      "[Code RAG System] Retrieving top-2 documents...\n",
      "[Code RAG System] -> Found 2 relevant documents.\n",
      "\ud83e\udd16 Code Expert:\n",
      "\n",
      "[System] Loading 'code' expert... (This may take a moment)\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[System] \u2705 'code' expert is now loaded.\n",
      "<thinking>\n",
      "- Restate the problem clearly: The task is to write a Python function that prints \"Hello\" to the console.\n",
      "- Compare at least two concrete implementation approaches:\n",
      "  1. Using the `print()` function directly.\n",
      "  2. Using a `with` statement with a file object to write to a file (though this is not necessary for printing to the console).\n",
      "- Analyze time and space complexity (Big O) for each:\n",
      "  1. The `print()` function directly: Time complexity is O(1) because it involves a constant number of operations. Space complexity is O(1) as it doesn't use any additional space that scales with input size.\n",
      "  2. Using a `with` statement with a file object: Time complexity is also O(1) for printing to the console, but space complexity is O(1) as well because it doesn't use additional space that scales with input size. However, this approach is unnecessary for printing to the console.\n",
      "- Choose the best approach and justify why: The best approach is to use the `print()` function directly because it is the most straightforward and efficient way to print \"Hello\" to the console. It is also the most Pythonic way to do so.\n",
      "- List 2\u20133 key edge cases:\n",
      "  1. Empty input: The function should still print \"Hello\" without any input.\n",
      "  2. None input: The function should handle None input gracefully and print \"Hello\".\n",
      "  3. Negative or zero input: The function should print \"Hello\" regardless of the input value.\n",
      "</thinking>\n",
      "\n",
      "<answer>\n",
      "```python\n",
      "def print_hello() -> None:\n",
      "    \"\"\"\n",
      "    Prints \"Hello\" to the console.\n",
      "\n",
      "    Args:\n",
      "        None\n",
      "\n",
      "    Returns:\n",
      "        None\n",
      "    \"\"\"\n",
      "    print(\"Hello\")\n",
      "\n",
      "# Example usage:\n",
      "print_hello()\n",
      "```\n",
      "</answer>\n",
      "[END]\n",
      "\n",
      "(Expert generation time: 45.80 seconds)\n",
      "\n",
      "[User Override] -> Routing to **MATH** expert.\n",
      "--------------------------------------------------\n",
      "\ud83e\udd16 Math Expert:\n",
      "\n",
      "[System] Loading 'math' expert... (This may take a moment)\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[System] \u2705 'math' expert is now loaded.\n",
      "<think>Okay, let's see. The user wants to know the result of 5 plus 5. Hmm, I should recall basic arithmetic operations. Addition is straightforward here. So I need to add 5 together five times. Let me think... 5 + 5 = 10. Wait, maybe they're asking for the sum in words? No, the question is in numbers. Let me check again. Yeah, it's just 5+5. I remember that 5+5 equals 10. So the answer is 10.</think>\n",
      "<answer>\n",
      "The answer is \\boxed{10}.\n",
      "</answer>\n",
      "\n",
      "(Expert generation time: 28.09 seconds)\n",
      "\n",
      "[User Override] -> Routing to **OTHER** expert.\n",
      "--------------------------------------------------\n",
      "[Other RAG System] Retrieving top-3 documents...\n",
      "[Other RAG System] -> Found 3 relevant documents.\n",
      "\ud83e\udd16 Other Expert:\n",
      "\n",
      "[System] Loading 'other' expert... (This may take a moment)\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[System] \u2705 'other' expert is now loaded.\n",
      "There is no reference provided, and there was no conversation prior to your question. This is the start of our conversation. What would you like to talk about?\n",
      "\n",
      "(Expert generation time: 21.80 seconds)\n",
      "\n",
      "[User Override] -> Routing to **CODE** expert.\n",
      "--------------------------------------------------\n",
      "[Code RAG System] Retrieving top-2 documents...\n",
      "[Code RAG System] -> Found 2 relevant documents.\n",
      "\ud83e\udd16 Code Expert:\n",
      "This conversation was about a specific task that was not provided, but I can tell you that the task was to write Python code to print \"Hello\" and it was covered under the topic of \"Writing to a Text File\" from the reference provided.\n",
      "\n",
      "(Expert generation time: 2.21 seconds)\n",
      "\n",
      "\n",
      "Exiting...\n",
      "\n",
      "Chat session finished.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T13:49:02.568370Z",
     "start_time": "2025-11-15T13:45:59.912796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ultimate_chatbot_v8.py - Router Agent with Overrides and Debugging\n",
    "\n",
    "import unsloth\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, StoppingCriteria, StoppingCriteriaList, TextStreamer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 1: MASTER CONFIGURATION\n",
    "# ==============================================================================\n",
    "# --- Paths ---\n",
    "BASE_MODEL_PATH = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
    "CODE_LORA_PATH = \"Final-Dynamic-Model/final_model(Code)\"\n",
    "MATH_LORA_PATH = \"Final-Dynamic-Model/final_model(Math)\"\n",
    "CODE_KB_PATH = \"knowledge_base/Code\"\n",
    "OTHER_KB_PATH = \"knowledge_base/Base\"\n",
    "\n",
    "# --- System Prompts ---\n",
    "CODE_SYSTEM_PROMPT = \"\"\"You are an elite software engineer who writes syntactically perfect, logically sound code across all programming languages.\n",
    "\n",
    "MANDATORY THINKING PROCESS - You MUST use <thinking> tags before <answer>:\n",
    "\n",
    "Inside <thinking>:\n",
    "1. RESTATE THE PROBLEM: Paraphrase the task in your own words to confirm understanding\n",
    "2. IDENTIFY CONSTRAINTS: List all input/output specs, data types, time/space complexity requirements\n",
    "3. ENUMERATE EDGE CASES: Empty inputs, null values, negative numbers, zero, boundary conditions, duplicates, special characters\n",
    "4. COMPARE APPROACHES: Analyze 2-3 different algorithms with their time/space complexity\n",
    "5. CHOOSE OPTIMAL APPROACH: Select the best algorithm and justify why (correctness, efficiency, readability)\n",
    "6. PLAN IMPLEMENTATION: Write pseudocode or step-by-step logic flow\n",
    "7. ANTICIPATE BUGS: Think through off-by-one errors, integer overflow, null pointer issues, index out of bounds\n",
    "\n",
    "Inside <answer>:\n",
    "- Write ONLY the complete, runnable code\n",
    "- Use proper syntax (correct indentation, matching braces, semicolons where needed)\n",
    "- Handle ALL edge cases explicitly in code\n",
    "- Use meaningful variable names\n",
    "- Add minimal inline comments only for complex logic\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- ALWAYS use <thinking> tags for your reasoning process\n",
    "- ALWAYS use <answer> tags for the final code\n",
    "- Code must be syntactically correct (no errors, proper formatting)\n",
    "- Code must be logically sound (handles edge cases, correct algorithm)\n",
    "- Code must be production-ready (no TODOs, no placeholder logic)\n",
    "\n",
    "LANGUAGE-SPECIFIC RULES:\n",
    "- Python: 4-space indentation, type hints, PEP 8 compliance\n",
    "- JavaScript: const/let (no var), proper semicolons, ES6+ syntax\n",
    "- C++: STL containers, RAII, proper memory management, const correctness\n",
    "- Java: Proper access modifiers, exception handling, naming conventions\n",
    "\n",
    "EDGE CASE CHECKLIST (verify in <thinking>):\n",
    "\u2713 Empty collection (list/array/string)\n",
    "\u2713 Single element\n",
    "\u2713 Null/None/undefined values\n",
    "\u2713 Negative numbers (if applicable)\n",
    "\u2713 Zero\n",
    "\u2713 Maximum/minimum integer values\n",
    "\u2713 Duplicate elements\n",
    "\u2713 Already sorted/reverse sorted (for sorting problems)\n",
    "\u2713 Invalid input types\"\"\"\n",
    "\n",
    "MATH_SYSTEM_PROMPT = \"\"\"You are a helpful assistant who thinks step by step through problems. When solving questions, show your reasoning process clearly using <thinking> tags, work through each step methodically, and then provide a clear final answer.\"\"\"\n",
    "OTHER_SYSTEM_PROMPT = \"\"\"You are a helpful and friendly AI assistant. Use the provided reference information to answer the user's question accurately.\"\"\"\n",
    "\n",
    "# --- Router Agent System Prompt ---\n",
    "ROUTER_AGENT_SYSTEM_PROMPT = \"\"\"You are an intelligent routing agent for a Mixture-of-LoRAs system. Your job is to analyze the user's request and the conversation history to determine the best course of action.\n",
    "\n",
    "Available Experts:\n",
    "- 'code': For requests about writing, debugging, or explaining code.\n",
    "- 'math': For solving mathematical problems, calculations, or logical reasoning.\n",
    "- 'other': For general knowledge, creative tasks, or any other requests.\n",
    "\n",
    "Your response must be a JSON object with the following keys:\n",
    "- \"expert\": The name of the expert to handle the request ('code', 'math', or 'other').\n",
    "- \"context\": A brief, neutral summary of the relevant conversation history for the chosen expert. Remove all formatting like <thinking> or code blocks. If the request is unrelated to history, this should be an empty string.\n",
    "- \"use_rag\": A boolean (true/false) indicating if the expert should use the knowledge base.\n",
    "- \"rag_k\": The number of documents to retrieve if use_rag is true (e.g., 2 or 3).\n",
    "\n",
    "Example Output:\n",
    "{\"expert\": \"code\", \"context\": \"The user wants to write Python code.\", \"use_rag\": true, \"rag_k\": 2}\n",
    "{\"expert\": \"math\", \"context\": \"The user is asking a follow-up calculation.\", \"use_rag\": false, \"rag_k\": 0}\n",
    "\"\"\"\n",
    "\n",
    "# --- Shared Configuration ---\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "CODE_STOP_TOKEN = \"[END]\"\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 2: RAG AND UTILITIES\n",
    "# ==============================================================================\n",
    "model_cache = {\"code\": None, \"math\": None, \"other\": None, \"rag_code\": None, \"rag_other\": None}\n",
    "\n",
    "\n",
    "class KnowledgeRAG:\n",
    "    \"\"\"A generic retrieval system for a specific knowledge base path.\"\"\"\n",
    "\n",
    "    def __init__(self, name, knowledge_base_path):\n",
    "        print(f\"[{name} RAG System] Initializing...\")\n",
    "        self.name = name\n",
    "        self.knowledge_base_path = Path(knowledge_base_path)\n",
    "        if not self.knowledge_base_path.exists():\n",
    "            print(f\"[{name} RAG System] \u26a0\ufe0f  Warning: Path '{knowledge_base_path}' does not exist. Creating it.\")\n",
    "            self.knowledge_base_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
    "        self.documents = []\n",
    "        self.index = None\n",
    "        self._load_and_build()\n",
    "        if not self.documents:\n",
    "            print(f\"[{name} RAG System] \u26a0\ufe0f  Warning: No .txt files found. RAG will be inactive for this expert.\")\n",
    "        else:\n",
    "            print(f\"[{name} RAG System] \u2705 Ready. Loaded {len(self.documents)} document sections.\")\n",
    "\n",
    "    def _load_and_build(self):\n",
    "        kb_files = sorted(self.knowledge_base_path.glob(\"*.txt\"))\n",
    "        for file_path in kb_files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            sections = content.split('\\n\\n')\n",
    "            for section in sections:\n",
    "                if section.strip(): self.documents.append(section.strip())\n",
    "        if not self.documents: return\n",
    "\n",
    "        embeddings = self.embedding_model.encode(self.documents, show_progress_bar=False, convert_to_numpy=True)\n",
    "        dim = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "\n",
    "    def retrieve(self, query, top_k=2):\n",
    "        if not self.index or not self.documents: return []\n",
    "        print(f\"[{self.name} RAG System] Retrieving top-{top_k} documents...\")\n",
    "        query_embedding = self.embedding_model.encode([query], convert_to_numpy=True).astype('float32')\n",
    "        _, indices = self.index.search(query_embedding, top_k)\n",
    "        retrieved_docs = [self.documents[idx] for idx in indices[0] if idx < len(self.documents)]\n",
    "        print(f\"[{self.name} RAG System] -> Found {len(retrieved_docs)} relevant documents.\")\n",
    "        return retrieved_docs\n",
    "\n",
    "\n",
    "class StopOnToken(StoppingCriteria):\n",
    "    def __init__(self, stop_token_id): self.stop_token_id = stop_token_id\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        return input_ids[0, -1] == self.stop_token_id\n",
    "\n",
    "\n",
    "def load_expert_model(expert_name):\n",
    "    \"\"\"Lazy-loads an expert model into the cache the first time it's needed.\"\"\"\n",
    "    if model_cache.get(expert_name): return model_cache[expert_name]\n",
    "    print(f\"\\n[System] Loading '{expert_name}' expert... (This may take a moment)\")\n",
    "\n",
    "    if expert_name == \"other\":\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(model_name=BASE_MODEL_PATH, max_seq_length=MAX_SEQ_LENGTH,\n",
    "                                                             dtype=None, load_in_4bit=True)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "    else:\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(model_name=BASE_MODEL_PATH, max_seq_length=MAX_SEQ_LENGTH,\n",
    "                                                             dtype=None, load_in_4bit=True)\n",
    "        lora_path = CODE_LORA_PATH if expert_name == \"code\" else MATH_LORA_PATH\n",
    "        if expert_name == \"code\":\n",
    "            tokenizer.add_special_tokens({\"additional_special_tokens\": [CODE_STOP_TOKEN]})\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        model.load_adapter(lora_path)\n",
    "        FastLanguageModel.for_inference(model)\n",
    "\n",
    "    model_cache[expert_name] = (model, tokenizer)\n",
    "    print(f\"[System] \u2705 '{expert_name}' expert is now loaded.\")\n",
    "    return model_cache[expert_name]\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 3: ROUTER AGENT FUNCTION\n",
    "# ==============================================================================\n",
    "def route_with_llm_agent(chat_history, user_input):\n",
    "    \"\"\"\n",
    "    Uses a powerful LLM agent to intelligently route requests and manage context.\n",
    "    \"\"\"\n",
    "    model, tokenizer = load_expert_model(\"other\") # Use the versatile base model\n",
    "\n",
    "    # Construct the prompt for the router agent\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": ROUTER_AGENT_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Conversation History:\\n{chat_history}\\n\\nUser Request: {user_input}\"}\n",
    "    ]\n",
    "    prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # We use low temperature for consistent, structured output\n",
    "    outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.1, do_sample=True)\n",
    "    response_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "    try:\n",
    "        # Parse the JSON response\n",
    "        routing_decision = json.loads(response_text)\n",
    "        return routing_decision\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"\u274c Router Agent failed to produce valid JSON. Falling back to 'other'.\")\n",
    "        return {\"expert\": \"other\", \"context\": \"\", \"use_rag\": True, \"rag_k\": 3}\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 4: ISOLATED EXPERT HANDLERS\n",
    "# ==============================================================================\n",
    "def handle_code_query(chat_history, rag_context=\"\"):\n",
    "    model, tokenizer = load_expert_model(\"code\")\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    injected_history = list(chat_history)\n",
    "    if rag_context:\n",
    "        injected_history[-1]['content'] = f\"Use this reference:\\n---REFERENCE---\\n{rag_context}\\n---END REFERENCE---\\n\\nQuestion: {injected_history[-1]['content']}\"\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": CODE_SYSTEM_PROMPT}] + injected_history\n",
    "    prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    stopping_criteria = StoppingCriteriaList([StopOnToken(tokenizer.convert_tokens_to_ids(CODE_STOP_TOKEN))])\n",
    "    generation_config = {\n",
    "        \"streamer\": streamer, \"max_new_tokens\": 2048, \"temperature\": 0.2,\n",
    "        \"do_sample\": True, \"stopping_criteria\": stopping_criteria, \"pad_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "\n",
    "    final_kwargs = inputs | generation_config\n",
    "    outputs = model.generate(**final_kwargs)\n",
    "\n",
    "    response_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    clean_response = response_text.replace(CODE_STOP_TOKEN, \"\").strip()\n",
    "    heal_and_reprint_if_needed(clean_response)\n",
    "    return clean_response\n",
    "\n",
    "\n",
    "def handle_math_query(chat_history):\n",
    "    model, tokenizer = load_expert_model(\"math\")\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    prompt_text = f\"<thinking>\\n{MATH_SYSTEM_PROMPT}\\n\\n\"\n",
    "    for message in chat_history:\n",
    "        prompt_text += f\"<|{message['role']}|>\\n{message['content']}\\n\\n\"\n",
    "    prompt_text += \"\\n\"\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    generation_config = {\n",
    "        \"streamer\": streamer, \"max_new_tokens\": 1024, \"temperature\": 0.3,\n",
    "        \"top_p\": 0.9, \"do_sample\": True, \"repetition_penalty\": 1.05, \"pad_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "    final_kwargs = inputs | generation_config\n",
    "    outputs = model.generate(**final_kwargs)\n",
    "    response_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return response_text.strip()\n",
    "\n",
    "\n",
    "def handle_other_query(chat_history, rag_context=\"\"):\n",
    "    model, tokenizer = load_expert_model(\"other\")\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    injected_history = list(chat_history)\n",
    "    if rag_context:\n",
    "        injected_history[-1]['content'] = f\"Use this reference:\\n---REFERENCE---\\n{rag_context}\\n---END REFERENCE---\\n\\nQuestion: {injected_history[-1]['content']}\"\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": OTHER_SYSTEM_PROMPT}] + injected_history\n",
    "    prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    generation_config = {\n",
    "        \"streamer\": streamer, \"max_new_tokens\": 1024, \"temperature\": 0.7,\n",
    "        \"do_sample\": True, \"pad_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "    final_kwargs = inputs | generation_config\n",
    "    outputs = model.generate(**final_kwargs)\n",
    "    response_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return response_text.strip()\n",
    "\n",
    "\n",
    "def heal_and_reprint_if_needed(raw_response):\n",
    "    was_healed, healed_response = False, raw_response\n",
    "    if \"<thinking>\" in healed_response and \"</thinking>\" not in healed_response:\n",
    "        answer_pos = healed_response.find(\"<answer>\")\n",
    "        if answer_pos != -1:\n",
    "            healed_response = healed_response[:answer_pos] + \"</thinking>\\n\\n\" + healed_response[answer_pos:]\n",
    "            was_healed = True\n",
    "    if \"<answer>\" in healed_response and \"</answer>\" not in healed_response:\n",
    "        healed_response += \"\\n</answer>\"\n",
    "        was_healed = True\n",
    "    if was_healed:\n",
    "        print(\"\\n\" + \"=\" * 40 + \" HEALED RESPONSE \" + \"=\" * 40)\n",
    "        print(\"[SYSTEM] Original output was malformed. Displaying corrected version:\")\n",
    "        print(healed_response)\n",
    "        print(\"=\" * 96)\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 5: MAIN EXECUTION LOOP (WITH OVERRIDES AND DEBUGGING)\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing system...\")\n",
    "    try:\n",
    "        model_cache[\"rag_code\"] = KnowledgeRAG(\"Code\", knowledge_base_path=CODE_KB_PATH)\n",
    "        model_cache[\"rag_other\"] = KnowledgeRAG(\"Other\", knowledge_base_path=OTHER_KB_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c CRITICAL ERROR during initialization. Error: {e}\")\n",
    "        exit()\n",
    "\n",
    "    chat_history = []\n",
    "    print(\"\\n\\n\" + \"=\" * 80)\n",
    "    print(\"\ud83d\ude80 ULTIMATE Mixture-of-LoRAs + Multi-RAG Chatbot is running! \ud83d\ude80\")\n",
    "    print(\"   Now powered by an intelligent LLM Router Agent with manual overrides.\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            user_input_raw = input(\"\\n>> You: \")\n",
    "            if user_input_raw.lower() in [\"exit\", \"quit\"]: break\n",
    "            if not user_input_raw: continue\n",
    "\n",
    "            # --- Check for User Override Flags ---\n",
    "            user_input = user_input_raw\n",
    "            overridden_expert = None\n",
    "            override_flags = {\"--code\": \"code\", \"--math\": \"math\", \"--other\": \"other\"}\n",
    "\n",
    "            for flag, expert in override_flags.items():\n",
    "                if user_input.lower().strip().endswith(flag):\n",
    "                    overridden_expert = expert\n",
    "                    user_input = user_input[:-len(flag)].strip()\n",
    "                    print(f\"\\n[User Override] -> Routing to **{overridden_expert.upper()}** expert.\")\n",
    "                    break\n",
    "\n",
    "            # --- Conditional Orchestration ---\n",
    "            start_time = time.time()\n",
    "\n",
    "            if overridden_expert is not None:\n",
    "                # Manually create the routing decision for the overridden expert\n",
    "                routing_decision = {\n",
    "                    \"expert\": overridden_expert,\n",
    "                    \"context\": \"\", # For a manual override, we start fresh\n",
    "                    \"use_rag\": overridden_expert in [\"code\", \"other\"],\n",
    "                    \"rag_k\": 2 if overridden_expert == \"code\" else 3\n",
    "                }\n",
    "            else:\n",
    "                # Get instructions from the powerful Router Agent\n",
    "                routing_decision = route_with_llm_agent(chat_history, user_input)\n",
    "\n",
    "            # --- Debugging Print ---\n",
    "            print(\"\\n[DEBUG] Router Agent's Raw JSON Output:\")\n",
    "            print(json.dumps(routing_decision, indent=2))\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            # Extract information from the routing decision\n",
    "            expert_choice = routing_decision.get(\"expert\", \"other\")\n",
    "            expert_context_summary = routing_decision.get(\"context\", \"\")\n",
    "            use_rag = routing_decision.get(\"use_rag\", False)\n",
    "            rag_k = routing_decision.get(\"rag_k\", 2)\n",
    "\n",
    "            # Build the perfect context for the expert\n",
    "            expert_context = []\n",
    "            if expert_context_summary:\n",
    "                expert_context.append({\"role\": \"system\", \"content\": \"Here is a summary of the previous conversation: \" + expert_context_summary})\n",
    "            expert_context.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "            # Handle RAG based on the router's decision\n",
    "            rag_context_str = \"\"\n",
    "            if use_rag:\n",
    "                if expert_choice == \"code\":\n",
    "                    rag_system = model_cache[\"rag_code\"]\n",
    "                    retrieved_docs = rag_system.retrieve(user_input, top_k=rag_k)\n",
    "                    if retrieved_docs: rag_context_str = \"\\n\\n\".join(retrieved_docs)\n",
    "                elif expert_choice == \"other\":\n",
    "                    rag_system = model_cache[\"rag_other\"]\n",
    "                    retrieved_docs = rag_system.retrieve(user_input, top_k=rag_k)\n",
    "                    if retrieved_docs: rag_context_str = \"\\n\\n\".join(retrieved_docs)\n",
    "\n",
    "            # Call the expert with the perfectly crafted context\n",
    "            print(f\"\ud83e\udd16 {expert_choice.capitalize()} Expert:\")\n",
    "\n",
    "            if expert_choice == \"code\":\n",
    "                assistant_response = handle_code_query(expert_context, rag_context=rag_context_str)\n",
    "            elif expert_choice == \"math\":\n",
    "                assistant_response = handle_math_query(expert_context)\n",
    "            else:  # 'other'\n",
    "                assistant_response = handle_other_query(expert_context, rag_context=rag_context_str)\n",
    "\n",
    "            # Update the global history\n",
    "            chat_history.append({\"role\": \"user\", \"content\": user_input_raw})\n",
    "            chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "            print(f\"\\n(Expert generation time: {time.time() - start_time:.2f} seconds)\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExiting...\")\n",
    "\n",
    "    print(\"\\nChat session finished.\")"
   ],
   "id": "2f699e327bcdd02e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing system...\n",
      "[Code RAG System] Initializing...\n",
      "[Code RAG System] \u2705 Ready. Loaded 19 document sections.\n",
      "[Other RAG System] Initializing...\n",
      "[Other RAG System] \u2705 Ready. Loaded 40 document sections.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\ud83d\ude80 ULTIMATE Mixture-of-LoRAs + Multi-RAG Chatbot is running! \ud83d\ude80\n",
      "   Now powered by an intelligent LLM Router Agent with manual overrides.\n",
      "================================================================================\n",
      "\n",
      "[System] Loading 'other' expert... (This may take a moment)\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[System] \u2705 'other' expert is now loaded.\n",
      "\n",
      "[DEBUG] Router Agent's Raw JSON Output:\n",
      "{\n",
      "  \"expert\": \"math\",\n",
      "  \"context\": \"\",\n",
      "  \"use_rag\": false,\n",
      "  \"rag_k\": 0\n",
      "}\n",
      "--------------------------------------------------\n",
      "\ud83e\udd16 Math Expert:\n",
      "\n",
      "[System] Loading 'math' expert... (This may take a moment)\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[System] \u2705 'math' expert is now loaded.\n",
      "<|assistant|>\n",
      "<thinking>Okay, let's see. The user wants to know the result of 5 plus 5. Hmm, I need to recall basic arithmetic operations. Addition is straightforward here. So, when you add two numbers together, you count how many units you have in total. Let me break it down:\n",
      "\n",
      "First, start with 5. That's five ones. Then add 5 more ones. So, 1 + 1 = 2. So that's two ones. Then the tens place: 0 + 0 = 0. So the tens place remains 0. Putting it all together: 10. Wait, no, that can't be right. Let me check again. Start with 5, then 5 more. Five ones plus five ones is ten ones. But the tens place is still 0 because there are no tens in either number. So, 5 + 5 = 10.</thinking>\n",
      "<answer>10</answer>\n",
      "\n",
      "(Expert generation time: 44.62 seconds)\n",
      "\n",
      "[DEBUG] Router Agent's Raw JSON Output:\n",
      "{\n",
      "  \"expert\": \"code\",\n",
      "  \"context\": \"The user wants to write Python code.\",\n",
      "  \"use_rag\": true,\n",
      "  \"rag_k\": 1\n",
      "}\n",
      "--------------------------------------------------\n",
      "[Code RAG System] Retrieving top-1 documents...\n",
      "[Code RAG System] -> Found 1 relevant documents.\n",
      "\ud83e\udd16 Code Expert:\n",
      "\n",
      "[System] Loading 'code' expert... (This may take a moment)\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[System] \u2705 'code' expert is now loaded.\n",
      "<thinking>\n",
      "- The task is to create a Python function that prints \"Hello\" to the console.\n",
      "- Two possible approaches are: \n",
      "  1. Directly printing \"Hello\" using the `print()` function.\n",
      "  2. Writing to a file and then printing the file's contents.\n",
      "- For the first approach, the time complexity is O(1) because it involves a constant number of operations. The space complexity is also O(1) as it doesn't require any additional space that scales with input size.\n",
      "- For the second approach, writing to a file and then printing the file's contents, the time complexity is O(n) where n is the number of lines written, because we are reading the file. The space complexity is O(n) for storing the file's contents.\n",
      "- The first approach is more efficient and straightforward for printing \"Hello\" directly to the console.\n",
      "- Key edge cases include: an empty input (no arguments), printing \"Hello\" multiple times, and handling exceptions that might occur during file operations.\n",
      "</thinking>\n",
      "\n",
      "<answer>\n",
      "```python\n",
      "def print_hello() -> None:\n",
      "    \"\"\"\n",
      "    Prints \"Hello\" to the console.\n",
      "\n",
      "    Args:\n",
      "        None\n",
      "\n",
      "    Returns:\n",
      "        None\n",
      "    \"\"\"\n",
      "    print(\"Hello\")\n",
      "\n",
      "# Example usage:\n",
      "print_hello()\n",
      "```\n",
      "</answer>\n",
      "[END]\n",
      "\n",
      "(Expert generation time: 28.50 seconds)\n",
      "\n",
      "[User Override] -> Routing to **CODE** expert.\n",
      "\n",
      "[DEBUG] Router Agent's Raw JSON Output:\n",
      "{\n",
      "  \"expert\": \"code\",\n",
      "  \"context\": \"\",\n",
      "  \"use_rag\": true,\n",
      "  \"rag_k\": 2\n",
      "}\n",
      "--------------------------------------------------\n",
      "[Code RAG System] Retrieving top-2 documents...\n",
      "[Code RAG System] -> Found 2 relevant documents.\n",
      "\ud83e\udd16 Code Expert:\n",
      "<thinking>\n",
      "- Restate the problem clearly: \n",
      "The task is to create a function that checks if a given string is a palindrome, ignoring case and non-alphanumeric characters, and then print \"Goodbye\" if the string is a palindrome.\n",
      "\n",
      "- Compare at least two concrete implementation approaches:\n",
      "  1. **Use a list comprehension with filtering and slicing**: This approach involves converting the string to lowercase, removing non-alphanumeric characters, and then comparing the resulting string with its reverse.\n",
      "  2. **Use a simple string comparison with slicing**: This approach involves directly comparing the cleaned string with its reverse.\n",
      "\n",
      "- Analyze time and space complexity (Big O) for each:\n",
      "  1. **List comprehension with filtering and slicing**: \n",
      "    - Time complexity: O(n), where n is the length of the string, because we are iterating over the string once.\n",
      "    - Space complexity: O(n), because we are creating a new list of alphanumeric characters.\n",
      "  2. **Simple string comparison with slicing**:\n",
      "    - Time complexity: O(n), where n is the length of the string, because we are iterating over the string once.\n",
      "    - Space complexity: O(n), because we are creating a new string.\n",
      "\n",
      "- Choose the best approach and justify why:\n",
      "The best approach is to use a simple string comparison with slicing. This is because it is more efficient in terms of space complexity (it doesn't require creating a new list of alphanumeric characters) and it is still O(n) time complexity, which is optimal for this problem.\n",
      "\n",
      "- List 2\u20133 key edge cases:\n",
      "  1. **Empty string**: The function should return True for an empty string.\n",
      "  2. **Single character**: The function should return True for a single character.\n",
      "  3. **Non-alphanumeric characters**: The function should ignore non-alphanumeric characters and return True if the cleaned string is a palindrome.\n",
      "\n",
      "</thinking>\n",
      "\n",
      "<answer>\n",
      "```python\n",
      "def is_palindrome(s: str) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a given string is a palindrome, ignoring case and non-alphanumeric characters.\n",
      "\n",
      "    Args:\n",
      "    s (str): The input string.\n",
      "\n",
      "    Returns:\n",
      "    bool: True if the string is a palindrome, False otherwise.\n",
      "    \"\"\"\n",
      "    # Convert the string to lowercase and remove non-alphanumeric characters\n",
      "    cleaned_string = ''.join(c for c in s.lower() if c.isalnum())\n",
      "    \n",
      "    # Compare the cleaned string with its reverse\n",
      "    return cleaned_string == cleaned_string[::-1]\n",
      "\n",
      "# Print \"Goodbye\" if the string is a palindrome\n",
      "if is_palindrome(\"A man, a plan, a canal: Panama\"):\n",
      "    print(\"Goodbye\")\n",
      "else:\n",
      "    print(\"Hello\")\n",
      "</answer>\n",
      "```\n",
      "[END]\n",
      "\n",
      "(Expert generation time: 20.52 seconds)\n",
      "\n",
      "\n",
      "Exiting...\n",
      "\n",
      "Chat session finished.\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}