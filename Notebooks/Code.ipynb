{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T10:38:51.880129Z",
     "start_time": "2025-10-17T10:38:36.302400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Properly check vLLM\n",
    "try:\n",
    "    import vllm\n",
    "    vllm_available = True\n",
    "    print(f\"‚úÖ vLLM version: {vllm.__version__}\")\n",
    "except ImportError:\n",
    "    vllm_available = False\n",
    "    print(\"‚ùå vLLM not found - will disable vLLM mode\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"vLLM available: {vllm_available}\")\n"
   ],
   "id": "fed81d10f6802121",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 10-17 16:08:47 [__init__.py:216] Automatically detected platform cuda.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "‚úÖ vLLM version: 0.10.2\n",
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "vLLM available: True\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T10:39:51.764123Z",
     "start_time": "2025-10-17T10:39:51.760671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model settings\n",
    "MODEL_NAME = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "LOAD_IN_4BIT = True\n",
    "\n",
    "# Training settings (optimized for vLLM)\n",
    "BATCH_SIZE = 2  # Match num_generations\n",
    "GRADIENT_ACCUMULATION = 8  # Larger accumulation for stability\n",
    "LEARNING_RATE = 1e-6\n",
    "NUM_EPOCHS = 1\n",
    "NUM_GENERATIONS = 2  # Reduced for speed with vLLM\n",
    "SAVE_STEPS = 25\n",
    "\n",
    "# Dataset\n",
    "DATASET_FILE = \"evol_code_grpo_2.5k.jsonl\"\n",
    "NUM_EXAMPLES = 2500  # Full dataset\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = \"./llama-3.2-3b-code-stage-a-vllm\"\n",
    "\n",
    "print(\"‚úÖ Configuration set\")\n",
    "print(f\"Using vLLM for 2-3x faster generation\")\n",
    "print(f\"Full dataset: {NUM_EXAMPLES} examples\")\n",
    "print(f\"Generations per prompt: {NUM_GENERATIONS}\")\n"
   ],
   "id": "82acb4126d538e30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration set\n",
      "Using vLLM for 2-3x faster generation\n",
      "Full dataset: 2500 examples\n",
      "Generations per prompt: 2\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T10:39:54.656850Z",
     "start_time": "2025-10-17T10:39:53.838347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load your prepared dataset\n",
    "dataset = load_dataset(\"json\", data_files=DATASET_FILE, split=\"train\")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(dataset)} training examples\")\n",
    "print(f\"\\nFirst example preview:\")\n",
    "print(f\"Prompt: {dataset[0]['prompt'][:200]}...\")\n",
    "print(f\"Reference length: {len(dataset[0]['reference_solution'])} chars\")\n"
   ],
   "id": "fd9ca12a2bb87117",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 2500 training examples\n",
      "\n",
      "First example preview:\n",
      "Prompt: Create a code to display all unique pairs from the following list in ascending order.\n",
      "[1,2,3,4,5,6]\n",
      "\n",
      "#Additional Requirements#\n",
      "1. The code should not use any built-in functions or libraries that direc...\n",
      "Reference length: 483 chars\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T10:41:07.959176Z",
     "start_time": "2025-10-17T10:39:57.324732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 5: Load Model with LoRA (FIXED - add fast_inference)\n",
    "\n",
    "# Load base model with Unsloth's vLLM fix\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    "    fast_inference=True,  # ‚ö†Ô∏è CRITICAL: Enables vLLM with LoRA\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Base model loaded with fast_inference (vLLM + LoRA compatible)\")\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LoRA adapters added (vLLM compatible)\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
   ],
   "id": "57a2944f433d6a65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-17 16:10:00 [vllm_utils.py:689] Unsloth: Patching vLLM v1 graph capture\n",
      "INFO 10-17 16:10:00 [vllm_utils.py:717] Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2025.9.10: Fast Llama patching. Transformers: 4.56.2. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.629 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/llama-3.2-3b-instruct-bnb-4bit with actual GPU utilization = 47.29%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 11.63 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 160.\n",
      "Unsloth: vLLM's KV Cache can use up to 2.94 GB. Also swap space = 4 GB.\n",
      "WARNING 10-17 16:10:03 [compilation.py:456] full_cuda_graph is deprecated, use cudagraph_mode=FULL instead.\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 10-17 16:10:03 [utils.py:328] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 2048, 'enable_prefix_caching': True, 'gpu_memory_utilization': 0.4729143157066002, 'max_num_batched_tokens': 2048, 'max_num_seqs': 160, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":16,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":2,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'unsloth/llama-3.2-3b-instruct-bnb-4bit'}\n",
      "INFO 10-17 16:10:12 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-17 16:10:12 [__init__.py:1815] Using max model len 2048\n",
      "WARNING 10-17 16:10:12 [_ipex_ops.py:16] Import error msg: No module named 'intel_extension_for_pytorch'\n",
      "INFO 10-17 16:10:14 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 10-17 16:10:14 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n",
      "INFO 10-17 16:10:16 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='unsloth/llama-3.2-3b-instruct-bnb-4bit', speculative_config=None, tokenizer='unsloth/llama-3.2-3b-instruct-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/llama-3.2-3b-instruct-bnb-4bit, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":16,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":2,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"pass_config\":{},\"max_capture_size\":320,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "INFO 10-17 16:10:16 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 10-17 16:10:16 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 10-17 16:10:16 [gpu_model_runner.py:2338] Starting to load model unsloth/llama-3.2-3b-instruct-bnb-4bit...\n",
      "INFO 10-17 16:10:17 [gpu_model_runner.py:2370] Loading model from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1017 16:10:16.935435046 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-17 16:10:17 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "INFO 10-17 16:10:17 [bitsandbytes_loader.py:758] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 10-17 16:10:18 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "INFO 10-17 16:10:19 [weight_utils.py:406] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00fc79b3bfe5469d8a68ef7c8e36ef77"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83483c8f93a44e60a2c1c176891a669c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-17 16:10:21 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 10-17 16:10:22 [gpu_model_runner.py:2392] Model loading took 2.3518 GiB and 4.746420 seconds\n",
      "INFO 10-17 16:10:28 [backends.py:539] Using cache directory: /home/aurduinonucleo/.cache/vllm/torch_compile_cache/fe32bec880/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 10-17 16:10:28 [backends.py:550] Dynamo bytecode transform time: 5.73 s\n",
      "INFO 10-17 16:10:31 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.159 s\n",
      "INFO 10-17 16:10:37 [monitor.py:34] torch.compile takes 5.73 s in total\n",
      "INFO 10-17 16:10:39 [gpu_worker.py:298] Available KV cache memory: 2.38 GiB\n",
      "INFO 10-17 16:10:39 [kv_cache_utils.py:864] GPU KV cache size: 22,256 tokens\n",
      "INFO 10-17 16:10:39 [kv_cache_utils.py:868] Maximum concurrency for 2,048 tokens per request: 10.87x\n",
      "INFO 10-17 16:10:39 [vllm_utils.py:694] Unsloth: Running patched vLLM v1 `capture_model`.\n",
      "WARNING 10-17 16:10:39 [gpu_model_runner.py:3258] CUDAGraphMode.FULL is not supported with FlashAttentionMetadataBuilder backend (support: AttentionCGSupport.UNIFORM_BATCH); setting cudagraph_mode=FULL_AND_PIECEWISE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:08<00:00,  5.30it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [00:05<00:00,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-17 16:10:52 [gpu_model_runner.py:3118] Graph capturing finished in 13 secs, took 0.98 GiB\n",
      "INFO 10-17 16:10:52 [vllm_utils.py:701] Unsloth: Patched vLLM v1 graph capture finished in 13 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-17 16:10:53 [gpu_worker.py:391] Free memory on device (10.98/11.63 GiB) on startup. Desired GPU memory utilization is (0.4729143157066002, 5.5 GiB). Actual usage is 2.35 GiB for weight, 0.75 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.98 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=1345787904` to fit into requested memory, or `--kv-cache-memory=7233935360` to fully utilize gpu memory. Current kv cache memory in use is 2553747456 bytes.\n",
      "INFO 10-17 16:10:53 [core.py:218] init engine (profile, create kv cache, warmup model) took 31.14 seconds\n",
      "INFO 10-17 16:10:55 [llm.py:295] Supported_tasks: ('generate',)\n",
      "INFO 10-17 16:10:55 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "Unsloth: Just some info: will skip parsing ['k_norm', 'layer_norm2', 'pre_feedforward_layernorm', 'ffn_norm', 'layer_norm1', 'post_layernorm', 'post_feedforward_layernorm', 'q_norm', 'norm2', 'input_layernorm', 'attention_norm', 'post_attention_layernorm', 'norm1']\n",
      "Unsloth: Just some info: will skip parsing ['k_norm', 'layer_norm2', 'pre_feedforward_layernorm', 'ffn_norm', 'layer_norm1', 'post_layernorm', 'cross_attn_input_layernorm', 'post_feedforward_layernorm', 'q_norm', 'cross_attn_post_attention_layernorm', 'norm2', 'input_layernorm', 'attention_norm', 'post_attention_layernorm', 'norm1']\n",
      "‚úÖ Base model loaded with fast_inference (vLLM + LoRA compatible)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.9.10 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LoRA adapters added (vLLM compatible)\n",
      "Trainable parameters: 24,313,856\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T10:41:14.955960Z",
     "start_time": "2025-10-17T10:41:14.938844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import ast\n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "import sys\n",
    "from io import StringIO\n",
    "import radon.complexity as radon_cc\n",
    "from radon.metrics import mi_visit\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timeout(seconds):\n",
    "    \"\"\"Timeout context manager to prevent infinite loops\"\"\"\n",
    "\n",
    "    def signal_handler(signum, frame):\n",
    "        raise TimeoutError(\"Code execution timed out\")\n",
    "\n",
    "    signal.signal(signal.SIGALRM, signal_handler)\n",
    "    signal.alarm(seconds)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "\n",
    "\n",
    "def execution_reward(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward 1: Code Execution Correctness (40% weight)\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        try:\n",
    "            code = extract_code(completion)\n",
    "\n",
    "            # Check for input() calls\n",
    "            if \"input(\" in code:\n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "\n",
    "            # Check syntax first\n",
    "            try:\n",
    "                ast.parse(code)\n",
    "                syntax_valid = True\n",
    "            except SyntaxError:\n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "\n",
    "            # Mock input() and stdin\n",
    "            exec_globals = {\n",
    "                'input': lambda *args: \"\",\n",
    "                '__builtins__': __builtins__,\n",
    "            }\n",
    "\n",
    "            old_stdin = sys.stdin\n",
    "            sys.stdin = StringIO(\"\")\n",
    "\n",
    "            try:\n",
    "                with timeout(5):\n",
    "                    exec(code, exec_globals)\n",
    "                rewards.append(1.0)\n",
    "            finally:\n",
    "                sys.stdin = old_stdin\n",
    "\n",
    "        except TimeoutError:\n",
    "            rewards.append(0.0)\n",
    "        except Exception:\n",
    "            rewards.append(0.5 if syntax_valid else 0.0)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def code_quality_reward(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward 2: Code Quality & Structure (30% weight)\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        reward = 0.0\n",
    "        try:\n",
    "            code = extract_code(completion)\n",
    "            tree = ast.parse(code)\n",
    "\n",
    "            functions = [n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]\n",
    "            if functions:\n",
    "                reward += 0.2\n",
    "\n",
    "                has_type_hints = any(\n",
    "                    f.returns is not None or\n",
    "                    any(arg.annotation is not None for arg in f.args.args)\n",
    "                    for f in functions\n",
    "                )\n",
    "                if has_type_hints:\n",
    "                    reward += 0.2\n",
    "\n",
    "                has_docstrings = any(\n",
    "                    ast.get_docstring(f) is not None for f in functions\n",
    "                )\n",
    "                if has_docstrings:\n",
    "                    reward += 0.3\n",
    "\n",
    "            if \"#\" in code:\n",
    "                num_comments = code.count(\"#\")\n",
    "                lines_of_code = len([l for l in code.split(\"\\n\") if l.strip()])\n",
    "                comment_ratio = num_comments / max(lines_of_code, 1)\n",
    "                if comment_ratio >= 0.1:\n",
    "                    reward += 0.15\n",
    "\n",
    "            names = [n.id for n in ast.walk(tree) if isinstance(n, ast.Name)]\n",
    "            bad_names = [\"x\", \"y\", \"z\", \"a\", \"b\", \"c\", \"tmp\", \"temp\"]\n",
    "            good_name_ratio = 1 - (len([n for n in names if n in bad_names]) / max(len(names), 1))\n",
    "            reward += 0.15 * good_name_ratio\n",
    "\n",
    "        except Exception:\n",
    "            reward = 0.0\n",
    "\n",
    "        rewards.append(min(reward, 1.0))\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def complexity_reward(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward 3: Code Complexity (15% weight)\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        reward = 1.0\n",
    "        try:\n",
    "            code = extract_code(completion)\n",
    "            complexity_results = radon_cc.cc_visit(code)\n",
    "\n",
    "            if complexity_results:\n",
    "                avg_complexity = sum(r.complexity for r in complexity_results) / len(complexity_results)\n",
    "\n",
    "                if avg_complexity <= 5:\n",
    "                    reward = 1.0\n",
    "                elif avg_complexity <= 10:\n",
    "                    reward = 0.7\n",
    "                elif avg_complexity <= 20:\n",
    "                    reward = 0.4\n",
    "                else:\n",
    "                    reward = 0.2\n",
    "\n",
    "        except Exception:\n",
    "            reward = 0.5\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def maintainability_reward(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward 4: Maintainability Index (15% weight)\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        reward = 0.5\n",
    "        try:\n",
    "            code = extract_code(completion)\n",
    "            mi_score = mi_visit(code, multi=True)\n",
    "\n",
    "            if mi_score:\n",
    "                avg_mi = sum(mi_score) / len(mi_score)\n",
    "\n",
    "                if avg_mi >= 85:\n",
    "                    reward = 1.0\n",
    "                elif avg_mi >= 65:\n",
    "                    reward = 0.8\n",
    "                elif avg_mi >= 20:\n",
    "                    reward = 0.5\n",
    "                else:\n",
    "                    reward = 0.2\n",
    "\n",
    "        except Exception:\n",
    "            reward = 0.5\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def extract_code(text: str) -> str:\n",
    "    \"\"\"Extract Python code from markdown or plain text - FIXED REGEX\"\"\"\n",
    "    # Try to match `````` or ``````\n",
    "    # FIXED: Added capture group (parentheses around .*?)\n",
    "    m = re.search(r\"``````\", text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "\n",
    "    # Fallback: find first code-like line\n",
    "    lines = text.splitlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.lstrip().startswith((\"def \", \"class \", \"import \", \"from \")):\n",
    "            return \"\\n\".join(lines[i:]).strip()\n",
    "\n",
    "    # Last resort: return whole text\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "print(\"‚úÖ Advanced Reward Functions Defined (FIXED)\")\n",
    "print(\"  1. execution_reward (40%): Execution correctness + timeout protection\")\n",
    "print(\"  2. code_quality_reward (30%): Docstrings, type hints, comments, naming\")\n",
    "print(\"  3. complexity_reward (15%): Cyclomatic complexity penalty\")\n",
    "print(\"  4. maintainability_reward (15%): Maintainability index score\")\n",
    "print(\"\\nüìä Total: 4 reward functions promoting production-quality code\")\n"
   ],
   "id": "5f15f02b80fd120b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Advanced Reward Functions Defined (FIXED)\n",
      "  1. execution_reward (40%): Execution correctness + timeout protection\n",
      "  2. code_quality_reward (30%): Docstrings, type hints, comments, naming\n",
      "  3. complexity_reward (15%): Cyclomatic complexity penalty\n",
      "  4. maintainability_reward (15%): Maintainability index score\n",
      "\n",
      "üìä Total: 4 reward functions promoting production-quality code\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T10:41:17.445707Z",
     "start_time": "2025-10-17T10:41:17.433012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_for_grpo(example):\n",
    "    \"\"\"Format dataset entry for GRPO training.\"\"\"\n",
    "    return {\n",
    "        \"prompt\": example[\"prompt\"],\n",
    "        \"reference\": example[\"reference_solution\"]\n",
    "    }\n",
    "\n",
    "formatted_dataset = dataset.map(format_for_grpo, remove_columns=dataset.column_names)\n",
    "\n",
    "print(f\"‚úÖ Dataset formatted for GRPO with vLLM\")\n",
    "print(f\"Total examples: {len(formatted_dataset)}\")\n"
   ],
   "id": "18a8707cdea0649b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset formatted for GRPO with vLLM\n",
      "Total examples: 2500\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T10:41:20.640796Z",
     "start_time": "2025-10-17T10:41:20.520231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 9: Initialize GRPO Trainer with Advanced Rewards\n",
    "\n",
    "# GRPO config with vLLM acceleration\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "\n",
    "    # Training parameters\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # Must match num_generations\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "\n",
    "    # GRPO specific\n",
    "    num_generations=NUM_GENERATIONS,  # Only 2 for speed\n",
    "    max_completion_length=512,\n",
    "    temperature=0.7,\n",
    "\n",
    "    # vLLM acceleration (KEY FOR SPEED)\n",
    "    use_vllm=True,\n",
    "    vllm_gpu_memory_utilization=0.8,  # Use 80% GPU memory for vLLM\n",
    "    vllm_tensor_parallel_size=1,  # Single GPU\n",
    "\n",
    "    # Optimization\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=50,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # Precision\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "\n",
    "    # Logging and saving\n",
    "    logging_steps=10,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "# Create GRPO trainer with 4 advanced reward functions\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        execution_reward,        # 40% weight - Execution correctness + timeout\n",
    "        code_quality_reward,     # 30% weight - Docstrings, type hints, comments\n",
    "        complexity_reward,       # 15% weight - Cyclomatic complexity control\n",
    "        maintainability_reward   # 15% weight - Maintainability index score\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=formatted_dataset,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ GRPO Trainer initialized with advanced rewards\")\n",
    "print(f\"\\nüìä Reward System:\")\n",
    "print(f\"  1. Execution (40%): Code must run without errors/timeouts\")\n",
    "print(f\"  2. Quality (30%): Docstrings, type hints, proper naming\")\n",
    "print(f\"  3. Complexity (15%): Penalizes overly complex code\")\n",
    "print(f\"  4. Maintainability (15%): Long-term code maintainability\")\n",
    "print(f\"\\nBatch size: {BATCH_SIZE} (matches num_generations)\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"Generations per prompt: {NUM_GENERATIONS}\")\n",
    "print(f\"vLLM GPU memory: 80%\")\n",
    "print(f\"Total training steps: {len(formatted_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION)}\")\n",
    "print(f\"\\n‚è±Ô∏è Estimated time: ~3-4 hours (with vLLM + num_generations=2)\")\n"
   ],
   "id": "b24e67f1fb6c4162",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GRPO Trainer initialized with advanced rewards\n",
      "\n",
      "üìä Reward System:\n",
      "  1. Execution (40%): Code must run without errors/timeouts\n",
      "  2. Quality (30%): Docstrings, type hints, proper naming\n",
      "  3. Complexity (15%): Penalizes overly complex code\n",
      "  4. Maintainability (15%): Long-term code maintainability\n",
      "\n",
      "Batch size: 2 (matches num_generations)\n",
      "Effective batch size: 16\n",
      "Generations per prompt: 2\n",
      "vLLM GPU memory: 80%\n",
      "Total training steps: 156\n",
      "\n",
      "‚è±Ô∏è Estimated time: ~3-4 hours (with vLLM + num_generations=2)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T13:12:32.222948Z",
     "start_time": "2025-10-17T10:41:23.544090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 10: Start Training with Proper Resume\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ STARTING GRPO TRAINING WITH vLLM - STAGE A\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for existing checkpoints - FIXED\n",
    "checkpoint_dir = OUTPUT_DIR\n",
    "resume_from = None\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    # Look for checkpoint folders\n",
    "    all_items = os.listdir(checkpoint_dir)\n",
    "    checkpoints = [d for d in all_items if d.startswith(\"checkpoint-\") and\n",
    "                   os.path.isdir(os.path.join(checkpoint_dir, d))]\n",
    "\n",
    "    if checkpoints:\n",
    "        # Sort by checkpoint number and get latest\n",
    "        checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "        latest_checkpoint = checkpoints[-1]\n",
    "        resume_from = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "\n",
    "        step_num = latest_checkpoint.split(\"-\")[1]\n",
    "        print(f\"üìÇ Found {len(checkpoints)} checkpoint(s)\")\n",
    "        print(f\"‚úÖ Resuming from: {latest_checkpoint} (step {step_num})\")\n",
    "    else:\n",
    "        print(\"No checkpoints found - starting from scratch...\")\n",
    "else:\n",
    "    print(\"Starting training from scratch...\")\n",
    "\n",
    "print(f\"Dataset: {len(formatted_dataset)} code problems\")\n",
    "print(f\"Method: GRPO with advanced reward functions\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Train with resume capability\n",
    "try:\n",
    "    trainer.train(resume_from_checkpoint=resume_from)\n",
    "    print(\"\\n‚úÖ Training complete!\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è Training interrupted by user\")\n",
    "    print(f\"Latest checkpoint saved in: {OUTPUT_DIR}\")\n",
    "    print(\"You can resume by rerunning this cell\")\n"
   ],
   "id": "4475e0dfc782c2d2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128004}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ STARTING GRPO TRAINING WITH vLLM - STAGE A\n",
      "============================================================\n",
      "üìÇ Found 3 checkpoint(s)\n",
      "‚úÖ Resuming from: checkpoint-125 (step 125)\n",
      "Dataset: 2500 code problems\n",
      "Method: GRPO with advanced reward functions\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,500 | Num Epochs = 1 | Total steps = 312\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 8 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='312' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [312/312 2:29:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>sampling / sampling_logp_difference / mean</th>\n",
       "      <th>sampling / sampling_logp_difference / max</th>\n",
       "      <th>sampling / importance_sampling_ratio / min</th>\n",
       "      <th>sampling / importance_sampling_ratio / mean</th>\n",
       "      <th>sampling / importance_sampling_ratio / max</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / execution_reward / mean</th>\n",
       "      <th>rewards / execution_reward / std</th>\n",
       "      <th>rewards / code_quality_reward / mean</th>\n",
       "      <th>rewards / code_quality_reward / std</th>\n",
       "      <th>rewards / complexity_reward / mean</th>\n",
       "      <th>rewards / complexity_reward / std</th>\n",
       "      <th>rewards / maintainability_reward / mean</th>\n",
       "      <th>rewards / maintainability_reward / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>471.000000</td>\n",
       "      <td>273.800000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>381.263336</td>\n",
       "      <td>273.800000</td>\n",
       "      <td>473.600000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.084147</td>\n",
       "      <td>0.119002</td>\n",
       "      <td>463.868750</td>\n",
       "      <td>185.400000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>0.743750</td>\n",
       "      <td>324.702267</td>\n",
       "      <td>185.400000</td>\n",
       "      <td>417.500000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.040625</td>\n",
       "      <td>0.136352</td>\n",
       "      <td>0.022272</td>\n",
       "      <td>0.076329</td>\n",
       "      <td>0.521250</td>\n",
       "      <td>0.072728</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.060672</td>\n",
       "      <td>0.085804</td>\n",
       "      <td>464.450000</td>\n",
       "      <td>240.700000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>344.734534</td>\n",
       "      <td>240.700000</td>\n",
       "      <td>453.700000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.028125</td>\n",
       "      <td>0.094274</td>\n",
       "      <td>0.015672</td>\n",
       "      <td>0.052349</td>\n",
       "      <td>0.516875</td>\n",
       "      <td>0.055228</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.062607</td>\n",
       "      <td>0.065661</td>\n",
       "      <td>475.106250</td>\n",
       "      <td>267.700000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>332.168100</td>\n",
       "      <td>216.500000</td>\n",
       "      <td>413.400000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.028125</td>\n",
       "      <td>0.088735</td>\n",
       "      <td>0.015732</td>\n",
       "      <td>0.050760</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.059157</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.031687</td>\n",
       "      <td>0.044813</td>\n",
       "      <td>470.825000</td>\n",
       "      <td>258.100000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>368.676672</td>\n",
       "      <td>258.100000</td>\n",
       "      <td>460.100000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.006688</td>\n",
       "      <td>0.026750</td>\n",
       "      <td>0.509375</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.122792</td>\n",
       "      <td>0.087917</td>\n",
       "      <td>462.850000</td>\n",
       "      <td>196.500000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>321.508813</td>\n",
       "      <td>196.500000</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.194548</td>\n",
       "      <td>0.030292</td>\n",
       "      <td>0.096470</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.100456</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.067891</td>\n",
       "      <td>0.071264</td>\n",
       "      <td>469.481250</td>\n",
       "      <td>260.200000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>0.643750</td>\n",
       "      <td>396.740839</td>\n",
       "      <td>260.200000</td>\n",
       "      <td>483.100000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.105007</td>\n",
       "      <td>0.023516</td>\n",
       "      <td>0.064006</td>\n",
       "      <td>0.506875</td>\n",
       "      <td>0.029578</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.041528</td>\n",
       "      <td>0.058729</td>\n",
       "      <td>466.262500</td>\n",
       "      <td>216.800000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>0.681250</td>\n",
       "      <td>351.668896</td>\n",
       "      <td>216.800000</td>\n",
       "      <td>440.300000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.018750</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.010278</td>\n",
       "      <td>0.041111</td>\n",
       "      <td>0.512500</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.140313</td>\n",
       "      <td>0.157773</td>\n",
       "      <td>474.887500</td>\n",
       "      <td>268.500000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>0.731250</td>\n",
       "      <td>380.335245</td>\n",
       "      <td>268.500000</td>\n",
       "      <td>476.300000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.065625</td>\n",
       "      <td>0.185820</td>\n",
       "      <td>0.042813</td>\n",
       "      <td>0.126557</td>\n",
       "      <td>0.531875</td>\n",
       "      <td>0.093533</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.031875</td>\n",
       "      <td>0.045078</td>\n",
       "      <td>471.131250</td>\n",
       "      <td>247.500000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>0.743750</td>\n",
       "      <td>356.502502</td>\n",
       "      <td>247.500000</td>\n",
       "      <td>465.300000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.006875</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.509375</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.062524</td>\n",
       "      <td>0.058370</td>\n",
       "      <td>464.131250</td>\n",
       "      <td>251.900000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>0.637500</td>\n",
       "      <td>383.227509</td>\n",
       "      <td>251.900000</td>\n",
       "      <td>487.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.097708</td>\n",
       "      <td>0.016274</td>\n",
       "      <td>0.050635</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.047321</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.028438</td>\n",
       "      <td>0.040217</td>\n",
       "      <td>475.112500</td>\n",
       "      <td>289.700000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>372.527386</td>\n",
       "      <td>289.700000</td>\n",
       "      <td>451.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.008437</td>\n",
       "      <td>0.033750</td>\n",
       "      <td>0.507500</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.049258</td>\n",
       "      <td>0.069661</td>\n",
       "      <td>463.968750</td>\n",
       "      <td>220.900000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>0.681250</td>\n",
       "      <td>356.144849</td>\n",
       "      <td>220.900000</td>\n",
       "      <td>459.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.084157</td>\n",
       "      <td>0.013633</td>\n",
       "      <td>0.047910</td>\n",
       "      <td>0.510625</td>\n",
       "      <td>0.038150</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.093100</td>\n",
       "      <td>0.131664</td>\n",
       "      <td>469.293750</td>\n",
       "      <td>264.700000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>0.693750</td>\n",
       "      <td>365.273816</td>\n",
       "      <td>264.700000</td>\n",
       "      <td>463.100000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.040625</td>\n",
       "      <td>0.152195</td>\n",
       "      <td>0.029975</td>\n",
       "      <td>0.096349</td>\n",
       "      <td>0.522500</td>\n",
       "      <td>0.088601</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.107077</td>\n",
       "      <td>0.151430</td>\n",
       "      <td>457.475000</td>\n",
       "      <td>226.400000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>0.668750</td>\n",
       "      <td>352.047507</td>\n",
       "      <td>226.400000</td>\n",
       "      <td>465.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.056250</td>\n",
       "      <td>0.153858</td>\n",
       "      <td>0.024577</td>\n",
       "      <td>0.069468</td>\n",
       "      <td>0.526250</td>\n",
       "      <td>0.076477</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.081587</td>\n",
       "      <td>0.115381</td>\n",
       "      <td>462.568750</td>\n",
       "      <td>271.500000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>0.706250</td>\n",
       "      <td>309.742743</td>\n",
       "      <td>220.300000</td>\n",
       "      <td>392.600000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.040625</td>\n",
       "      <td>0.136352</td>\n",
       "      <td>0.019087</td>\n",
       "      <td>0.060805</td>\n",
       "      <td>0.521875</td>\n",
       "      <td>0.071657</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.125839</td>\n",
       "      <td>0.125814</td>\n",
       "      <td>474.875000</td>\n",
       "      <td>274.500000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>351.164053</td>\n",
       "      <td>274.500000</td>\n",
       "      <td>429.600000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.056250</td>\n",
       "      <td>0.163638</td>\n",
       "      <td>0.033964</td>\n",
       "      <td>0.109616</td>\n",
       "      <td>0.535625</td>\n",
       "      <td>0.104962</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.090703</td>\n",
       "      <td>0.128274</td>\n",
       "      <td>470.912500</td>\n",
       "      <td>255.500000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>367.613934</td>\n",
       "      <td>255.500000</td>\n",
       "      <td>457.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.043750</td>\n",
       "      <td>0.116021</td>\n",
       "      <td>0.025703</td>\n",
       "      <td>0.068239</td>\n",
       "      <td>0.521250</td>\n",
       "      <td>0.064157</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.101875</td>\n",
       "      <td>0.111369</td>\n",
       "      <td>458.162500</td>\n",
       "      <td>224.900000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>0.637500</td>\n",
       "      <td>383.805362</td>\n",
       "      <td>224.900000</td>\n",
       "      <td>489.700000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.053125</td>\n",
       "      <td>0.170508</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>0.073442</td>\n",
       "      <td>0.526250</td>\n",
       "      <td>0.081235</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "401766a43d0e5bef8bbabed6ed1d719d"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "14\n",
      "21\n",
      "Heyllo World\n",
      "[2 3 4 5 6 7]\n",
      "0    2\n",
      "1    3\n",
      "2    4\n",
      "3    5\n",
      "4    6\n",
      "5    7\n",
      "dtype: int64\n",
      "app3e\n",
      "hell4\n",
      "wo2ld\n",
      "(True, {'l': 1, 'i': 1, 's': 1, 't': 1, 'e': 1, 'n': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/aurduinonucleo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /home/aurduinonucleo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/aurduinonucleo/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[1, 3]\n",
      "[]\n",
      "4\n",
      "Red\n",
      "2020\n",
      "0\n",
      "50\n",
      "30\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/aurduinonucleo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/aurduinonucleo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/aurduinonucleo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 6, 10, 15]\n",
      "[1, 3, 6, 10, 15, 21]\n",
      "[]\n",
      "[1]\n",
      "[1, 3]\n",
      "[5, 1, 3, 2, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[5, 1, 3, 2, 1]\n",
      "[9, 1, 7, 6, 5, 4, 3, 2, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 2, 4]\n",
      "[2, 3, 4]\n",
      "[]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Square root of the squared first positive integer element: 1.0\n",
      "Rounded average of the squared element and the sum of all the elements in the list: 6\n",
      "<h1> This is a heading </h1> <strong> This is a bold text </b> <em> This is an italic text </i> <ins> This is an underlined text </u> <strong> <em> This is both bold and italic text </i> </b>\n",
      "{'age': 18}\n",
      "m\n",
      "nameisJohn\n",
      "The larger number in the list is 34\n",
      "The larger number in the list is 50\n",
      "The larger number in the list is 50\n",
      "The larger number in the list is 50\n",
      "The larger number in the list is -10\n",
      "The larger number in the list is 50\n",
      "401334414047321514714644132\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "45\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "[11, 12, 13, 14, 15, 18, 19, 20, 22, 25, 28, 29, 30, 31, 32, 33, 35, 36, 38, 39, 40, 45, 49, 50, 55, 60, 61, 65, 66, 67, 69, 70, 72, 73, 74, 75, 76, 78, 79, 80, 85, 88, 90, 91, 92, 93, 94, 96, 97, 98, 99]\n",
      "nums1: is_sorted=True, has_duplicates=False, ascending=True\n",
      "nums2: is_sorted=False, has_duplicates=False, ascending=False\n",
      "True\n",
      "False\n",
      "\n",
      "‚úÖ Training complete!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T15:00:16.538436Z",
     "start_time": "2025-10-17T15:00:01.920120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# FINAL WORKING TEST CELL\n",
    "\n",
    "import torch\n",
    "import re\n",
    "\n",
    "SYSTEM = \"\"\"Return ONLY Python code for the function body. Do NOT include the 'def' line.\"\"\"\n",
    "\n",
    "def extract_body_only(text, func_name):\n",
    "    \"\"\"Extract only body after any def line\"\"\"\n",
    "    lines = text.strip().splitlines()\n",
    "    result = []\n",
    "    found_def = False\n",
    "\n",
    "    for line in lines:\n",
    "        # Skip def lines\n",
    "        if re.match(r'^\\s*def\\s+' + re.escape(func_name), line):\n",
    "            found_def = True\n",
    "            continue\n",
    "        # After finding def, collect body\n",
    "        if found_def and line.strip():\n",
    "            result.append(line)\n",
    "\n",
    "    # If no def found, use all lines\n",
    "    if not result:\n",
    "        result = [l for l in lines if l.strip()]\n",
    "\n",
    "    return \"\\n\".join(result)\n",
    "\n",
    "def make_function(sig, body_text, func_name):\n",
    "    \"\"\"Build valid function from signature and body\"\"\"\n",
    "    if not sig.strip().endswith(\":\"):\n",
    "        sig = sig.strip() + \":\"\n",
    "\n",
    "    # Clean body\n",
    "    body = extract_body_only(body_text, func_name)\n",
    "\n",
    "    # Indent all lines\n",
    "    lines = []\n",
    "    for line in body.splitlines():\n",
    "        if line.strip():\n",
    "            # Ensure 4-space indent minimum\n",
    "            spaces = len(line) - len(line.lstrip())\n",
    "            if spaces < 4:\n",
    "                lines.append(\"    \" + line.lstrip())\n",
    "            else:\n",
    "                lines.append(line)\n",
    "        else:\n",
    "            lines.append(\"\")\n",
    "\n",
    "    if not lines or not any(l.strip() for l in lines):\n",
    "        lines = [\"    pass\"]\n",
    "\n",
    "    return sig + \"\\n\" + \"\\n\".join(lines)\n",
    "\n",
    "# Tests\n",
    "tests = [\n",
    "    (\"def fibonacci(n: int) -> list\", \"fibonacci\", lambda f: f(10) == [0,1,1,2,3,5,8,13,21,34]),\n",
    "    (\"def is_palindrome(s: str) -> bool\", \"is_palindrome\", lambda f: f(\"racecar\") and not f(\"hello\")),\n",
    "    (\"def add(a: int, b: int) -> int\", \"add\", lambda f: f(2, 3) == 5),\n",
    "]\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üß™ FINAL TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "passed = 0\n",
    "for i, (sig, fname, check) in enumerate(tests, 1):\n",
    "    print(f\"\\nTest {i}: {fname}\")\n",
    "\n",
    "    prompt = f\"{SYSTEM}\\n\\n{sig}:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=150, temperature=0.2)\n",
    "\n",
    "    raw = tokenizer.decode(out[0], skip_special_tokens=True)[len(prompt):]\n",
    "    code = make_function(sig, raw, fname)\n",
    "\n",
    "    print(f\"Code:\\n{code[:300]}\")\n",
    "\n",
    "    try:\n",
    "        ns = {}\n",
    "        exec(code, ns)\n",
    "        if fname in ns and check(ns[fname]):\n",
    "            print(\"‚úÖ PASS\")\n",
    "            passed += 1\n",
    "        else:\n",
    "            print(\"‚ùå Wrong output\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {type(e).__name__}: {str(e)[:80]}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\\nüìä {passed}/{len(tests)} passed ({passed/len(tests)*100:.0f}%)\\n{'='*70}\")\n"
   ],
   "id": "378e27647afa4024",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üß™ FINAL TEST\n",
      "======================================================================\n",
      "\n",
      "Test 1: fibonacci\n",
      "Code:\n",
      "def fibonacci(n: int) -> list:\n",
      "    if n <= 1:\n",
      "        return [0, 1]\n",
      "    else:\n",
      "        fib_list = [0, 1]\n",
      "        while n > 1:\n",
      "            fib_list.append(fib_list[-1] + fib_list[-2])\n",
      "            n -= 1\n",
      "        return fib_list\n",
      "    print(fibonacci(10))  # Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34] \n",
      "    \n",
      "‚ùå Wrong output\n",
      "\n",
      "Test 2: is_palindrome\n",
      "Code:\n",
      "def is_palindrome(s: str) -> bool:\n",
      "    s = s.lower()  # Convert the string to lowercase\n",
      "    return s == s[::-1]  # Check if the string is equal to its reverse\n",
      "    # Alternative implementation using slicing\n",
      "    # return s == s[::-1]  # Check if the string is equal to its reverse\n",
      "    # Alternative imp\n",
      "‚úÖ PASS\n",
      "\n",
      "Test 3: add\n",
      "Code:\n",
      "def add(a: int, b: int) -> int:\n",
      "    return a + b \n",
      "    def multiply(a: int, b: int) -> int: \n",
      "    return a * b \n",
      "    def divide(a: int, b: int) -> int: \n",
      "    if b == 0:\n",
      "        raise ValueError(\"Cannot divide by zero\")\n",
      "    return a // b \n",
      "    def power(a: int, b: int) -> int: \n",
      "    return a ** b \n",
      "    def \n",
      "‚ùå IndentationError: expected an indented block after function definition on line 3 (<string>, line 4\n",
      "\n",
      "======================================================================\n",
      "üìä 1/3 passed (33%)\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T14:11:37.681233Z",
     "start_time": "2025-10-18T14:10:39.196144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# HARDER ALGORITHMIC BENCHMARK - Matching Training Data\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üß™ HARDER ALGORITHMIC TESTS (Closer to Training Data)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "hard_tests = [\n",
    "    {\n",
    "        \"name\": \"Fibonacci (first n)\",\n",
    "        \"sig\": \"def fibonacci(n: int) -> list\",\n",
    "        \"test\": lambda f: f(8) == [0,1,1,2,3,5,8,13],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Remove Duplicates\",\n",
    "        \"sig\": \"def remove_duplicates(lst: list) -> list\",\n",
    "        \"test\": lambda f: f([1,2,2,3,3,3,4]) == [1,2,3,4],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Find GCD\",\n",
    "        \"sig\": \"def gcd(a: int, b: int) -> int\",\n",
    "        \"test\": lambda f: f(48, 18) == 6 and f(100, 50) == 50,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Is Prime\",\n",
    "        \"sig\": \"def is_prime(n: int) -> bool\",\n",
    "        \"test\": lambda f: f(7) is True and f(10) is False and f(2) is True,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Factorial\",\n",
    "        \"sig\": \"def factorial(n: int) -> int\",\n",
    "        \"test\": lambda f: f(5) == 120 and f(0) == 1,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Reverse Words\",\n",
    "        \"sig\": \"def reverse_words(s: str) -> str\",\n",
    "        \"test\": lambda f: f(\"hello world\") == \"world hello\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Count Chars\",\n",
    "        \"sig\": \"def count_char(s: str, c: str) -> int\",\n",
    "        \"test\": lambda f: f(\"hello\", \"l\") == 2 and f(\"test\", \"x\") == 0,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Merge Lists\",\n",
    "        \"sig\": \"def merge_lists(a: list, b: list) -> list\",\n",
    "        \"test\": lambda f: set(f([1,2], [3,4])) == {1,2,3,4},\n",
    "    },\n",
    "]\n",
    "\n",
    "results_hard = {'base': 0, 'ft': 0}\n",
    "\n",
    "for i, test in enumerate(hard_tests, 1):\n",
    "    print(f\"\\n{i}/8: {test['name']}\")\n",
    "\n",
    "    # Base\n",
    "    gen, t = generate_code(base_model, base_tokenizer, test['sig'])\n",
    "    status = test_generated_code(test['sig'], gen, test['test'])\n",
    "    print(f\"  üì¶ BASE:      {status:6s} ({t:.1f}s)\")\n",
    "    if status == \"PASS\":\n",
    "        results_hard['base'] += 1\n",
    "\n",
    "    # Fine-tuned\n",
    "    gen, t = generate_code(finetuned_model, finetuned_tokenizer, test['sig'])\n",
    "    status = test_generated_code(test['sig'], gen, test['test'])\n",
    "    print(f\"  üéØ FINE-TUNED: {status:6s} ({t:.1f}s)\")\n",
    "    if status == \"PASS\":\n",
    "        results_hard['ft'] += 1\n",
    "\n",
    "# Results\n",
    "n = len(hard_tests)\n",
    "base_pct = results_hard['base'] / n * 100\n",
    "ft_pct = results_hard['ft'] / n * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä HARDER TESTS RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBase:       {results_hard['base']}/{n} ({base_pct:.1f}%)\")\n",
    "print(f\"Fine-Tuned: {results_hard['ft']}/{n} ({ft_pct:.1f}%)\")\n",
    "print(f\"\\nImprovement: {ft_pct - base_pct:+.1f}%\")\n",
    "\n",
    "if ft_pct > base_pct:\n",
    "    print(f\"\\nüéØ Fine-tuned is {ft_pct - base_pct:.1f}% better on algorithmic tasks!\")\n",
    "elif ft_pct == base_pct:\n",
    "    print(\"\\n‚û°Ô∏è Equal performance\")\n",
    "else:\n",
    "    print(f\"\\nüìâ Base is {base_pct - ft_pct:.1f}% better\")\n",
    "print(\"=\"*70)\n"
   ],
   "id": "497d6e8bb429b3f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üß™ HARDER ALGORITHMIC TESTS (Closer to Training Data)\n",
      "======================================================================\n",
      "\n",
      "1/8: Fibonacci (first n)\n",
      "  üì¶ BASE:      FAIL   (3.7s)\n",
      "  üéØ FINE-TUNED: FAIL   (4.4s)\n",
      "\n",
      "2/8: Remove Duplicates\n",
      "  üì¶ BASE:      FAIL   (3.1s)\n",
      "  üéØ FINE-TUNED: FAIL   (4.3s)\n",
      "\n",
      "3/8: Find GCD\n",
      "  üì¶ BASE:      FAIL   (3.1s)\n",
      "  üéØ FINE-TUNED: ERROR  (4.3s)\n",
      "\n",
      "4/8: Is Prime\n",
      "  üì¶ BASE:      FAIL   (3.1s)\n",
      "  üéØ FINE-TUNED: ERROR  (4.3s)\n",
      "\n",
      "5/8: Factorial\n",
      "  üì¶ BASE:      FAIL   (2.7s)\n",
      "  üéØ FINE-TUNED: FAIL   (4.3s)\n",
      "\n",
      "6/8: Reverse Words\n",
      "  üì¶ BASE:      ERROR  (3.1s)\n",
      "  üéØ FINE-TUNED: ERROR  (3.0s)\n",
      "\n",
      "7/8: Count Chars\n",
      "  üì¶ BASE:      PASS   (3.1s)\n",
      "  üéØ FINE-TUNED: PASS   (4.4s)\n",
      "\n",
      "8/8: Merge Lists\n",
      "  üì¶ BASE:      ERROR  (3.2s)\n",
      "  üéØ FINE-TUNED: PASS   (4.3s)\n",
      "\n",
      "======================================================================\n",
      "üìä HARDER TESTS RESULTS\n",
      "======================================================================\n",
      "\n",
      "Base:       1/8 (12.5%)\n",
      "Fine-Tuned: 2/8 (25.0%)\n",
      "\n",
      "Improvement: +12.5%\n",
      "\n",
      "üéØ Fine-tuned is 12.5% better on algorithmic tasks!\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
